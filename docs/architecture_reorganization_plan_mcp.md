# Detailed Plan: MCP Architecture for PostChain with SSE Streaming and Model Abstraction

## 1. Define Model-Aware MCP Servers for Each Phase (SSE Streaming & Langchain Utils Integration)

We will create a separate MCP server for each of the six PostChain phases: Action, Experience, Intention, Observation, Understanding, and Yield. Each server will be enhanced to support Server-Sent Events (SSE) for streaming output and will utilize the existing model abstraction layer in `api/app/langchain_utils.py`. Each server will provide *two* SSE streams:

*   **`reasoning` stream:**  For emitting tokens representing the reasoning process of the phase. This stream is optional and may be empty for phases without explicit reasoning steps.
*   **`answer` stream:** For emitting the final output tokens of the phase.

Each MCP server will:

*   **Random Model Selection:**  Each server will randomly select a model at initialization from the list returned by `get_tool_compatible_models(config)` in `langchain_utils.py` (for random model mode).
*   **Fixed Model for Prototype:** For the initial prototype (in "fixed model mode"), each server will use `config.GOOGLE_GEMINI_20_FLASH` for simplicity and speed.
*   **Utilize `langchain_utils.py`:** Import and use functions from `langchain_utils.py` (especially `get_base_model` and `convert_to_langchain_messages`) to initialize and interact with LLMs. This ensures consistency with the existing model abstraction layer.
*   **Model Configuration:**  Each server will have a configuration option to switch between *fixed model* (`GOOGLE_GEMINI_20_FLASH`) and *random model selection* modes.
*   **SSE Streaming with Reasoning Token Extraction:** Implement dual SSE streams (`reasoning` and `answer`) and model-specific logic for extracting reasoning tokens.  This logic will be tailored to the models chosen for each phase and will likely involve conditional parsing based on the model provider (e.g., checking for `</think>` for DeepSeek, `"reasoning"` field for Claude, etc.).

## 2. Model and Tool Allocation per Phase (Refined Model Choices & Langchain Utils)

*   **Action Server:**
    *   Purpose: Handle user input, initial prompting, and route messages.
    *   Model: Randomly selected from `get_tool_compatible_models(config)` (random mode).  *Fixed Prototype Model:* `config.GOOGLE_GEMINI_20_FLASH`.
    *   Tools: `ask_followup_question`.
    *   SSE Streams:  Primarily `answer` stream for the initial prompt or clarification questions. `reasoning` stream likely minimal.

*   **Experience Server:**
    *   Purpose: Enrich context with past knowledge, retrieve relevant information.
    *   Models:  Randomly selected from `get_tool_compatible_models(config)` (random mode). *Fixed Prototype Model:* `config.GOOGLE_GEMINI_20_FLASH`.
    *   Tools: `brave_web_search`, `fetch_url`, `qdrant`.
    *   SSE Streams: `reasoning` stream could show search queries and document summaries as they are processed. `answer` stream would contain the final enriched context.

*   **Intention Server:**
    *   Purpose:  Model user intent, track goals, and focus information.
    *   Model: Randomly selected from `get_tool_compatible_models(config)` (random mode). *Fixed Prototype Model:* `config.GOOGLE_GEMINI_20_FLASH`.
    *   Tools:  None initially, focus on model-driven intent inference.
    *   SSE Streams: `reasoning` stream would emit the thoughts generated by `sequentialthinking`. `answer` stream would contain the final inferred intent and goals.

*   **Observation Server:**
    *   Purpose: Track semantic connections, tag information, and preserve relationships.
    *   Model: Randomly selected from `get_tool_compatible_models(config)` (random mode). *Fixed Prototype Model:* `config.GOOGLE_GEMINI_20_FLASH`.
    *   Tools: None initially, focus on internal state management.
    *   SSE Streams: `reasoning` stream could describe the semantic analysis process. `answer` stream would confirm the observation actions taken (tagging, linking, etc.).

*   **Understanding Server:**
    *   Purpose: Evaluate context, filter information, and manage information release.
    *   Model: Randomly selected from `get_tool_compatible_models(config)` (random mode). *Fixed Prototype Model:* `config.GOOGLE_GEMINI_20_FLASH`.
    *   Tools: None initially, focus on model-driven context evaluation and filtering.
    *   SSE Streams: `reasoning` stream (showing context evaluation and filtering steps), `answer` stream (final refined context).

*   **Yield Server:**
    *   Purpose: Format output, decide on recursion, and complete the process.
    *   Model: Randomly selected from `get_tool_compatible_models(config)` (random mode). *Fixed Prototype Model:* `config.GOOGLE_GEMINI_20_FLASH`.
    *   Tools: `attempt_completion`, `write_to_file`.
    *   SSE Streams: `reasoning` stream (for output formatting steps), `answer` stream (final formatted output).

## 3. Orchestration and Communication (Python API, SSE, Langchain Utils, Dynamic Models)

The Python API will:

*   Use `langchain_utils.py` for any client-side model interactions if needed (though most model interactions will be within MCP servers).
*   Orchestrate MCP server calls and SSE stream handling.
*   Be aware that each phase can now use a *randomly selected* tool-compatible model.

## 4. Prototyping and Testing (Model-Specific SSE, Langchain Utils Integration)

Prototyping will involve:

*   Selecting specific models for each of the Action, Experience, and Yield phases (as outlined above).
*   Implementing model-specific reasoning token extraction in each server, tailored to the chosen models (DeepSeek, Claude, etc.).
*   Integrating `langchain_utils.py` for model initialization within each MCP server.
*   Testing the dual SSE streams with a UI that can display "reasoning" and "answer" tokens separately.  Start with a basic terminal UI for initial testing.

## 5. Economic Actions and PySUI Integration (Later Phase)

Integration with economic actions and PySUI will be considered in a later phase, after the core MCP and SSE architecture is functional and tested.

## Mermaid Diagram for Plan:

```mermaid
graph LR
    A[User Input] --> ActionServer
    ActionServer -- Reasoning SSE --> PythonAPI
    ActionServer -- Answer SSE --> PythonAPI
    PythonAPI --> ExperienceServer
    ExperienceServer -- Reasoning SSE --> PythonAPI
    ExperienceServer -- Answer SSE --> PythonAPI
    PythonAPI --> IntentionServer
    IntentionServer -- Reasoning SSE --> PythonAPI
    IntentionServer -- Answer SSE --> PythonAPI
    PythonAPI --> ObservationServer
    ObservationServer -- Reasoning SSE --> PythonAPI
    ObservationServer -- Answer SSE --> PythonAPI
    PythonAPI --> UnderstandingServer
    UnderstandingServer -- Reasoning SSE --> PythonAPI
    UnderstandingServer -- Answer SSE --> PythonAPI
    PythonAPI --> YieldServer
    YieldServer -- Reasoning SSE --> PythonAPI
    YieldServer -- Answer SSE --> PythonAPI
    PythonAPI --> B[User Output / Attempt Completion]

    subgraph MCP Servers
    ActionServer
    ExperienceServer
    IntentionServer
    ObservationServer
    UnderstandingServer
    YieldServer
    end

    subgraph Python API
    PythonAPI
    end


    style MCP Servers fill:#f9f,stroke:#333,stroke-width:2px
    style Python API fill:#ccf,stroke:#333,stroke-width:2px
    linkStyle 0,2,4,6,8,10,12,14,16,18 stroke-dasharray: 5 5;
