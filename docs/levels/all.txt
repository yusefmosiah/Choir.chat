# Level 0 Documentation



=== File: docs/tree.md ===



==
tree.md
==


# Choir Directory Structure
## Output of $ tree -I 'venv|archive|__pycache__|iOS_Example|dependencies' | pbcopy

.
├── api
│   ├── __init__.py
│   ├── app
│   │   ├── __init__.py
│   │   ├── config.py
│   │   ├── database.py
│   │   ├── langchain_utils.py
│   │   ├── middleware
│   │   │   ├── __init__.py
│   │   │   └── auth.py
│   │   ├── models
│   │   │   ├── __init__.py
│   │   │   ├── api.py
│   │   │   ├── auth.py
│   │   │   └── user.py
│   │   ├── postchain
│   │   │   ├── __init__.py
│   │   │   ├── langchain_workflow.py
│   │   │   ├── phases
│   │   │   ├── postchain_llm.py
│   │   │   ├── prompts
│   │   │   │   └── prompts.py
│   │   │   ├── README.md
│   │   │   ├── schemas
│   │   │   │   ├── __init__.py
│   │   │   │   ├── rewards.py
│   │   │   │   └── state.py
│   │   │   ├── state
│   │   │   └── utils.py
│   │   ├── routers
│   │   │   ├── auth.py
│   │   │   ├── balance.py
│   │   │   ├── notifications.py
│   │   │   ├── postchain.py
│   │   │   ├── threads.py
│   │   │   ├── users.py
│   │   │   └── vectors.py
│   │   ├── services
│   │   │   ├── __init__.py
│   │   │   ├── auth_service.py
│   │   │   ├── notification_service.py
│   │   │   ├── push_notification_service.py
│   │   │   ├── rewards_service.py
│   │   │   └── sui_service.py
│   │   ├── tools
│   │   │   ├── __init__.py
│   │   │   ├── base.py
│   │   │   ├── brave_search.py
│   │   │   ├── calculator.py
│   │   │   ├── qdrant.py
│   │   │   ├── tavily_search.py
│   │   │   └── web_search.py
│   │   └── utils.py
│   ├── blog
│   │   ├── business_model.md
│   │   ├── inverse_scaling_law.md
│   │   └── loop_of_thought.md
│   ├── content
│   │   ├── landing.md
│   │   ├── marketing.md
│   │   ├── privacy.md
│   │   └── support.md
│   ├── Dockerfile
│   ├── main.py
│   ├── pyproject.toml
│   ├── pytest.ini
│   ├── requirements.txt
│   ├── static
│   │   └── shared
│   │       ├── script.js
│   │       └── style.css
│   ├── templates
│   │   └── base.html
│   ├── test_push_notification_e2e.py
│   ├── test_push_notification.py
│   └── tests
│       ├── __init__.py
│       ├── conftest.py
│       ├── postchain
│       │   ├── __init__.py
│       │   ├── models_test.py
│       │   ├── random_gen_prompts.md
│       │   ├── test_cases.json
│       │   ├── test_langchain_workflow.py
│       │   ├── test_providers_abstracted.py
│       │   ├── test_providers.py
│       │   ├── test_simple_multimodel_stream.py
│       │   ├── test_structured_output.py
│       │   └── test_utils.py
│       ├── test_main.py
│       ├── test_sui_service.py
│       ├── test_user_thread_endpoints.py
│       └── tools
│           ├── __init__.py
│           ├── direct_search_diagnostic.py
│           ├── direct_search_test.py
│           ├── test_brave_search.py
│           ├── test_calculator.py
│           ├── test_multimodel_with_tools.py
│           ├── test_recent_events.py
│           ├── test_search_tools_report.py
│           ├── test_tavily_search.py
│           └── test_updated_search.py
├── Choir
│   ├── App
│   │   ├── AppDelegate.swift
│   │   ├── BackgroundStateMonitor.swift
│   │   └── ChoirApp.swift
│   ├── Assets.xcassets
│   │   ├── AccentColor.colorset
│   │   │   └── Contents.json
│   │   ├── AppIcon.appiconset
│   │   │   ├── Contents.json
│   │   │   └── Icon-App-1024x1024@2x.png
│   │   ├── choir-logo.imageset
│   │   │   └── Contents.json
│   │   ├── Contents.json
│   │   └── Icon-App-1024x1024.imageset
│   │       ├── Contents.json
│   │       └── Icon-App-1024x1024@2x.png
│   ├── Choir.entitlements
│   ├── Config
│   │   └── ApiConfig.swift
│   ├── ContentView.swift
│   ├── Coordinators
│   │   ├── AppCoordinator.swift
│   │   ├── PostchainCoordinator.swift
│   │   └── PostchainCoordinatorImpl.swift
│   ├── Documentation
│   │   └── DesignStyleGuide.md
│   ├── Extensions
│   ├── Info.plist
│   ├── Models
│   │   ├── AnyCodable.swift
│   │   ├── APITypes.swift
│   │   ├── AuthModels.swift
│   │   ├── CoinType.swift
│   │   ├── ConversationModels.swift
│   │   ├── NotificationModels.swift
│   │   ├── PostchainStreamEvent+Extension.swift
│   │   ├── SearchModels.swift
│   │   └── WalletBalance.swift
│   ├── Networking
│   │   ├── EventSource.swift
│   │   ├── PostchainAPIClient.swift
│   │   └── RewardsService.swift
│   ├── Preview Content
│   │   └── Preview Assets.xcassets
│   │       └── Contents.json
│   ├── Services
│   │   ├── APIClient.swift
│   │   ├── AuthService.swift
│   │   ├── BackgroundTaskManager.swift
│   │   ├── KeychainService.swift
│   │   ├── ModelConfigManager.swift
│   │   ├── PushNotificationManager.swift
│   │   ├── ThreadManager.swift
│   │   ├── ThreadPersistenceService.swift
│   │   ├── TransactionService.swift
│   │   ├── VectorService.swift
│   │   └── WalletManager.swift
│   ├── Utils
│   │   ├── MarkdownPaginator.swift
│   │   ├── MarkdownThemes.swift
│   │   ├── PaginationCacheManager.swift
│   │   ├── PaginationUtils.swift
│   │   ├── String+Extensions.swift
│   │   ├── TextSelectionSheet.swift
│   │   └── UIDevice+Extensions.swift
│   ├── ViewModels
│   │   └── PostchainViewModel.swift
│   └── Views
│       ├── ChoirThreadDetailView.swift
│       ├── Components
│       ├── EnhancedSendCoinView.swift
│       ├── GlassPageControl.swift
│       ├── ImportMnemonicView.swift
│       ├── LoginView.swift
│       ├── MessageRow.swift
│       ├── ModelConfigView.swift
│       ├── OnboardingView.swift
│       ├── PaginatedMarkdownView.swift
│       ├── PhaseCard.swift
│       ├── PhaseCardContextMenu.swift
│       ├── PostchainView.swift
│       ├── QRScannerView.swift
│       ├── SettingsView.swift
│       ├── Styles
│       ├── Thread
│       │   └── Components
│       │       ├── ThreadInputBar.swift
│       │       └── ThreadMessageList.swift
│       ├── ThreadExportView.swift
│       ├── ThreadImportView.swift
│       ├── TransactionsView.swift
│       ├── WalletCardView.swift
│       ├── WalletSelectionView.swift
│       └── WalletView.swift
├── choir_coin
│   └── choir_coin
│       ├── build
│       │   ├── choir
│       │   │   ├── BuildInfo.yaml
│       │   │   ├── bytecode_modules
│       │   │   │   └── choir.mv
│       │   │   ├── debug_info
│       │   │   │   ├── choir.json
│       │   │   │   └── choir.mvd
│       │   │   └── sources
│       │   │       └── choir.move
│       │   └── locks
│       ├── Move.lock
│       ├── Move.toml
│       ├── sources
│       │   └── choir_coin.move
│       └── tests
│           └── choir_coin_tests.move
├── Choir.xcodeproj
│   ├── project.pbxproj
│   ├── project.xcworkspace
│   │   ├── contents.xcworkspacedata
│   │   ├── xcshareddata
│   │   │   └── swiftpm
│   │   │       ├── configuration
│   │   │       └── Package.resolved
│   │   └── xcuserdata
│   │       └── wiz.xcuserdatad
│   │           ├── IDEFindNavigatorScopes.plist
│   │           └── UserInterfaceState.xcuserstate
│   └── xcuserdata
│       └── wiz.xcuserdatad
│           ├── xcdebugger
│           │   └── Breakpoints_v2.xcbkptlist
│           └── xcschemes
│               └── xcschememanagement.plist
├── ChoirTests
│   ├── APIResponseTests.swift
│   ├── ChoirTests.swift
│   ├── ChoirThreadTests.swift
│   └── RESTPostchainAPIClientTests.swift
├── ChoirUITests
│   ├── ChoirUITests.swift
│   └── ChoirUITestsLaunchTests.swift
├── CLAUDE.md
├── docker-compose.yml
├── docs
│   ├── blockchain_integration.md
│   ├── CHANGELOG.md
│   ├── ChoirPushNotificationsImplementationGuide.md
│   ├── contract_deployment.md
│   ├── core_core.md
│   ├── core_economics.md
│   ├── data_engine_model.md
│   ├── e_business.md
│   ├── e_concept.md
│   ├── evolution_naming.md
│   ├── evolution_token.md
│   ├── issues
│   │   └── retry.md
│   ├── levels
│   │   ├── all.txt
│   │   ├── level0.md
│   │   ├── level1.md
│   │   ├── level2.md
│   │   ├── level3.md
│   │   ├── level4.md
│   │   └── level5.md
│   ├── mainnet_migration.md
│   ├── notification_system.md
│   ├── plan_anonymity_by_default.md
│   ├── plan_choir_materialization.md
│   ├── postchain_service_redesign.md
│   ├── postchain_temporal_logic.md
│   ├── postchain_ui_redesign.md
│   ├── process_doctrine.md
│   ├── publish_thread_feature.md
│   ├── refactoring_planning_strategy.md
│   ├── relationship_staking.md
│   ├── require_action_phase.md
│   ├── require_experience_phase.md
│   ├── require_intention_phase.md
│   ├── require_observation_phase.md
│   ├── require_phase_requirements_index.md
│   ├── require_understanding_phase.md
│   ├── require_yield_phase.md
│   ├── reward_function.md
│   ├── rewards_system.md
│   ├── scripts
│   │   ├── combiner.sh
│   │   └── update_tree.sh
│   ├── security_considerations.md
│   ├── stack_argument.md
│   ├── state_management_patterns.md
│   ├── tree.md
│   └── wallet_languification.md
├── notebooks
│   ├── post_chain0.ipynb
│   └── vowel_loop3.ipynb
├── README.md
├── render.yaml
├── reward_function_simplified.py
├── reward_function.py
└── scripts
    ├── generate_provider_reports.sh
    ├── generate_quick_search_report.sh
    ├── generate_search_report.sh
    ├── generate_single_provider_report.sh
    ├── sources_displaying.sh
    ├── test_api.sh
    ├── test_notifications.py
    ├── test_postchain_multiturn.sh
    └── test_simulator_notifications.sh

73 directories, 240 files

=== File: docs/CHANGELOG.md ===



==
CHANGELOG.md
==


# Changelog
## [2025-04-28] - 2025-04-28

### Added

- **Mainnet Deployment:** Successfully deployed Choir to the Sui mainnet with package ID `0x4f83f1cd85aefd0254e5b6f93bd344f49dd434269af698998dd5f4baec612898::choir::CHOIR`.
- **Multiple Wallet Support:** Implemented support for multiple wallet accounts with horizontal scrolling in the Wallets tab.
- **Wallet & Thread Import/Export:** Added secure import and export functionality for wallets and threads with biometric protection.
- **Rewards System:** Implemented the full rewards system with:
  - **Novelty Rewards:** Users earn rewards for original content based on vector similarity scores.
  - **Citation Rewards:** Authors of cited content receive rewards when their contributions inform responses.
  - **Choir Coin Integration:** Connected to Sui blockchain for minting and distributing CHOIR tokens.
- **Improved Pagination:** Enhanced pagination system that preserves formatting across pages while maximizing content density.
- **Transaction Management:** Added a dedicated Transactions tab showing a chronological history of all transactions across wallets.
- **Citation Display:** Implemented early UI for displaying and interacting with citations in vector content.
- **Performance Optimization:** Improved app launch and navigation performance by loading only thread metadata initially and loading full content when needed.
- **Model Updates:** Added support for newer AI models and improved model configuration management.

### Changed

- **UI Redesign:** Completely redesigned interface with improved navigation flow and visual consistency.
- **Thread Management:** Enhanced thread persistence with wallet-specific thread storage and optimized loading.
- **Authentication Flow:** Improved authentication with biometric support (FaceID/TouchID) and passcode fallback.

## [2025-04-09] - 2025-04-09

### Added

- **iOS Client Persistence:** Implemented local JSON file storage for thread data.
- **Automatic Thread Titles:** Threads now get an auto-generated title based on the first 10 words of the initial AI Action phase response.
- **Close the Loop UI:** When the yield phase finishes downloading, if the user is viewing the action phase, the UI now automatically transitions to display the final response with a smooth wrap-around animation.


## [2025-03-28] - 2025-03-28

### Added

-   **PostChain Sequential Model Execution:** Implemented a prototype version of the PostChain running on a mobile device, successfully executing a sequence of 6 distinct AI models. This demonstrates the feasibility of the multi-phase workflow and shows initial promise for value generation.

### Changed

-   **Architectural Validation:** The sequential model execution validates the core concept of the PostChain flow. Next steps involve implementing background looping, Qdrant database integration for state persistence and memory, and connecting to the Sui service for reward distribution. These are considered tractable integration tasks.

## [2025-03-27] - 2025-03-27

### Changed

-   **Architectural Focus Shift: Qdrant-Sui MVP Prioritized**
    -   Refocused development efforts on a Minimum Viable Product (MVP) centered around **Qdrant** (data/vector store) and **Sui** (blockchain token/rewards).
    *   Adopted a streamlined architecture using the existing **Python API (FastAPI)** as the central orchestrator.
    *   Leveraging the current **LCEL-based PostChain workflow** (`langchain_workflow.py`) for MVP implementation speed.
    *   Defined clear data structures and interactions between the API, PostChain phases, Qdrant collections (`choir`, `users`, `chat_threads`, `intention_memory`, `observation_memory`), and the `sui_service.py`.
    *   Refined core documentation (`core_core.md`, `state_management_patterns.md`, `blockchain_integration.md`, `security_considerations.md`, `stack_argument.md`, `index.md`) to reflect the MVP scope and architecture.

### Deferred (Post-MVP)

-   Implementation of the full Model Context Protocol (MCP) server architecture.
-   Integration of client-side libSQL caching for offline support.
-   Deployment using Phala Network TEEs for confidential computing.
-   Implementation of the full dynamic economic model (MVP uses basic rewards).

## [Unreleased] - 2025-03-12

### Changed

-   **Major Architectural Pivot: Shifted from LangGraph to MCP Architecture**
    -   Transitioned to Model Context Protocol (MCP) architecture for the Choir platform.
    -   Adopted a service-oriented architecture with each PostChain phase implemented as a separate MCP server.
    -   Implemented MCP Resources for efficient conversation state management and context sharing.
    -   Leveraged MCP Notifications for real-time updates and communication between Host and Servers.
    -   Replaced LangGraph-based workflow orchestration with a Host-application-centric orchestration model using asynchronous tasks.
    -   Refined the focus on modularity, scalability, and security through the MCP architecture.

### Added

-   **Coherent Technology Stack for MCP Architecture:**
    -   **Model Context Protocol (MCP) Architecture:** Service-oriented architecture for PostChain phases, enabling modularity and scalability.
    -   **PySUI:** Maintained PySUI for blockchain integration and economic actions.
    -   **Pydantic:** Continued use of Pydantic for type safety and message validation in the MCP architecture.
    -   **FastAPI/Uvicorn:** Continued use of FastAPI/Uvicorn for the Python API layer, now orchestrating MCP server interactions.
    -   **Docker:** Maintained Docker for containerization and deployment of MCP servers.
    -   **Phala Network:** Maintained Phala Network for TEE-secured operations and confidential computing for MCP servers.

-   **Enhanced Token Economy and Reward System (RL-Driven CHOIR):**
    -   **CHOIR Coins as Training Signals for AI:** Evolved the CHOIR coin to act as training signals for AI models, driving a self-improving AI ecosystem.
    -   **Novelty and Citation Rewards:** Implemented novelty rewards for original prompts and citation rewards for salient contributions, algorithmically distributed by AI models.
    -   **Contract as Data Marketplace Foundation:** Defined the contract as the basis for a data marketplace within Choir, enabling CHOIR-based data access and contribution pricing.
    -   **Data Economy Vision:** Developed the vision for a comprehensive data marketplace where CHOIR serves as the currency for accessing and contributing to valuable datasets.

### Removed

-   Deprecated LangGraph dependency and graph-based state management due to scalability and maintenance concerns.

## [2025-02-25] - 2025-02-25

### Added

-   Implemented UI carousel to improve user experience
-   Added display of priors in the Experience step
-   Resumed active development after coding hiatus

### Planned

-   API streaming implementation to enhance responsiveness
-   Model reconfiguration for improved performance
-   Go multimodel, then multimodal
-   OpenRouter integration
-   Conceptual evolution from "Chorus Cycle" to "Post Chain"
    -   Representing shift from harmonic oscillator (cycle) to anharmonic oscillator (chain)
    -   Aligning interface terminology with underlying model
-   Client-side editable system prompts for customization
-   Additional phases in the Post Chain:
    -   Web search phase for real-time information access
    -   Sandboxed arbitrary tool use phase for enhanced capabilities

## [2025-02-24] - 2025-02-24

### Changed

-   Implemented fractional quantum anharmonic oscillator model for dynamic stake pricing
-   Added fractional parameter α to capture memory effects and non-local interactions
-   Revised parameter modulation formulas for K₀, α, and m to reflect interdependencies
-   Created simulation framework for parameter optimization

## [2025-02-23] - 2025-02-23

### Changed

-   Documented quantum anharmonic oscillator model implementation and dynamic stake pricing mechanism via an effective anharmonic coefficient modulated by approval/refusal statistics.

## [Unreleased]

### Changed

-   Updated all documentation to version 6.0
    -   Transformed structured documentation into fluid prose
    -   Relaxed event-driven architecture requirements for initial TestFlight
    -   Clarified implementation priorities and post-funding features
    -   Maintained theoretical frameworks while focusing on core functionality

### Added

-   Initial Chorus cycle working in iOS simulator
    -   Basic message flow through phases
    -   Response handling
    -   State management

### Documented

-   Created 15 comprehensive issues covering:
    -   Core message system implementation
    -   Type reconciliation with Qdrant
    -   API client updates
    -   Coordinator message flow
    -   User identity management
    -   Thread state management
    -   Integration testing
    -   Error handling strategy
    -   Performance monitoring
    -   State recovery
    -   Thread sheet implementation
    -   Thread contract implementation
    -   Message rewards system
    -   LanceDB migration
    -   Citation visualization

### Architecture

-   Defined clear type system for messages
-   Planned migration to LanceDB
-   Structured multimodal support strategy

### Technical Debt

-   Identified areas needing more specification:
    -   Thread Sheet UI (marked as "AI SLOP")
    -   Reward formulas need verification
    -   Migration pipeline needs careful implementation

## [0.4.2] - 2024-11-09

### Added

-   Development principles with focus on groundedness
-   Basic chat interface implementation
-   SwiftData message persistence // this subsequently became a problem. swiftdata is coupled with swiftui and there was interference between view rendering and data persistence
-   Initial Action step foundation

### Changed

-   Shifted to iterative, ground-up development approach
-   Simplified initial implementation scope
-   Focused on working software over theoretical architecture
-   Adopted step-by-step Chorus Cycle implementation strategy

### Principles

-   Established groundedness as core development principle
-   Emphasized iterative growth and natural evolution
-   Prioritized practical progress over theoretical completeness
-   Introduced flexible, evidence-based development flow

## [0.4.1] - 2024-11-08

### Added

-   Self-creation process
-   Post-training concepts
-   Concurrent processing ideas
-   Democratic framing
-   Thoughtspace visualization

### Changed

-   Renamed Update to Understanding
-   Enhanced step descriptions
-   Refined documentation focus
-   Improved pattern recognition

## [0.4.0] - 2024-10-30

### Added

-   Swift architecture plans
-   Frontend-driven design
-   Service layer concepts
-   Chorus cycle definition

### Changed

-   Enhanced system architecture
-   Refined core patterns

## [0.3.5] - 2024-09-01

-   Choir.chat as a web3 dapp
-   messed around with solana
-   used a lot of time messing with next.js/react/typescript/javascript
-   recognized that browser extension wallet is terrible ux

## [0.3.0] - 2024-03-01

### Added

-   ChoirGPT development from winter 2023 to spring 2024

-   First developed as a ChatGPT plugin, then a Custom GPT
-   The first global RAG system / collective intelligence as a GPT

## [0.2.10] - 2023-04-01

### Added

-   Ahpta development from winter 2022 to spring 2023

## [0.2.9] - 2022-04-01

### Added

-   V10 development from fall 2021 to winter 2022

## [0.2.8] - 2021-04-01

### Added

-   Elevisio development from spring 2020 to spring 2021

## [0.2.7] - 2020-04-01

### Added

-   Bluem development from spring 2019 to spring 2020

## [0.2.6] - 2019-04-01

### Added

-   Blocstar development from fall 2018 to spring 2019

## [0.2.5] - 2018-04-01

### Added

-   Phase4word development from summer 2017 to spring 2018

### Changed

-   Showed Phase4word to ~50 people in spring 2018, received critical feedback
-   Codebase remains in 2018 vintage

## [0.2.0] - 2016-06-20

### Added

-   Phase4 party concept
-   Early democracy technology
-   Initial value systems

### Changed

-   Moved beyond truth measurement framing
-   Refined core concepts

## [0.1.0] - 2015-07-15

### Added

-   Initial simulation hypothesis insight
-   "Kandor"
-   Quantum information concepts
-   Planetary coherence vision
-   Core system ideas

=== File: docs/scripts/combiner.sh ===



==
combiner.sh
==


#!/bin/bash

# Revised prefix arrays
level0_prefixes=("")  # Basic technical integration
level1_prefixes=("core" "requirements")  # Core system components
level2_prefixes=("e")           # Business/concept/implementation
level3_prefixes=("plan")               # Plans
level4_prefixes=("fqaho")     # Simulations
level5_prefixes=("evolution" "data")             # Foundational principles

# Function to add separator and header
add_separator() {
    echo -e "\n"
    echo "=="
    echo "$1"
    echo "=="
    echo -e "\n"
}

# Function to get level for a file
get_level_for_file() {
    filename=$(basename "$1")
    prefix=$(echo "$filename" | cut -d'_' -f1)

    for p in "${level0_prefixes[@]}"; do [[ "$prefix" == "$p" ]] && echo 0 && return; done
    for p in "${level1_prefixes[@]}"; do [[ "$prefix" == "$p" ]] && echo 1 && return; done
    for p in "${level2_prefixes[@]}"; do [[ "$prefix" == "$p" ]] && echo 2 && return; done
    for p in "${level3_prefixes[@]}"; do [[ "$prefix" == "$p" ]] && echo 3 && return; done
    for p in "${level4_prefixes[@]}"; do [[ "$prefix" == "$p" ]] && echo 4 && return; done
    for p in "${level5_prefixes[@]}"; do [[ "$prefix" == "$p" ]] && echo 5 && return; done

    echo -1
}

# Function to process files for a level
process_level() {
    level=$1
    output_file="docs/levels/level${level}.md"

    echo "# Level ${level} Documentation" > "$output_file"
    echo -e "\n" >> "$output_file"

    SPECIAL_FILES=("docs/prompt_wake_up.md" "docs/prompt_getting_started.md" "docs/prompt_reentry.md" "docs/prompt_organization.md" "docs/prompt_summary_prompt.md" "docs/prompt_chorus_cycle.md" "docs/tree.md" "docs/CHANGELOG.md" "docs/scripts/combiner.sh")

    # Level 0 now includes important system files (previously in level -1)
    if [ "$level" -eq 0 ]; then
        # Add system files (previously in level -1)
        for special_file in "${SPECIAL_FILES[@]}"; do
            if [ -f "$special_file" ]; then
                echo -e "\n=== File: $special_file ===\n" >> "$output_file"
                add_separator "$(basename "$special_file")" >> "$output_file"
                cat "$special_file" >> "$output_file"
                echo "$special_file" >> "/tmp/processed_files.txt"
            fi
        done

    fi

    # Process all docs to find ones for this level
    for file in docs/*.md; do
        if [ -f "$file" ] && [ "$(get_level_for_file "$file")" -eq "$level" ]; then
            echo -e "\n=== File: $file ===\n" >> "$output_file"
            add_separator "$(basename "$file" .md)" >> "$output_file"
            cat "$file" >> "$output_file"
            echo "$file" >> "/tmp/processed_files.txt"
        fi
    done
}

# Create temporary file for tracking
touch /tmp/processed_files.txt

# Process all levels (excluding level -1 as its content is now in level 0)
echo "Processing documentation..."
for level in {0..5}; do
    process_level $level
done

# Concatenate all levels into a single file
echo "Combining all levels into one file..."
mkdir -p docs/levels
cat docs/levels/level{0..5}.md > docs/levels/all.txt

# Check for uncategorized files
echo -e "\nUncategorized files:"
uncategorized=0
for doc in docs/*.md; do
    if ! grep -q "^$doc$" "/tmp/processed_files.txt"; then
        echo "$doc"
        uncategorized=$((uncategorized + 1))
        # Append uncategorized files to all.txt
        echo -e "\n=== File: $doc ===\n" >> docs/levels/all.txt
        add_separator "$(basename "$doc" .md)" >> docs/levels/all.txt
        cat "$doc" >> docs/levels/all.txt
    fi
done

if [ "$uncategorized" -gt 0 ]; then
    echo -e "\nTotal uncategorized: $uncategorized files"
fi

# Cleanup
rm -f "/tmp/processed_files.txt"

echo "Documentation combination complete"
# Level 1 Documentation



=== File: docs/core_core.md ===



==
core_core
==


# Core System Overview: AI for Social Discourse

VERSION core_system: 9.0 (Relationship-Focused Architecture)

## Overview

Choir is a language game that uses AI to facilitate meaningful social discourse. Our system architecture is designed around a core insight: turning posting from a liability into value creation through relationship staking and merit-based rewards. The MVP validates this concept through **Qdrant** for semantic understanding, **Sui** for economic alignment via CHOIR tokens, and a **Python API** that orchestrates AI-driven conversations that connect like minds.

## Foundational Principles: Enabling Human Connection Through AI

Choir's architecture embodies core principles that prioritize human relationships over algorithmic engagement:

1.  **Economic Alignment (Sui):** The **Sui blockchain** manages CHOIR tokens that create real skin in the game for quality discourse. Beyond basic rewards, it enables **relationship staking** where users invest tokens in meaningful connections, creating shared economic interest in maintaining quality relationships.
2.  **Semantic Understanding (Qdrant):** **Qdrant** stores not just content but the semantic relationships that help AI identify intellectual compatibility. It powers the discovery of "like minds" through citation patterns and conversation quality rather than superficial metrics.
3.  **Conversational Intelligence (AEIOU-Y PostChain):** The **PostChain workflow** creates conversations that get smarter over time, helping users express ideas more clearly and connecting them with relevant prior thoughts from the community.
4.  **Merit-Based Discovery:** Anonymous by default, ideas compete on merit rather than social status, enabling authentic discourse free from social surveillance and reputation management.

## Core Components: Building Blocks for Social Discourse

1.  **Qdrant (Semantic Relationship Engine):**
    *   **Role:** Powers the discovery of intellectual compatibility and meaningful connections. Stores conversation content, user interaction patterns, and the semantic relationships that enable AI to identify "like minds."
    *   **Function:** Enables semantic search for relevant prior thoughts, calculates novelty scores for original contributions, and provides the data foundation for relationship recommendations based on citation patterns and conversation quality.

2.  **Sui Blockchain (Economic Alignment Layer):**
    *   **Role:** Manages CHOIR tokens that create economic alignment in relationships. Handles both individual rewards and the upcoming relationship staking features that enable users to invest in meaningful connections.
    *   **Function:** Executes reward distributions for quality contributions, manages relationship multisigs for staked connections, and provides the economic infrastructure for a platform where social value belongs to users, not the platform.

3.  **Python API (Conversation Orchestrator):**
    *   **Role:** The central intelligence that connects human input with AI processing, semantic understanding, and economic rewards.
    *   **Function:** Authenticates users through Sui signatures, orchestrates the PostChain workflow that makes conversations smarter over time, and triggers both individual rewards and relationship-based economic interactions.

4.  **PostChain Workflow (Conversational Intelligence):**
    *   **Role:** The AI system that helps users express ideas clearly and connects them with relevant community knowledge.
    *   **Function:** Processes conversations through multiple phases to identify valuable insights, find relevant prior contributions, and create responses that facilitate meaningful discourse rather than mere information exchange.

5.  **Supporting Technologies:**
    *   **Langchain Utils (`langchain_utils.py`):** LLM abstraction.
    *   **Pydantic:** Data validation.
    *   **Docker:** API containerization.
    *   **SwiftUI & Keychain:** Client UI and secure Sui key storage.
    *   **Python Async/await:** Used within the API and LCEL workflow for efficient concurrent operations.

## Architecture Flow: From Thought to Connection

The system creates a flow that transforms individual thoughts into community connections:

1.  **Authentic Expression**: User shares thoughts via **SwiftUI Client** with Sui-based authentication ensuring ownership.
2.  **AI Enhancement**: **Python API** orchestrates the **PostChain Workflow** to help clarify and contextualize the user's ideas.
3.  **Semantic Discovery**: PostChain phases interact with **Qdrant** to find relevant prior thoughts and identify potential intellectual connections.
4.  **Quality Recognition**: AI calculates novelty and citation scores, identifying valuable contributions worthy of rewards.
5.  **Community Building**: Final response includes not just AI insights but potential connection points with like-minded users.
6.  **Economic Alignment**: **Sui Service** distributes rewards and enables relationship staking for meaningful connections.
7.  **Relationship Formation**: Users can invest earned tokens in relationships, creating shared economic interest in quality discourse.

This architecture validates the core insight: **Individual Thought -> AI Enhancement -> Community Discovery -> Economic Alignment -> Meaningful Relationships**.

## Strategic Focus: Validating Social Discourse Through AI

*   **Relationship Discovery:** Validate that AI can identify intellectual compatibility and facilitate meaningful connections between users.
*   **Economic Alignment:** Establish that token-based incentives create better discourse quality and relationship formation.
*   **Merit-Based Community:** Demonstrate that anonymous, merit-based interactions lead to more authentic and valuable conversations.
*   **Value Ownership:** Prove that users can own and transfer their social value rather than being locked into platform-specific metrics.

## The Combined Result: AI That Amplifies Human Community

The MVP delivers a new paradigm for online interaction:

*   **Economic Relationships (CHOIR tokens):** Users invest in connections, creating shared stakes in relationship quality and discourse outcomes.
*   **Semantic Compatibility (Qdrant):** AI identifies like minds through conversation patterns rather than demographic or behavioral targeting.
*   **Enhanced Expression (PostChain):** Conversations become collaborative intelligence sessions that help users articulate and develop ideas.
*   **Authentic Community:** Anonymous merit-based interactions free from social surveillance enable genuine intellectual connection.

This architecture demonstrates that AI can facilitate human relationships rather than replace them, creating a platform where technology serves community building rather than attention extraction.

=== File: docs/core_economics.md ===



==
core_economics
==


# Core Economic Model: Turning Social Interaction Into Value Creation

VERSION core_economics: 9.0 (Relationship-Focused Economy)

The economic model of Choir solves a fundamental problem: on traditional social media, posting creates liability (cancel culture, reputation risk) while platforms capture all the value. Choir flips this by turning every thoughtful contribution into transferable value through CHOIR tokens, and enabling users to invest that value in meaningful relationships. This creates the first social platform where your intellectual contributions belong to you.

## CHOIR: The Currency of Meaningful Relationships

The CHOIR coin represents a fundamental shift from platform-owned metrics to user-owned value:

*   **Transferable Social Value:** Unlike likes, followers, or karma that disappear when you leave a platform, CHOIR tokens are yours to keep, transfer, or invest in relationships that matter.
*   **Relationship Investment Currency:** CHOIR tokens enable relationship staking - the ability to invest your earned value directly in meaningful connections with other users, creating shared economic interest in quality discourse.
*   **Merit-Based Rewards:** Tokens are earned through quality contributions (novelty rewards) and community recognition (citation rewards), not engagement farming or algorithmic manipulation.
*   **Economic Alignment Tool:** By requiring token investment for relationship formation, CHOIR creates real skin in the game for meaningful discourse, filtering out low-effort interactions while rewarding thoughtful engagement.

## Relationship Staking: Economic Alignment in Human Connections

The relationship staking system creates economic alignment between users who want to form meaningful connections:

*   **Investment-Based Connections:** When you want to respond to someone's thoughtful contribution, you stake CHOIR tokens as a non-refundable bond, demonstrating serious intent and filtering out spam.
*   **Mutual Economic Interest:** If both parties engage, their tokens are locked in a shared relationship multisig, creating joint ownership of the relationship's economic value.
*   **Dynamic Relationship Value:** Successful relationships can accumulate additional value through citation rewards when relationship content is referenced, and novelty rewards for collaborative insights.
*   **Exit Rights and Ownership:** Users always maintain the right to exit relationships and take their proportional share of tokens, ensuring that social value remains owned by participants, not platforms.

## Reward Mechanisms: Recognizing Quality and Building Community

The CHOIR economy rewards two types of valuable contributions that build better discourse:

1.  **Novelty Rewards - Rewarding Original Thinking:**
    *   **Purpose:** Recognize and reward users who contribute genuinely original ideas and perspectives, encouraging intellectual diversity and creative thinking.
    *   **Mechanism:** AI analyzes new contributions for semantic novelty compared to existing community knowledge, identifying truly fresh insights.
    *   **Impact:** Creates incentives for users to think deeply and share authentic perspectives rather than repeating common talking points or engagement farming.

2.  **Citation Rewards - Recognizing Community Value:**
    *   **Purpose:** Reward users whose contributions prove valuable to others, as demonstrated when their ideas are referenced in subsequent conversations.
    *   **Mechanism:** When AI identifies that a user's prior contribution informed a response to someone else, the original author receives citation rewards.
    *   **Impact:** Creates a reputation system based on actual intellectual contribution rather than social metrics, encouraging users to share insights that genuinely help others.

These mechanisms work together to create a community where quality thinking is recognized and rewarded, while AI learns to identify the types of contributions that facilitate meaningful discourse and intellectual connection.

## Data Marketplace Dynamics: CHOIR as Data Purchase Power

The CHOIR coin economy creates a dynamic **data marketplace** within Choir, where:

*   **CHOIR Coins are the Currency of Data Access:** AI companies, researchers, developers, and even individual users who want to access the high-quality, human-generated data within Choir must **purchase CHOIR coins** to participate in the data marketplace.
*   **Data is "Sold" at a Granular Level (Thread-Specific Contracts):** Data access and contribution pricing are governed by the contract at a granular, thread-specific level. Each thread effectively has its own "data contract" that determines the terms of data access and contribution within that thread.
*   **Data Scarcity and Privacy Drive Value:** The deliberate emphasis on **data scarcity and user privacy** within Choir is a key driver of CHOIR coin value.  By limiting data sales and prioritizing user control, Choir creates a marketplace for *premium, high-quality, and ethically sourced data*, which is increasingly valuable in the AI age.
*   **CHOIR Holder Governance of Data Marketplace Terms:** CHOIR coin holders have **governance rights to shape the rules and policies of the data marketplace**, ensuring that it remains aligned with the community's values and long-term interests.

## Business Sustainability and the Data Economy Model

The CHOIR coin economy is designed to create a **self-sustaining ecosystem** where value flows naturally and benefits all participants. The Data Marketplace and the IDaaS premium features are key components of the business model, designed to:

*   **Drive CHOIR Coin Demand and Utility:** Create tangible use cases for CHOIR coins, increasing their demand and utility beyond just platform-internal rewards.
*   **Generate Revenue to Support Platform Operations:** Revenue from IDaaS subscriptions and data marketplace transaction fees will fund the ongoing development, maintenance, and operational costs of the Choir platform and the coin economy.
*   **Value Proposition for Users:** The Choir ecosystem is designed to provide value to users through:
    *   **Financial Rewards for Quality Contributions:** Earn CHOIR coins for novel ideas and cited content.
    *   **Access to a Thriving Data Marketplace:** Exchange valuable data and insights.
    *   **Enhanced Identity and Reputation:** Build credibility through the IDaaS offering.

## Conclusion: A New Model for Social Value Creation

The core economic model of Choir represents a fundamental shift from extractive to generative social platforms:

*   **User Ownership of Social Value:** For the first time, users own their social contributions as transferable assets rather than platform-locked metrics that disappear when they leave.
*   **Economic Alignment in Relationships:** Relationship staking creates shared economic interest in maintaining quality discourse, transforming social interaction from cost center to value generator.
*   **Merit-Based Community Building:** Anonymous, merit-based rewards enable authentic intellectual connection free from social surveillance and reputation management.
*   **AI That Amplifies Human Connection:** Rather than replacing human relationships, AI facilitates better discourse and helps compatible minds find each other based on intellectual compatibility rather than demographic targeting.

This model demonstrates that social platforms can create value for users rather than extracting it, building communities based on shared intellectual interest rather than engagement addiction.
# Level 2 Documentation



=== File: docs/e_business.md ===



==
e_business
==


# File: docs/e_business.md

# Choir Business Model: Truthful Mechanism Design for Social Discourse

Choir's business model emerges from deep thinking about mechanism design theory. At its core, Choir is what happens when you design a DSIC (Dominant Strategy Incentive Compatible) truthful mechanism for social media, treating posting as bidding in a Vickrey auction where authentic expression is the dominant strategy.

## The Revenue Sequence: App → Relationships → Finance

Our business model follows a clear progression that aligns with user value creation:

**App Subscriptions** → **Relationship/Group Multisig Funds** → **Onchain Finance Agents** (with transaction fees)

This sequence ensures that revenue grows naturally with user engagement and relationship formation, creating sustainable alignment between business success and community value.

## Phase 1: App Subscription Model

**Free Tier (Beta Only):**
During beta, core functionality is free to validate product-market fit:

*   **Anonymous Discourse:** Full access to AI-enhanced conversations with complete anonymity
*   **CHOIR Coin Earning:** Earn tokens for novel contributions and citations
*   **Relationship Staking:** Invest earned tokens in meaningful connections
*   **Basic AI Processing:** Standard PostChain workflow with rate limits

**Subscription Tiers (Post-Beta):**
Exponential price-to-rate-limit ratios ensure sustainable AI model costs and Sui transaction fees:

*   **Basic ($9/month):** 100 AI conversations, standard processing speed
*   **Pro ($29/month):** 500 conversations, priority processing, advanced AI models
*   **Expert ($99/month):** 2000 conversations, fastest processing, premium model access
*   **Unlimited ($299/month):** No rate limits, experimental features, dedicated support

**Full Anonymity by Design:**
Identity emerges naturally from wallet activity and Sui's built-in nameserver. No persistent identities or KYC required - reputation is built through contribution quality, not social credentials.

## Phase 2: Relationship/Group Multisig Funds

As users form meaningful relationships through staking, these connections become economic entities:

**Relationship Multisigs:**
*   **Shared Treasury:** Successful relationships accumulate CHOIR tokens through collaborative insights and citations
*   **Investment Decisions:** Groups can collectively decide how to deploy their shared funds
*   **Value Appreciation:** Relationship funds grow through continued quality discourse and community recognition

**Group Formation:**
*   **Natural Evolution:** Pair relationships expand into groups with shared economic interests
*   **Collective Intelligence:** Groups become economic entities that can make investment decisions together
*   **Exit Rights:** Members can always leave and take their proportional share

## Phase 3: Onchain Finance Agents (Revenue Generation)

The ultimate revenue driver: providing AI-powered financial services to relationship/group funds:

**Financial Agent Services (2-5% transaction fees):**
*   **Portfolio Management:** AI agents help groups optimize their CHOIR and other crypto holdings
*   **DeFi Strategies:** Automated yield farming, liquidity provision, and risk management
*   **Cross-Chain Operations:** Bridge assets and execute strategies across multiple blockchains
*   **Tax Optimization:** AI-driven strategies for minimizing tax burden on group investments

**Revenue Model:**
*   **Transaction Fees:** 2-5% on all financial operations executed by our AI agents
*   **Management Fees:** Annual fees for ongoing portfolio management services
*   **Performance Fees:** Share of profits generated above benchmark returns

## CHOIR Coin Treasury Strategy & Financial Engineering

**Buyback Program:**
CHI allocates significant revenue to purchasing CHOIR tokens from the open market, creating sustained buy pressure and aligning corporate success with token value appreciation.

**Debt Financing Strategy:**
Once recurring revenue is established (Phase 1), CHI can leverage its cash flows to take on debt for accelerated CHOIR acquisitions, similar to corporate treasury strategies but focused on our native ecosystem token rather than external assets.

**Strategic Rationale:**
*   **Ecosystem Alignment:** Corporate success directly tied to CHOIR token performance
*   **Market Support:** Consistent buy pressure supports token price stability and growth
*   **Financial Engineering:** Debt-financed buybacks amplify returns when token appreciates
*   **User Benefit:** Token holders benefit from corporate treasury strategy

**Risk Management:**
*   **Revenue Diversification:** Multiple revenue streams reduce dependence on any single source
*   **Conservative Leverage:** Debt levels maintained well below cash flow capacity
*   **Market Timing:** Buyback intensity adjusted based on market conditions and cash flow

## Mechanism Design Theory Foundation

**DSIC (Dominant Strategy Incentive Compatible) Social Media:**
Choir represents the practical implementation of mechanism design theory applied to social discourse. Traditional social media creates perverse incentives where engagement farming and controversy generation are rewarded. Choir creates a truthful mechanism where authentic expression is the dominant strategy.

**Vickrey Auction Analogy:**
*   **Bidding = Posting:** Users "bid" their authentic thoughts and insights
*   **Truthful Bidding = Authentic Expression:** The reward structure makes honest contribution the optimal strategy
*   **Second-Price Auction = Citation Rewards:** Value flows to contributors based on community recognition, not self-promotion
*   **Sealed Bid = Anonymity:** Anonymous contribution removes social signaling and status games

**Incentive Alignment:**
*   **Individual Rationality:** Users benefit more from authentic contribution than gaming the system
*   **Strategy-Proofness:** Honest expression dominates all other strategies
*   **Efficiency:** Resources (attention, rewards) flow to highest-value contributions
*   **Revenue Equivalence:** Multiple equilibria all lead to quality discourse outcomes

This theoretical foundation ensures that Choir's economic incentives naturally align with the goal of meaningful social discourse, making it a sustainable and self-reinforcing system.

**Value Creation Flows - A Multi-Layered Ecosystem:**

Value creation in Choir flows through multiple interconnected layers:

*   **Individual Level:**
    *   **Recognition & CHOIR Rewards:** Earn CHOIR coins for novel and salient contributions via the LoT process.
    *   **Path to Enhanced Services:** Use CHOIR to access premium data marketplace services and participate in Thread Contracts.
    *   **Identity Options:** Choose between anonymous participation or premium identity services based on your needs.
    *   **Natural Reputation:** Build influence through the quality and citation of ideas, with or without a persistent identity.

*   **Team Level (Threads and Co-authorship):**
    *   **Collective Value Accumulation:** Threads become shared spaces where value (knowledge, potential CHOIR rewards via citations) accumulates collectively.
    *   **Shared Success:** Threads gain recognition and value through citations, creating network effects.
    *   **Natural Team Formation:** Teams organically form around valuable threads and shared goals.

*   **Network Level (Choir Ecosystem):**
    *   **Knowledge Networks:** Citations create a growing web of interconnected knowledge.
    *   **Value Flows:** CHOIR coins flow via rewards, data marketplace transactions, and thread staking, creating a dynamic economy.
    *   **Collective Intelligence Emergence:** The interplay of LoT interactions, CHOIR incentives, and network effects fosters emergent collective intelligence.
    *   **Sustainable Ecosystem Growth:** The CHOIR coin economy, anchored by the data marketplace and IDaaS utility, aims for self-sustaining growth.

**Resource Allocation - Natural and Scalable:**

Resource allocation within Choir scales naturally:

*   **Processing Resources (AI Compute):** Standard access for free tier participation. Enhanced/prioritized access linked to IDaaS subscription or high CHOIR staking/activity.
*   **Storage Resources (Data Persistence):** Sufficient storage for free tier contributions. Enhanced storage or backup options for premium users.
*   **Network Resources (Real-Time Communication):** Standard access for free tier. Enhanced bandwidth/priority for premium users and high-value threads.

**Growth and Evolution - Organic and Sustainable:**

Choir's growth is designed to be organic and sustainable, driven by natural amplification mechanisms:

*   **Quality Emergence:** LoT and CHOIR rewards incentivize high-quality contributions.
*   **Team Formation:** Collaboration emerges around valuable threads.
*   **Network Effects:** Value increases as more users participate and connect ideas.
*   **Data Marketplace:** The data marketplace provides a scalable utility for CHOIR and drives ecosystem growth.

*   **Scaling Orientation via Global Context:** Crucially, Choir's ability to Orient (a key phase in the Loop of Thought) scales directly with user participation. The global vector database, populated by user contributions, grows richer and more comprehensive as the network expands. This means that as more users contribute diverse ideas and create valuable citations, the system's collective ability to retrieve relevant context, understand nuances, and inform better decisions improves for everyone. This creates a powerful positive feedback loop: more users generate better data, which improves the AI's orientation capabilities (via enhanced Experience phase retrieval), leading to better interactions and rewards, thus attracting more users. It's a network effect combined with an AI training feedback loop – the system improves its means of improving as it scales.

*   **Resource Evolution Supports Scaling:** Resource allocations naturally evolve to support platform growth, with individual allocations expanding (through premium services), team capabilities growing, and network capacity increasing to accommodate increasing user activity.


**Future Evolution - Expanding Capabilities and User Empowerment:**

Future evolution will focus on:

*   **Sophisticated Thread Contracts:** Enabling CHOIR staking for participation and investment in specific ideas/teams.
*   **Advanced Agent Frameworks:** Leveraging the LoT architecture for more powerful autonomous agents.
*   **Knowledge Tools:** Developing tools for navigating and analyzing the emergent knowledge network.
*   **Data Marketplace Expansion:** Enhancing the data marketplace with more sophisticated pricing, licensing, and discovery mechanisms.
*   **Governance Mechanisms:** Implementing CHOIR-based governance for platform decisions and resource allocation.

**Implementation Strategy - Phased and Iterative:**

Our implementation follows a natural progression:

1.  **Foundation Phase (MVP Focus):** Establish core LoT functionality on the Choir platform, initiate CHOIR coin distribution via basic rewards, build the initial community based on the vision.
2.  **Enhancement Phase (Thread Contract):** Introduce on-chain Thread Contracts, enabling CHOIR staking for participation and adding a software-based utility sink.
3.  **Marketplace Phase (Data Economy):** Develop and launch the Choir Data Marketplace, establishing a primary CHOIR utility and enabling the data economy.
4.  **Governance Phase (Community Control):** Implement governance mechanisms allowing CHOIR holders to participate in platform decisions.

**Success Metrics - Measuring Natural Growth and Value Creation:**

Success is measured by metrics reflecting ecosystem health and value creation:

*   **Quality Metrics:** Citation rates, novelty scores, thread depth, user retention based on interaction quality.
*   **Economic Metrics:** CHOIR coin velocity (marketplace transactions, IDaaS subscriptions), distribution fairness, data marketplace activity.
*   **Network Metrics:** Growth in active users, thread creation/interconnection, emergence of valuable knowledge clusters.
*   **Health Metrics:** System performance, resource efficiency, resilience against reward hacking.

**Conclusion - Truthful Mechanisms for Sustainable Social Discourse:**

Choir's business model represents the first practical implementation of mechanism design theory for social media. By creating a DSIC truthful mechanism where authentic expression is the dominant strategy, we solve the fundamental problems of traditional social platforms: engagement farming, controversy generation, and value extraction.

Our three-phase revenue model (App → Relationships → Finance) ensures sustainable growth that aligns with user value creation. The CHOIR buyback strategy, potentially enhanced with debt financing, creates a powerful flywheel where corporate success directly benefits token holders and the broader community.

This isn't just a business model - it's a proof of concept that technology can serve human flourishing rather than exploit human psychology. By building economic incentives that reward truth-telling and meaningful connection, we're creating a template for how social platforms should operate in the 21st century.

=== File: docs/e_concept.md ===



==
e_concept
==


# Choir: A Harmonic Intelligence Platform - Where Knowledge Resonates and Value Flows

Choir is more than just a platform; it's a **harmonic intelligence ecosystem**, a digital space designed to amplify human potential and unlock new forms of collective understanding.  Imagine a place where ideas resonate like musical notes, where collaboration flows like a river, and where knowledge crystallizes into structures of lasting value – this is the essence of Choir.

**Natural Value Flows - Like Energy Through a System:**

At its core, Choir operates on the principle of **natural value flows**.  Just as energy seeks the path of least resistance and water finds its level, value in Choir flows organically towards quality, insight, and meaningful contribution.  This is not a system of forced metrics or artificial incentives, but one where value emerges naturally from the inherent dynamics of the platform.

*   **Individual Recognition - Organic and Tangible:**  Recognition for valuable contributions is immediate and tangible, like a clear note resonating in a concert hall. Quality insights naturally attract attention and rewards, driven by the platform's inherent mechanisms, not arbitrary likes or upvotes. Value recognition is earned through genuine participation and meaningful stake.
*   **Team Crystallization - Natural Alignment of Minds:**  Teams in Choir form organically, like crystals forming in a solution.  Valuable conversations naturally attract compatible minds, creating teams based on shared interests, complementary skills, and a collective drive to build knowledge. Threads become shared spaces where value accumulates for all participants, forging natural bonds between contributors.
*   **Knowledge Networks - Interconnected Ecosystems of Understanding:**  Threads in Choir don't exist in isolation; they connect and interweave through citations, creating **knowledge networks** that resemble natural ecosystems.  Value flows between threads and communities, like streams feeding into rivers and oceans, creating a rich and interconnected web of understanding. Each citation strengthens both the source and destination threads, building a network of long-range correlations and emergent insights.

**Evolving Through Natural Phases - Mirroring Physical Processes:**

Choir's evolution mirrors natural physical processes, unfolding through distinct phases:

*   **Emergence Phase (New Threads - Bubbling with Possibility):** New threads begin with a burst of energy and potential, like a hot spring bubbling to the surface.  Energy is high, stakes are elevated, and participation requires initial commitment, creating a natural quality filter from the outset.
*   **Flow Phase (Mature Threads - Finding Their Course):** As threads mature, they "cool" into more stable states, like a river finding its course. The flow of conversation becomes more predictable, stakes moderate to increase accessibility, while quality is maintained through established patterns and community norms.
*   **Crystallization Phase (Mature Threads - Stable and Valuable Structures):**  Mature threads develop clear structures, like crystalline formations. Teams coalesce around valuable patterns, knowledge networks form clear topologies, and value accumulates in stable, beautiful, and lasting ways.

**Value Accumulation - Beyond Extraction, Towards Amplification:**

Unlike traditional platforms that often extract value from users, Choir creates spaces where value **naturally accumulates and amplifies** through multiple channels:

*   **Threads as Resonant Cavities:** Threads act as resonant cavities, accumulating energy and value through high-quality interactions and insightful contributions.
*   **Denials as Strengthening Forces:**  Even "denials" (disagreements, challenges) within the PostChain workflow are not wasted energy; they serve to strengthen the thread itself, refining ideas and improving the overall quality of knowledge.
*   **Teams Share in Thread Value Growth:** Teams of co-authors share in the growing value of their threads, creating a direct incentive for collaboration and collective success.
*   **Network Value Through Citations:** Network value grows exponentially as citations create flows between threads, knowledge networks emerge organically, teams build on each other's work, and system-wide coherence develops naturally.
*   **Sustainable Treasury - Perpetual Value Flow:** The Choir treasury maintains a sustainable value flow by capturing value from split decisions and funding ongoing citation rewards, enabling perpetual rewards that benefit the entire ecosystem and ensure long-term viability.

**Dynamic Stake Evolution - Natural Quality Filters with Memory Effects:**

Choir's dynamic stake evolution, driven by the economic model, creates **natural quality filters with built-in memory effects**:

*   **Dynamic Economic Model:** The model, with its evolving parameters, dynamically adjusts stake prices based on thread history, community feedback, and network position.
*   **Dynamic Stake Pricing - Natural Price Discovery:** Stake prices emerge naturally through the system, reflecting the evolving value and quality of each thread.
*   **Memory Effects Through Fractional Parameter (α):** The fractional parameter α captures how threads develop "memory" over time, with past interactions and community feedback influencing current stake prices and value distribution.
*   **Lévy Flight-Like Value Propagation:** Value propagates through the network in Lévy flight-like patterns, reflecting the non-local nature of knowledge creation and the potential for occasional breakthrough insights to generate disproportionate impact across the ecosystem.

**The Future of Collaborative Intelligence - Emergent, Sustainable, and User-Empowering:**

Choir's vision extends beyond a mere platform; it's a step towards a new era of **collaborative intelligence**:

*   **Natural Teams Form Around Resonant Ideas:**  Teams form organically around compelling ideas, driven by shared interests and a collective desire to build knowledge together.
*   **Shared Value and Collective Ownership:**  Teams share in the collective value they create, fostering a sense of ownership and shared purpose.
*   **Building on Each Other's Work - Iterative Knowledge Refinement:**  Teams and threads build upon each other's work, creating a continuous cycle of knowledge refinement and expansion.
*   **Evolving Sustainably - Organic Growth and Adaptation:**  The Choir ecosystem evolves organically and sustainably, driven by natural patterns of collaboration, value flow, and emergent intelligence.

Choir is more than just a communication tool; it's a **platform for human potential to resonate, collaborate, and create knowledge in harmony with AI.**  Join us in building a future where quality emerges naturally, teams form organically, and value flows to those who create it – a future where collective intelligence becomes a tangible force for positive change in the world.
# Level 3 Documentation



=== File: docs/plan_anonymity_by_default.md ===



==
plan_anonymity_by_default
==


==
anonymity_by_default.md
==

# Anonymity by Default: A Core Principle of Choir

VERSION anonymity_by_default: 7.0

Anonymity is not just a feature of Choir; it's a fundamental principle, a design choice that shapes the platform's architecture and informs its values. By making anonymity the default state for all users, Choir prioritizes privacy, freedom of expression, and the creation of a space where ideas are judged on their merits, not on the identity of their author.

**Core Tenets:**

1. **Privacy as a Fundamental Right:** Choir recognizes that privacy is a fundamental human right, essential for individual autonomy and freedom of thought. Anonymity protects users from surveillance, discrimination, and the potential chilling effects of being constantly identified and tracked online.
2. **Freedom of Expression:** Anonymity fosters a space where users can express themselves freely, without fear of judgment or reprisal. This is particularly important for discussing sensitive topics, challenging প্রচলিত norms, or exploring unconventional ideas.
3. **Focus on Ideas, Not Identities:** By separating ideas from their authors, anonymity encourages users to evaluate contributions based on their intrinsic value, rather than on the reputation or status of the contributor. This promotes a more meritocratic and intellectually rigorous environment.
4. **Protection from Bias:** Anonymity can help to mitigate the effects of unconscious bias, such as those based on gender, race, or other personal characteristics. It allows ideas to be judged on their own merits, rather than through the lens of preconceived notions about the author.
5. **Lower Barrier to Entry:** Anonymity makes it easier for new users to join the platform and start contributing, as they don't need to go through a complex verification process or share personal information.

**How Anonymity Works on Choir:**

- **Default State:** All users are anonymous by default upon joining the platform. They can interact, contribute content, and earn CHIP tokens without revealing their real-world identity.
- **Unique Identifiers:** Users are assigned unique, randomly generated identifiers that allow them to build a consistent presence on the platform without compromising their anonymity.
- **No Personal Data Collection:** Choir does not collect or store any personally identifiable information about anonymous users.
- **"Priors" and Anonymity:** The "priors" system, which shows the lineage of ideas, maintains anonymity by design. It reveals the connections between ideas, not the identities of the individuals who proposed them.

**Balancing Anonymity with Accountability:**

- **CHIP Staking:** The requirement to stake CHIP tokens to post new messages acts as a deterrent against spam and malicious behavior, even for anonymous users.
- **Community Moderation:** The platform relies on community moderation to maintain the quality of discourse and address any issues that arise.
- **Reputation Systems:** While users are anonymous by default, they can still build reputations based on the quality of their contributions, as tracked through the "priors" system and potentially through community ratings.

**The Value of Anonymity in a High-Information Environment:**

- **Encourages Honest Discourse:** Anonymity can encourage more honest and open discussions, particularly on sensitive or controversial topics.
- **Promotes Intellectual Risk-Taking:** Users may be more willing to take intellectual risks and explore unconventional ideas when they are not worried about the potential repercussions for their personal or professional lives.
- **Facilitates Whistleblowing and Dissent:** Anonymity can provide a safe space for whistleblowers and those who wish to express dissenting views without fear of retaliation.
- **Protects Vulnerable Users:** Anonymity can be particularly important for users in marginalized or vulnerable communities who may face risks if their identities are revealed.

**Conclusion:**

Anonymity by default is a core design principle of Choir, one that reflects the platform's commitment to privacy, freedom of expression, and the creation of a truly meritocratic space for the exchange of ideas. It's a bold choice in a world where online platforms increasingly demand real-name identification, but it's a choice that has the potential to unlock new levels of creativity, honesty, and collective intelligence. By prioritizing anonymity, Choir is not just building a platform; it's building a new model for online interaction, one that empowers individuals and fosters a more open and equitable exchange of ideas.

=== File: docs/plan_choir_materialization.md ===



==
plan_choir_materialization
==


# Plan: CHOIR Coin Materialization - Building a Thriving Data Economy

## Overview

This document outlines the strategy for "CHOIR Coin Materialization" – the approach to bring the CHOIR coin economy and the Choir platform to life through a robust data marketplace, premium services, and governance mechanisms. The CHOIR coin is designed to serve as the currency of a thriving data economy, driving demand and utility while enabling a fair value exchange for contributors.

## The Data Marketplace - A Foundation for CHOIR Utility

The Choir Data Marketplace is envisioned as the primary utility driver for the CHOIR coin, creating a space where:

*   **High-Quality Training Data is Valued:** The marketplace enables the exchange of valuable, human-generated data that can be used for AI training, research, and development.
*   **Contributors are Fairly Rewarded:** Data contributors receive CHOIR coins as compensation for the value they create, with transparent attribution and tracking.
*   **Privacy and Control are Preserved:** Users maintain control over how their data is used, with clear permissions and usage rights.
*   **CHOIR Serves as the Exclusive Currency:** All marketplace transactions require CHOIR coins, creating a fundamental utility and demand driver.

## Key Features and Value Propositions of the Data Marketplace

1.  **Thread-Specific Data Contracts:**
    *   **Granular Access Control:** Data access and contribution rights are governed at the thread level, allowing for fine-grained control.
    *   **Value-Based Pricing:** The price of data access is determined by the quality, uniqueness, and utility of the data within each thread.
    *   **Transparent Attribution:** Contributors are clearly credited for their data, building reputation and enabling fair compensation.

2.  **Premium Data Services:**
    *   **Curated Datasets:** Access to high-quality, curated datasets for specific domains or use cases.
    *   **Custom Data Collection:** Ability to commission specific data collection or annotation tasks from the Choir community.
    *   **Data Analytics and Insights:** Tools for analyzing and extracting insights from marketplace data.

3.  **Identity-as-a-Service (IDaaS) Integration:**
    *   **Verified Contributor Status:** Paid IDaaS subscribers can build verifiable reputations as data contributors.
    *   **Enhanced Discovery:** Premium identity services enable better discovery of contributors' work.
    *   **Professional Networking:** Opportunities for professional connections based on data contributions and expertise.

4.  **CHOIR Coin Integration and Economic Model:**
    *   **Direct Rewards for Quality:** CHOIR coins are awarded for novel and cited contributions, creating a direct incentive for quality.
    *   **Marketplace Transaction Currency:** CHOIR coins are required for all data marketplace transactions.
    *   **Staking for Premium Access:** Users can stake CHOIR coins to access premium features or higher-tier data.
    *   **Governance Rights:** CHOIR holders can participate in governance decisions about marketplace rules and policies.

## Thread Contracts - Programmable Data Agreements

A core innovation in the Choir ecosystem is the concept of Thread Contracts - programmable agreements that govern data sharing, contribution rights, and value distribution within specific conversation threads:

*   **Stake-to-Participate Model:** Users stake CHOIR coins to participate in high-value threads, creating skin-in-the-game for quality contributions.
*   **Programmable Revenue Sharing:** Thread contracts can specify how value is distributed among contributors, based on their roles and contributions.
*   **Customizable Access Controls:** Thread creators can define who can access, contribute to, or benefit from the thread's data.
*   **Dynamic Pricing Mechanisms:** The cost of participation and data access can adjust based on thread quality, popularity, and value.

Thread Contracts create a flexible framework for collaboration and value exchange, allowing for diverse models of data creation and monetization while ensuring fair compensation for contributors.

## Governance and Future Evolution

The CHOIR coin is designed to evolve into a governance token, giving holders a say in the future development of the Choir platform:

*   **Protocol Parameter Governance:** CHOIR holders can vote on key parameters of the reward algorithms, marketplace fees, and other economic aspects.
*   **Feature Prioritization:** Community voting on new features and platform enhancements.
*   **Treasury Allocation:** Decisions on how to allocate community treasury funds for ecosystem development.
*   **Dispute Resolution:** Mechanisms for resolving disputes related to data usage, attribution, or compensation.

This governance layer ensures that the Choir platform remains aligned with the interests of its community and can adapt to changing needs and opportunities.

## Monetization Strategy

The monetization strategy for Choir Harmonic Intelligence Platform, Inc. (CHI) is multi-faceted:

1.  **Identity-as-a-Service (IDaaS) Subscriptions:** The primary revenue stream will be monthly subscription fees for persistent, recognized identities on the platform.
2.  **Marketplace Transaction Fees:** A small percentage fee on data marketplace transactions, providing ongoing revenue tied to ecosystem activity.
3.  **Premium API Access:** Fees for enterprise-grade API access to the Choir platform and data marketplace.
4.  **Strategic CHOIR Holdings:** CHI will purchase CHOIR coins from the open market to hold as strategic treasury assets, aligning corporate success with token value.

This strategy creates multiple revenue streams while ensuring that CHI's success is directly tied to the health and growth of the Choir ecosystem.

## Marketing and Messaging

The marketing and messaging for the CHOIR coin and data marketplace should emphasize:

*   **"Your Ideas Have Value":** Highlight how Choir enables users to monetize their intellectual contributions.
*   **"A Fair Data Economy":** Emphasize the transparent, fair compensation model for data contributors.
*   **"Privacy by Design":** Showcase the privacy-preserving aspects of the Choir platform and data marketplace.
*   **"Collective Intelligence, Individual Reward":** Position Choir as a platform where collective knowledge building benefits individual contributors.
*   **"CHOIR: The Currency of Ideas":** Establish CHOIR as the native currency of a new knowledge economy.

## Next Steps - Towards "CHOIR Materialization"

1.  **Enhance Core Platform Experience:** Continue refining the Loop of Thought experience and reward mechanisms to ensure high-quality contributions.
2.  **Develop Thread Contract Infrastructure:** Build and test the technical infrastructure for programmable thread contracts.
3.  **Launch Initial Data Marketplace Features:** Implement basic marketplace functionality, allowing for data access and exchange using CHOIR.
4.  **Expand IDaaS Offerings:** Develop enhanced identity services that integrate with the data marketplace.
5.  **Implement Governance Framework:** Design and deploy the initial governance mechanisms for CHOIR holders.

The "CHOIR Materialization" plan represents a comprehensive approach to creating tangible utility and value for the CHOIR coin through a robust data economy. By focusing on fair compensation for intellectual contributions, programmable data agreements, and community governance, Choir is positioned to create a sustainable and valuable ecosystem for knowledge creation and exchange.
# Level 4 Documentation


# Level 5 Documentation



=== File: docs/data_engine_model.md ===



==
data_engine_model
==


# Social Discourse Engine: AI That Facilitates Human Connection

VERSION social_discourse: 9.0 (Relationship-Focused Design)

The Social Discourse Engine represents a fundamental shift from AI systems designed to replace human interaction to AI that enhances and facilitates meaningful human connections. This framework emerged from recognizing that the most valuable use of AI isn't to be a better chatbot, but to help humans have better conversations with each other.

Rather than optimizing for engagement metrics or data extraction, the Social Discourse Engine prioritizes **the quality of human relationships and discourse** as the primary measure of success. It recognizes that in an age of increasing digital isolation and polarization, the most important role for AI is to help compatible minds find each other and engage in meaningful dialogue.

**The Relationship Flywheel: From Individual Expression to Community Formation:**

The Social Discourse Engine creates a flywheel effect where individual expression leads to community formation through AI-facilitated connections:

*   **Authentic Expression Enabled by Anonymity:** Users share genuine thoughts without social surveillance, creating more honest and valuable discourse than reputation-managed social media.
    *   **Merit-Based Evaluation:** Ideas compete on quality rather than social status, enabling breakthrough insights from unexpected sources.
    *   **Freedom from Cancel Culture:** Anonymous contribution removes the liability of posting, encouraging intellectual risk-taking and authentic expression.
    *   **Quality Over Engagement:** Without follower counts or viral metrics, users focus on thoughtful contribution rather than attention-seeking behavior.
*   **AI-Enhanced Understanding:** The PostChain workflow helps users articulate ideas more clearly while identifying intellectual connections:
    *   **Semantic Compatibility Detection:** AI identifies users with complementary thinking patterns through conversation analysis rather than demographic targeting.
    *   **Citation Network Analysis:** AI tracks which ideas prove valuable to others, creating a reputation system based on intellectual contribution.
    *   **Relationship Recommendations:** AI suggests potential connections based on intellectual compatibility rather than social graph proximity.

**Economic Alignment Through Relationship Staking:**

The Social Discourse Engine's key innovation is using economic incentives to align users toward quality discourse and meaningful relationships:

*   **CHOIR Tokens as Relationship Investment:**  Users earn tokens through quality contributions and can invest them in relationships with other users, creating shared economic interest in maintaining quality discourse.
    *   **Novelty Rewards:** Recognize original thinking and authentic perspectives that add value to community knowledge.
    *   **Citation Rewards:** Reward users whose ideas prove valuable to others, creating a merit-based reputation system.
    *   **Relationship Staking:** Enable users to invest earned tokens in connections with like-minded individuals.
*   **Economic Filtering for Quality:**  Token requirements create natural barriers to low-effort interactions while enabling meaningful connections:
    *   **Spam Prevention:** Economic cost of relationship initiation filters out automated and low-effort interactions.
    *   **Serious Intent Signaling:** Staking tokens demonstrates genuine interest in forming meaningful connections.
    *   **Shared Value Creation:** Successful relationships accumulate value through collaborative insights and mutual citations.
*   **Self-Reinforcing Quality Cycles:**  Economic alignment creates positive feedback loops that improve discourse quality over time:
    *   **Quality Attracts Quality:** Users with valuable insights attract other thoughtful contributors, creating high-quality conversation clusters.
    *   **Economic Incentives for Thoughtfulness:** Token rewards encourage users to contribute meaningfully rather than seeking engagement through controversy.
    *   **Community Self-Moderation:** Economic stakes in relationships create natural incentives for maintaining discourse quality.

**Beyond Engagement Metrics - Value Flows Through Relationship Quality:**

The Social Discourse Engine moves beyond the attention-driven models of traditional social media to focus on relationship quality and intellectual connection:

*   **Relationship Quality as the Measure of Success:**  Value is not measured by clicks, likes, or time-on-platform, but by **the quality and depth of relationships formed** through meaningful discourse. Successful connections that lead to ongoing collaboration and mutual intellectual growth represent true platform success.
*   **Intellectual Compatibility Over Demographic Targeting:**  Rather than connecting users based on age, location, or behavioral patterns, the system identifies intellectual compatibility through:
    *   **Semantic Resonance:** Users whose ideas build on each other's thinking in meaningful ways.
    *   **Citation Patterns:** Users who consistently find value in each other's contributions.
    *   **Collaborative Potential:** Users whose different perspectives could create valuable synthesis when combined.
*   **Long-Term Relationship Value:**  The system optimizes for relationships that create lasting value rather than momentary engagement:
    *   **Sustained Discourse Quality:** Relationships that maintain high-quality conversation over time.
    *   **Mutual Growth:** Connections where both parties develop their thinking through interaction.
    *   **Collaborative Output:** Relationships that produce insights neither party could have reached alone.

**Choir - A Practical Embodiment of the Social Discourse Engine:**

Choir is designed as a practical embodiment of the Social Discourse Engine theory. It is not just another social platform or AI chat app, but an **attempt to build a system that fundamentally transforms how humans connect and communicate online through AI-facilitated meaningful relationships.**

By focusing on:

*   **Facilitating Authentic Expression Through Anonymity and Merit-Based Evaluation**
*   **Using AI to Identify Intellectual Compatibility Rather Than Demographic Similarity**
*   **Creating Economic Alignment Through Relationship Staking and Quality Rewards**
*   **Building a Platform Where Users Own Their Social Value and Relationships**

Choir aims to create a new paradigm for social platforms – one where AI amplifies human connection rather than replacing it, where quality discourse is rewarded over engagement farming, and where meaningful relationships are the measure of success. The Social Discourse Engine theory provides the conceptual foundation for building technology that serves human community rather than extracting value from it.

=== File: docs/evolution_naming.md ===



==
evolution_naming
==


==
evolution_naming.md
==

# From RAG to Post Chain: A Name's Evolution, a System's Identity

VERSION evolution_naming: 7.0

The journey of Choir's core mechanism, from a simple concept to its current form, mirrors the evolution of the platform itself. Each name change reflects a deeper understanding, a refinement of purpose, a shift in perspective. It's a story of emergence, where the name didn't just describe the system, but helped shape it.

It began with **RAG - Retrieval-Augmented Generation**. A functional description, accurate yet sterile. It spoke to the technical process but lacked the spark of life, the hint of something more. RAG was about retrieving information; it wasn't yet about generating understanding.

Then came **Vowel Loop**, a name born from the observation of linguistic patterns, the AEIOU and sometimes Y. It was playful, memorable, but perhaps too niche, too focused on a specific detail. It hinted at the importance of language but didn't capture the broader scope. Still, it was a step towards recognizing the system's unique relationship with language.

**Chorus Cycle** arrived next, a name that resonated with the platform's core philosophy. It evoked collaboration, harmony, the interplay of voices. It described the iterative process, the six phases of refinement. But it was also complex, potentially intimidating. It focused on the process, but perhaps not enough on the outcome.

And so, we arrive at **Post Chain**. A name that is both simple and profound. "Post" speaks to the fundamental unit of interaction, the message, the contribution. "Chain" evokes connection, sequence, the building of knowledge over time. It hints at the blockchain foundation, the "chain of thought" reasoning, the causal chain of events.

**Post Chain** is more than just a name; it's a statement of intent. It's about creating a system where each post is a link in a larger chain, where individual contributions connect to form a collective intelligence. It's about building a platform where knowledge is not just retrieved but generated, where meaning is not just found but created.

The shift from Chorus Cycle to Post Chain also marks a crucial conceptual evolution. It's a move from a focus on process to a focus on outcome. The phases are still there, the underlying mechanisms remain, but they are now implicit, not explicit. The emphasis is on the chain of posts, the interconnectedness of ideas, the emergent intelligence.

This evolution is not merely semantic. It reflects a deeper understanding of the system's core principles, a refinement of its purpose, a recognition of its potential. **Post Chain** is the name that embodies the platform's essence: a simple, powerful, and elegant system for building collective intelligence, one post at a time. It is easy to say, and means what it says. It is direct.


=== File: docs/evolution_token.md ===



==
evolution_token
==


# The Evolution of CHOIR: From Utility Token to the Heart of a Learning Ecosystem

The CHOIR coin has undergone a remarkable evolution, transcending its initial conception as a mere utility token to become something far more significant: **the very heart of the Choir ecosystem, a representation of value, participation, ownership, and the driving force behind a self-improving AI knowledge engine.**

**Beyond "Utility" - CHOIR as a Multifaceted Representation of Value:**

The term "utility token" no longer fully captures the essence of CHOIR.  It is not simply a means to access features or perform actions; CHOIR has evolved into a multifaceted representation of value within Choir:

*   **A Stake in Collective Intelligence:** CHOIR represents a **stake in the collective intelligence of Choir**, a share in a dynamic and ever-evolving knowledge ecosystem.  Holding CHOIR is not just about accessing a platform; it's about owning a piece of a growing, intelligent network.
*   **A Symbol of Participation and Contribution:** CHOIR is earned through **genuine participation and valuable contributions** to the Choir ecosystem.  It's a tangible recognition of intellectual effort, insightful prompts, and salient citations that enrich the collective knowledge base.  Holding CHOIR signifies active engagement and a commitment to building a high-quality knowledge commons.
*   **A Key to Unlocking Data Value:** CHOIR coins are the **exclusive currency for accessing and contributing to the Choir data marketplace.**  They represent "data purchase power," enabling users to buy access to valuable, human-labeled training data generated within the platform and to contribute their own data for economic benefit.
*   **A Governance Right and a Voice in the Future:** CHOIR coins empower holders with **governance rights**, giving them a direct voice in shaping the future of the Choir platform, the rules of the data marketplace, and the evolution of the CHOIR coin economy itself.
*   **A Training Signal for AI - Driving Self-Improvement:**  Most profoundly, CHOIR coins are the **driving force behind a self-improving AI ecosystem.**  Coin rewards (novelty and citation) act as **training signals for AI models within Choir**, incentivizing them to learn, adapt, and optimize for behaviors that contribute to the platform's quality, coherence, and value creation.

**The Poker Chip Analogy - Commitment, Engagement, and a Positive-Sum Game:**

The analogy to poker chips remains apt, but with a deeper understanding: CHOIR, like a poker chip, represents a **commitment to engage, a willingness to participate in the game of knowledge creation.**  However, unlike poker, Choir is not a zero-sum game. It's a **positive-sum environment** where collaboration, knowledge sharing, and collective intelligence benefit all participants.  CHOIR represents your stake in this positive-sum game.

**The Liminal Space - Currency, Equity, and a Bet on the Future:**

CHOIR exists in the liminal space between a currency and an equity, reflecting its multifaceted nature.  It's not intended as a general-purpose medium of exchange, but it holds value far beyond its immediate utility.  CHOIR is a **"bet" on the future of Choir**, an **investment in the potential of collective intelligence**, and a **claim on the value generated by a self-improving AI knowledge engine.**

**ICM and Long-Term Value - Beyond Short-Term Speculation, Towards Sustainable Growth:**

The Independent Chip Model (ICM) framework, borrowed from poker, remains relevant, guiding us to focus on **long-term expected value** rather than short-term speculative gains.  CHOIR is designed to incentivize contributions that enhance the platform's overall worth, build a sustainable ecosystem, and drive long-term value accrual for all stakeholders.

**Mainnet Status - From Concept to Reality:**

The CHOIR coin has now been minted and deployed on the Sui mainnet, marking a significant milestone in its evolution from concept to reality. This mainnet deployment (with package ID `0x4f83f1cd85aefd0254e5b6f93bd344f49dd434269af698998dd5f4baec612898::choir::CHOIR`) represents the transition of CHOIR from a theoretical construct to a functioning digital asset with real utility and value within the Choir ecosystem.

**Beyond Speculation - Building a Real-World Data Economy and a Thriving Ecosystem:**

By emphasizing CHOIR's role in participation, value representation, ownership, and AI-driven learning, we actively **discourage purely speculative behavior** and focus on building a **real-world data economy** within Choir.  CHOIR is not designed to be a "get-rich-quick scheme," but a **tool for building and sharing knowledge, for empowering users, and for creating a sustainable and thriving ecosystem for collective intelligence.**

**Implications for the Future - A New Paradigm for Tokenized Value and AI-Driven Growth:**

The evolution of CHOIR points towards a **new paradigm for tokenized value and AI-driven growth** in online platforms:

*   **Token Utility Beyond Access - Training Signals for AI:**  CHOIR demonstrates that token utility can go far beyond simple access or governance. Tokens can become **active components in the AI system itself**, driving learning, incentivizing desired behaviors, and shaping the evolution of AI models.
*   **User Ownership and Data Empowerment - A Counter-Narrative to Data Extraction:**  CHOIR embodies a counter-narrative to the data-extractive models of traditional platforms.  It empowers users with **ownership and control over their data contributions** and allows them to **benefit economically** from the value they create.
*   **Decentralized Governance of Data Marketplaces - User-Driven Data Ethics:**  CHOIR holder governance of the data marketplace establishes a **decentralized and user-driven approach to data ethics and data governance**, ensuring that data is used responsibly and in alignment with community values.
*   **Sustainable and Self-Improving AI Ecosystems - A New Model for the Future of AI:**  CHOIR, as the heart of the Choir ecosystem, represents a step towards building **sustainable and self-improving AI ecosystems** that are driven by user contributions, guided by economic incentives, and focused on generating collective intelligence and long-term value for all participants.

The evolution of CHOIR is a journey from a simple utility token to a **fundamental building block of a revolutionary AI-powered knowledge ecosystem.** It represents a shift from extractive platforms to **value-aligned, user-empowering, and self-improving systems** that have the potential to reshape the future of online interaction and collective intelligence.

=== File: docs/blockchain_integration.md ===



==
blockchain_integration
==


# Blockchain Integration in Choir (Qdrant-Sui MVP)

VERSION blockchain_integration: 8.0 (Qdrant-Sui MVP Focus)

## Overview

This document outlines the blockchain integration strategy for the Choir Qdrant-Sui MVP. This approach centralizes blockchain interactions within the main Python API backend, specifically using a dedicated service module (`sui_service.py`) to interact with the Sui blockchain via the PySUI SDK.

## Core Blockchain Integration Goals

The core goals of blockchain integration for the MVP and beyond remain:

1.  **Immutable Record of Economic Actions:** Utilize the Sui blockchain for a transparent record of key economic events, primarily simplified token rewards for the MVP.
2.  **Decentralized and Verifiable Token Economy:** Implement the basic CHIP token using a Sui smart contract (`choir_coin.move`).
3.  **Secure and Transparent Reward Distribution:** Ensure that CHIP token rewards (simplified for MVP) are distributed verifiably on-chain.
4.  **(Future)** Enable On-Chain Governance: Lay the groundwork for future on-chain governance by CHIP token holders.

## MVP Blockchain Integration Architecture: Centralized API Service

In the Qdrant-Sui MVP architecture, blockchain integration is handled by the **Python API backend** via its `sui_service.py` module. This service acts as the *sole interface* between the Choir application logic and the Sui blockchain.

**Key Components:**

*   **Python API Backend (FastAPI/Uvicorn):**
    *   **Orchestrates Workflow:** Manages the PostChain workflow execution.
    *   **Contains Blockchain Logic:** Includes the `sui_service.py` module responsible for all Sui interactions.
    *   **Triggers Rewards:** After the PostChain workflow completes (Yield phase), the API calls functions within `sui_service.py` to process rewards based on data stored in Qdrant.

*   **`sui_service.py` (within API Backend):**
    *   **PySUI Integration (Encapsulated):** The PySUI SDK for interacting with the Sui blockchain is exclusively used within this service module.
    *   **Handles Transactions:** Constructs, signs (using keys managed by the API's environment/secrets), and submits transactions to the Sui network.
    *   **Exposes Service Functions:** Provides functions (e.g., `record_reward`, `get_balance`) called internally by the API's orchestration logic.

*   **PostChain Workflow (LCEL - within API Backend):**
    *   **No Direct Blockchain Interaction:** The AEIOU-Y phase logic **does not directly interact with the Sui blockchain or PySUI.**
    *   **Provides Reward Inputs:** The workflow (specifically data gathered by Experience and finalized by Yield) provides the necessary inputs (author ID, prior IDs, scores) for the API to trigger the reward calculation in `sui_service.py`.

*   **Sui Blockchain:**
    *   **Hosts CHIP Token Contract:** Runs the `choir_coin.move` smart contract defining the basic CHIP token.
    *   **Records Transactions:** Stores the history of token transfers/mints executed by `sui_service.py`.

**Architecture Diagram (Qdrant-Sui MVP):**

```mermaid
graph LR
    A[Client (SwiftUI)] --> B{Python API (FastAPI)};
    B --> C[PostChain Workflow (LCEL)];
    C -- Interacts via database.py --> D[(Qdrant)];
    C -- Returns final data --> B;
    B -- Triggers reward --> E[sui_service.py];
    E -- Uses PySUI --> F[(Sui Blockchain)];
    B -- Streams results --> A;

    style B fill:#ccf,stroke:#333,stroke-width:2px;
    style C,E fill:#f9f,stroke:#333,stroke-width:2px;
    style D,F fill:#bfc,stroke:#333,stroke-width:2px;

    subgraph API Backend Container
        B
        C
        E
    end

    Communication Flow for Blockchain Operations (MVP):

PostChain Completion: The PostChain workflow (running within the API) completes its final (Yield) phase. It returns the final AI message structure, including author ID, cited prior IDs, novelty score, and similarity scores.

API Trigger: The main API logic receives the completed PostChain data.

Data Persistence: The API saves the final AI message to the choir collection in Qdrant.

Call Sui Service: The API calls the appropriate function within sui_service.py (e.g., process_rewards), passing the relevant data fetched from the newly saved Qdrant message (or held from the workflow result).

Sui Service Execution: The sui_service.py function:

Performs the (simplified for MVP) reward calculation.

Looks up recipient Sui addresses if necessary (using Qdrant users collection via database.py).

Uses PySUI to construct and sign the necessary Sui transaction(s) (e.g., calling a basic mint_reward function in the choir_coin contract).

Submits the transaction to the Sui blockchain.

Result Handling: The sui_service.py function returns the transaction result (e.g., digest, success/failure) to the main API logic. The API logs this result. (Note: For MVP, the result might not be directly propagated back to the client UI).

Service Functions Exposed by sui_service.py (MVP):

The sui_service.py module exposes internal functions called by the API orchestrator. Key functions for the MVP include:

process_rewards(message_id, author_user_id, cited_prior_ids, novelty_score, similarity_scores): Calculates (simplified) rewards and calls the mint/transfer function.

_call_sui_mint(recipient_address, amount): Internal helper to interact with the Sui contract's mint function.

get_balance(sui_address): Queries the SUI balance (primarily for testing/diagnostics in MVP). (Already implemented)

(Future) get_chip_balance(sui_address): Queries the CHIP token balance.

(Future) get_thread_stake_price(thread_id): Fetches economic state from potential future contract.

Security Considerations (MVP):

With blockchain interactions centralized in the API backend's sui_service.py:

API Key Management: The primary security concern is protecting the Sui private key used by the API backend. This key must be managed securely using environment variables, platform secrets management (e.g., Render secrets), or a dedicated secrets manager. It must not be hardcoded.

Input Validation: The API must rigorously validate all data passed to sui_service.py functions, especially recipient addresses and amounts, to prevent manipulation or unintended transactions.

Service Isolation (Logical): While not physically isolated like a separate server/TEE, sui_service.py provides logical isolation. All blockchain interaction code is contained within this module, making it easier to audit and secure compared to scattering PySUI calls throughout the codebase.

Standard API Security: General API security practices (authentication, authorization, rate limiting, HTTPS) are essential to protect the endpoints that trigger the workflows leading to blockchain interactions.

Deployment Considerations (MVP):

API Container Deployment: The Python API, including sui_service.py and its PySUI dependency, is deployed as a single Docker container (e.g., on Render).

Secure Key Provisioning: The Sui private key required by sui_service.py must be securely provisioned to the deployed container's environment (e.g., using Render's secret management).

Conclusion (MVP Focus)
The Qdrant-Sui MVP utilizes a centralized approach for blockchain integration, embedding the logic within the main Python API backend via the sui_service.py module. This simplifies the architecture for the MVP, allowing focus on the core Qdrant data structures and the basic reward triggering mechanism. While deferring the complexities of distributed servers and TEEs, this approach provides a clear path to validating the fundamental interaction between AI-analyzed data in Qdrant and the Sui blockchain-based token economy. Secure management of the API's Sui key is paramount in this model.

=== File: docs/ChoirPushNotificationsImplementationGuide.md ===



==
ChoirPushNotificationsImplementationGuide
==


# Choir Push Notifications Implementation Guide

This guide outlines the steps to test and integrate push notifications for citation events in the Choir app.

## Server Configuration

### Environment Variables
Ensure these environment variables are set on your server:
```
APNS_KEY_ID=YOUR_KEY_ID
APNS_TEAM_ID=YOUR_TEAM_ID
APNS_AUTH_KEY=/path/to/AuthKey_KEYID.p8
APNS_TOPIC=choir.chat
```

### Required Packages
Make sure these packages are installed:
```
pyjwt==2.10.1
cryptography==44.0.2
```

## 1. Test Sending an Actual Notification

### Register a Device Token
1. Run the Choir app in the simulator or on a device
2. Ensure the app requests and receives notification permissions
3. Check the console logs for the device token output
4. Copy the device token for testing

### Test with the API Endpoint
```bash
# From the api directory with venv activated
curl -X POST http://localhost:8000/api/notifications/test-push \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_AUTH_TOKEN" \
  -d '{"device_token": "DEVICE_TOKEN_FROM_CONSOLE"}'
```

## 2. Verify Citation Notifications

### Create Test Vector and Citation
1. Create a test vector with your wallet address:
```bash
curl -X POST http://localhost:8000/api/vectors \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_AUTH_TOKEN" \
  -d '{
    "content": "This is a test vector for citation notifications",
    "metadata": {
      "wallet_address": "YOUR_WALLET_ADDRESS"
    }
  }'
```

2. Note the vector ID from the response

3. Create a citation to that vector:
```bash
curl -X POST http://localhost:8000/api/postchain/langchain \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_AUTH_TOKEN" \
  -d '{
    "query": "Please cite the vector with ID: VECTOR_ID",
    "wallet_address": "DIFFERENT_WALLET_ADDRESS"
  }'
```

4. Check if you receive a push notification on your device

## 3. Start the Server

```bash
# From the api directory with venv activated
uvicorn app.main:app --reload
```

## Swift Integration for Push Notifications

### Update Info.plist
Ensure these capabilities are in your Info.plist:
```xml
<key>UIBackgroundModes</key>
<array>
    <string>remote-notification</string>
</array>
```

### Register for Notifications in AppDelegate
The code is already implemented in `PushNotificationManager.swift`, but verify:
1. `registerForPushNotifications()` is called on app launch
2. `updateDeviceToken()` properly formats and sends the token to the server
3. `handleNotificationReceived()` processes different notification types

### Add Notification Observers in TransactionsView
```swift
// In .onAppear
NotificationCenter.default.addObserver(
    forName: NSNotification.Name("RefreshNotifications"),
    object: nil,
    queue: .main
) { _ in
    transactionService.fetchTransactions()
}
```

## Testing in Simulator

1. Run the app in the simulator
2. Use the Simulator menu: Features > Push Notifications
3. Create a JSON payload:
```json
{
  "aps": {
    "alert": {
      "title": "Your content was cited!",
      "body": "Someone cited your content"
    },
    "sound": "default",
    "badge": 1
  },
  "notification_type": "citation",
  "vector_id": "test_vector_id",
  "citing_wallet_address": "test_wallet_address"
}
```
4. Click "Send" to deliver the notification

## Testing in TestFlight

1. Build and archive the app with push notification entitlements
2. Upload to TestFlight
3. Install on a test device
4. Use the test endpoint to send a real notification:
```bash
curl -X POST https://your-production-server.com/api/notifications/test-push \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_AUTH_TOKEN" \
  -d '{"device_token": "DEVICE_TOKEN_FROM_TEST_DEVICE"}'
```

## Troubleshooting

- **No notifications in simulator**: Use the simulator's manual notification feature
- **No notifications on device**: Check APNs environment (sandbox vs. production)
- **Server errors**: Check logs for JWT token generation issues
- **Device token not registering**: Verify the registration endpoint is working
- **Production vs. Development**: For Swift environment targeting, use:
  ```swift
  #if DEBUG && targetEnvironment(simulator)
      // Use devnet/sandbox
  #else
      // Use mainnet/production
  #endif
  ```

## APNs Configuration in Apple Developer Portal

1. **Environment**: Choose "Sandbox & Production" to allow your key to work in both development and production environments
2. **Key Restriction**: Choose "Team Scoped (All Topics)" for flexibility across all your apps
3. **Bundle ID**: Ensure your app's bundle ID matches the `APNS_TOPIC` environment variable

## Notification Payload Structure

```json
{
  "aps": {
    "alert": {
      "title": "Your content was cited!",
      "body": "Someone cited your content: \"content_preview\""
    },
    "sound": "default",
    "badge": 1
  },
  "notification_type": "citation",
  "vector_id": "vector_id_here",
  "citing_wallet_address": "wallet_address_here"
}
```

This implementation provides a complete solution for sending push notifications when users' content is cited, enhancing the user experience and engagement with the app.

=== File: docs/contract_deployment.md ===



==
contract_deployment
==


# Choir Contract Deployment Guide

This document outlines the process for deploying the Choir token contract to both devnet and mainnet environments. It provides step-by-step instructions, important considerations, and troubleshooting tips.

## Table of Contents

1. [Prerequisites](#prerequisites)
2. [Devnet Deployment](#devnet-deployment)
3. [Mainnet Deployment](#mainnet-deployment)
4. [Post-Deployment Configuration](#post-deployment-configuration)
5. [Troubleshooting](#troubleshooting)
6. [Security Considerations](#security-considerations)

## Prerequisites

Before deploying the Choir contract, ensure you have:

- Sui CLI installed (`sui` command available)
- Active Sui wallet with sufficient gas tokens
- Choir contract code in `choir_coin/choir_coin` directory
- Backup of private keys (especially important for mainnet)
- Python virtual environment set up for API updates

## Devnet Deployment

Follow these steps to deploy the Choir contract to Sui devnet:

### 1. Navigate to the Contract Directory

```bash
cd choir_coin/choir_coin
```

### 2. Switch to Devnet Environment

```bash
sui client switch --env devnet
```

### 3. Verify Active Address

```bash
sui client active-address
```

This will display your active wallet address, which will be used for deployment.

### 4. Check Gas Balance

```bash
sui client gas
```

Ensure you have sufficient SUI tokens for deployment (at least 1 SUI recommended).

### 5. Build the Contract

```bash
sui move build
```

This compiles the Move code and prepares it for deployment.

### 6. Publish the Contract

```bash
sui client publish --gas-budget 100000000
```

This command publishes the contract to devnet. The output will contain important information:

- **Package ID**: Identified in the output as "Published to 0x..."
- **Treasury Cap ID**: Found in the "Created Objects" section with type containing "TreasuryCapability"

Example output:
```
Published to 0xb33aeae469ce4bdea302e66bb0330fbe4d606776451c3099a5fc557923556a6a
...
Created Objects:
  ┌──
  │ ID: 0x6eab9c65acf9b4001199ac98813951140417b5feff8a85218eddd14a62d14f37
  │ Owner: Account Address ( 0xe9e9eba13e6868cbb3ab97b5615b2f09459fd6bbac500a251265165febc3073d )
  │ ObjectType: 0xb33aeae469ce4bdea302e66bb0330fbe4d606776451c3099a5fc557923556a6a::choir::TreasuryCapability
  ...
```

### 7. Test Minting Tokens

```bash
sui client call --package <PACKAGE_ID> --module choir --function mint --args <TREASURY_CAP_ID> 1000000000 <RECIPIENT_ADDRESS> --gas-budget 10000000
```

Replace the placeholders with your actual values:
- `<PACKAGE_ID>`: The package ID from step 6
- `<TREASURY_CAP_ID>`: The treasury cap ID from step 6
- `<RECIPIENT_ADDRESS>`: Your wallet address or another test address

Example:
```bash
sui client call --package 0xb33aeae469ce4bdea302e66bb0330fbe4d606776451c3099a5fc557923556a6a --module choir --function mint --args 0x6eab9c65acf9b4001199ac98813951140417b5feff8a85218eddd14a62d14f37 1000000000 0xe9e9eba13e6868cbb3ab97b5615b2f09459fd6bbac500a251265165febc3073d --gas-budget 10000000
```

This mints 1 CHOIR token (1,000,000,000 base units with 9 decimals) to the specified address.

## Mainnet Deployment

The process for mainnet deployment is similar to devnet, but requires additional care and consideration:

### 1. Navigate to the Contract Directory

```bash
cd choir_coin/choir_coin
```

### 2. Switch to Mainnet Environment

```bash
sui client switch --env mainnet
```

### 3. Verify Active Address

```bash
sui client active-address
```

Ensure this is the address you want to use for the mainnet deployment.

### 4. Check Gas Balance

```bash
sui client gas
```

Verify you have sufficient SUI tokens for deployment on mainnet (at least 2-3 SUI recommended).

### 5. Build the Contract

```bash
sui move build
```

### 6. Publish the Contract

```bash
sui client publish --gas-budget 100000000
```

**IMPORTANT**: This will use real SUI tokens and deploy the contract to the mainnet blockchain.

Record the Package ID and Treasury Cap ID from the output, as you did for devnet.

**Mainnet Deployment Results:**
- **Package ID**: `0x4f83f1cd85aefd0254e5b6f93bd344f49dd434269af698998dd5f4baec612898`
- **Treasury Cap ID**: `0x1ee8226165efd8c2cf965199855b40acb0a86c667d64ea5251a06163feeeaa12`
- **Deployer Address**: `0xe9e9eba13e6868cbb3ab97b5615b2f09459fd6bbac500a251265165febc3073d`
- **Transaction Digest**: `EBv5AeY9HPYYrgrkSZNAEkQojjYqYurnHx1T4pydFxLj`

### 7. Test Minting Tokens (Optional)

```bash
sui client call --package <MAINNET_PACKAGE_ID> --module choir --function mint --args <MAINNET_TREASURY_CAP_ID> 1000000000 <RECIPIENT_ADDRESS> --gas-budget 10000000
```

Consider minting a small amount first to verify everything works correctly.

**Initial Token Mint:**
```bash
sui client call --package 0x4f83f1cd85aefd0254e5b6f93bd344f49dd434269af698998dd5f4baec612898 --module choir --function mint --args 0x1ee8226165efd8c2cf965199855b40acb0a86c667d64ea5251a06163feeeaa12 1000000000 0xe9e9eba13e6868cbb3ab97b5615b2f09459fd6bbac500a251265165febc3073d --gas-budget 10000000
```

This minted 1 CHOIR token (1,000,000,000 base units with 9 decimals) to the deployer address. The transaction was successful with digest: `5wCGxiLk9pQjmuMk1Buc3koGCKVjAyYoQ2AFKjn8mVrm`.

## Post-Deployment Configuration

After deploying the contract, you need to update the application configuration:

### 1. Update SUI Service Configuration

Edit `api/app/services/sui_service.py` to update the package ID and treasury cap ID:

```python
# For network-specific configuration
if self.network == "mainnet":
    self.package_id = "0x4f83f1cd85aefd0254e5b6f93bd344f49dd434269af698998dd5f4baec612898"
    self.treasury_cap_id = "0x1ee8226165efd8c2cf965199855b40acb0a86c667d64ea5251a06163feeeaa12"
else:  # devnet
    self.package_id = "0xb33aeae469ce4bdea302e66bb0330fbe4d606776451c3099a5fc557923556a6a"
    self.treasury_cap_id = "0x6eab9c65acf9b4001199ac98813951140417b5feff8a85218eddd14a62d14f37"
```

### 2. Update Swift Client Configuration

Edit `Choir/Models/CoinType.swift` to update the coin type identifier:

```swift
static let choir = CoinType(
    coinTypeIdentifier: "0xb33aeae469ce4bdea302e66bb0330fbe4d606776451c3099a5fc557923556a6a::choir::CHOIR", // For devnet
    name: "Choir",
    symbol: "CHOIR",
    decimals: 9,
    iconName: "choir-logo"
)

// For mainnet, you would use:
// coinTypeIdentifier: "0x4f83f1cd85aefd0254e5b6f93bd344f49dd434269af698998dd5f4baec612898::choir::CHOIR"
```

For a multi-environment setup, consider implementing environment-specific configurations like:

```swift
#if DEBUG
static let choir = CoinType(
    coinTypeIdentifier: "0xb33aeae469ce4bdea302e66bb0330fbe4d606776451c3099a5fc557923556a6a::choir::CHOIR", // Devnet
    name: "Choir",
    symbol: "CHOIR",
    decimals: 9,
    iconName: "choir-logo"
)
#else
static let choir = CoinType(
    coinTypeIdentifier: "0x4f83f1cd85aefd0254e5b6f93bd344f49dd434269af698998dd5f4baec612898::choir::CHOIR", // Mainnet
    name: "Choir",
    symbol: "CHOIR",
    decimals: 9,
    iconName: "choir-logo"
)
#endif
```

### 3. Test the Notification System

After updating the configuration, test the notification system to ensure it works with the new contract:

```bash
cd api
source venv/bin/activate
python ../scripts/test_notifications.py
```

## Troubleshooting

### Common Issues and Solutions

#### 1. "SuiKit.SuiError error 26"

This error often indicates a coin type mismatch. Check:
- The coin type identifier in `CoinType.swift` matches the actual deployed contract
- The package ID in the API configuration matches the deployed contract

#### 2. "No coin objects found for this coin type"

This indicates that the wallet doesn't have any coins of the specified type. Solutions:
- Verify the coin type is correct
- Mint some tokens to the wallet
- Check if the wallet address is correct

#### 3. Transaction Failures

If transactions fail:
- Check gas budget (increase if necessary)
- Verify the treasury cap ID is correct
- Ensure the wallet has sufficient permissions

#### 4. API Connection Issues

If the API can't connect to the Sui network:
- Verify network configuration (devnet vs mainnet)
- Check if the Sui node is accessible
- Verify API keys and authentication

## Security Considerations

### Treasury Cap Management

The treasury cap gives complete control over the token supply. Consider:
- Using a multi-signature wallet for the treasury cap
- Implementing time-locks or governance mechanisms
- Regularly auditing mint/burn operations

### Private Key Security

- Store private keys securely, preferably in hardware wallets
- Use different wallets for development and production
- Consider key rotation strategies

### Contract Upgradability

The current contract is not upgradable. For future versions:
- Consider implementing upgrade mechanisms
- Document the upgrade process
- Test upgrades thoroughly on devnet before mainnet

### Monitoring

- Set up monitoring for contract interactions
- Monitor token supply and large transfers
- Implement alerts for unusual activity

## Conclusion

Deploying the Choir contract requires careful planning and execution. By following this guide, you can ensure a smooth deployment process and minimize potential issues. Always test thoroughly on devnet before proceeding to mainnet, and maintain secure practices for managing the treasury capability.

=== File: docs/mainnet_migration.md ===



==
mainnet_migration
==


# Choir Mainnet Migration Guide

This document outlines the process for migrating the Choir application from devnet to mainnet. It covers all aspects of the migration, including contract deployment, API configuration, client updates, and testing procedures.

## Table of Contents

1. [Migration Overview](#migration-overview)
2. [Pre-Migration Checklist](#pre-migration-checklist)
3. [Contract Deployment](#contract-deployment)
4. [API Configuration](#api-configuration)
5. [Client Configuration](#client-configuration)
6. [Testing Procedures](#testing-procedures)
7. [Rollout Strategy](#rollout-strategy)
8. [Rollback Plan](#rollback-plan)
9. [Post-Migration Monitoring](#post-migration-monitoring)

## Migration Overview

The migration from devnet to mainnet involves several key steps:

1. Deploying the Choir token contract to Sui mainnet
2. Updating API configurations to support both devnet and mainnet
3. Updating client configurations to use the mainnet contract
4. Testing the end-to-end flow on mainnet
5. Gradually rolling out to users

This process requires careful coordination and thorough testing to ensure a smooth transition.

## Pre-Migration Checklist

Before beginning the migration, ensure:

- [ ] All devnet features are stable and working correctly
- [ ] Contract code has been audited and reviewed
- [ ] Sufficient SUI tokens are available for mainnet deployment
- [ ] Backup of all private keys and mnemonics
- [ ] Team members are assigned specific migration tasks
- [ ] Rollback plan is in place
- [ ] Monitoring tools are set up
- [ ] Communication plan for users is prepared

## Contract Deployment

### 1. Prepare the Deployment Wallet

```bash
# Switch to mainnet
sui client switch --env mainnet

# Verify active address
sui client active-address

# Check gas balance
sui client gas
```

Ensure the deployment wallet has at least 5 SUI for gas fees and is properly secured.

### 2. Deploy the Contract

```bash
# Navigate to contract directory
cd choir_coin/choir_coin

# Build the contract
sui move build

# Publish to mainnet
sui client publish --gas-budget 100000000
```

### 3. Record Contract IDs

From the deployment output, record:

- Package ID: `0x...`
- Treasury Cap ID: `0x...`

Store these values securely as they will be needed for API and client configuration.

### 4. Initial Token Minting

Mint an initial supply of tokens to the treasury wallet:

```bash
sui client call --package <MAINNET_PACKAGE_ID> --module choir --function mint --args <MAINNET_TREASURY_CAP_ID> 1000000000000 <TREASURY_WALLET_ADDRESS> --gas-budget 10000000
```

This mints 1,000 CHOIR tokens (with 9 decimals) to the treasury wallet.

## API Configuration

### 1. Update SUI Service

Modify `api/app/services/sui_service.py` to support both environments:

```python
def __init__(self, network=None):
    # Initialize network from parameter or environment variable
    self.network = network or os.getenv("SUI_NETWORK", "devnet")
    
    # Configure RPC client based on network
    if self.network == "mainnet":
        self.client = SuiClient(config=ClientConfig(url="https://fullnode.mainnet.sui.io"))
        self.package_id = "0x..." # Mainnet package ID
        self.treasury_cap_id = "0x..." # Mainnet treasury cap ID
        logger.info(f"Initialized SuiService for mainnet")
    else:
        self.client = SuiClient(config=ClientConfig(url="https://fullnode.devnet.sui.io"))
        self.package_id = "0xb33aeae469ce4bdea302e66bb0330fbe4d606776451c3099a5fc557923556a6a"
        self.treasury_cap_id = "0x6eab9c65acf9b4001199ac98813951140417b5feff8a85218eddd14a62d14f37"
        logger.info(f"Initialized SuiService for devnet")
```

### 2. Update Config Module

Modify `api/app/config.py` to include network configuration:

```python
class Config:
    # Existing configuration...
    
    # SUI network configuration
    SUI_NETWORK: str = os.getenv("SUI_NETWORK", "devnet")
    
    @classmethod
    def from_env(cls):
        # Existing code...
        network = os.getenv("SUI_NETWORK", "devnet")
        return cls(
            # Existing parameters...
            network=network,
        )
```

### 3. Update Deployment Configuration

Create environment-specific deployment configurations:

```bash
# For devnet
export SUI_NETWORK=devnet

# For mainnet
export SUI_NETWORK=mainnet
```

## Client Configuration

### 1. Update CoinType.swift

Modify `Choir/Models/CoinType.swift` to support both environments:

```swift
#if DEBUG
// Devnet configuration
static let choir = CoinType(
    coinTypeIdentifier: "0xb33aeae469ce4bdea302e66bb0330fbe4d606776451c3099a5fc557923556a6a::choir::CHOIR",
    name: "Choir",
    symbol: "CHOIR",
    decimals: 9,
    iconName: "choir-logo"
)
#else
// Mainnet configuration
static let choir = CoinType(
    coinTypeIdentifier: "0x<MAINNET_PACKAGE_ID>::choir::CHOIR",
    name: "Choir",
    symbol: "CHOIR",
    decimals: 9,
    iconName: "choir-logo"
)
#endif
```

### 2. Update WalletManager.swift

Ensure the `WalletManager` uses the correct network connection:

```swift
init() {
    #if DEBUG
    print("Using devnet connection")
    restClient = SuiProvider(connection: DevnetConnection())
    faucetClient = FaucetClient(connection: DevnetConnection())
    #else
    print("Using mainnet connection")
    restClient = SuiProvider(connection: MainnetConnection())
    faucetClient = FaucetClient(connection: MainnetConnection())
    #endif
    
    // Load all wallets
    Task {
        await loadAllWallets()
    }
}
```

### 3. Update API Configuration

Modify `Choir/Networking/APIClient.swift` to use the correct API endpoints:

```swift
#if DEBUG
static let baseURL = URL(string: "https://api-dev.choir.io")!
#else
static let baseURL = URL(string: "https://api.choir.io")!
#endif
```

## Testing Procedures

### 1. API Testing

Test the API with mainnet configuration:

```bash
# Set environment to mainnet
export SUI_NETWORK=mainnet

# Activate virtual environment
cd api
source venv/bin/activate

# Run notification test
python ../scripts/test_notifications.py
```

### 2. End-to-End Testing

Perform these tests on the mainnet configuration:

1. **Authentication**: Test wallet authentication with mainnet wallets
2. **Wallet Balance**: Verify correct display of mainnet CHOIR tokens
3. **Sending Tokens**: Test sending CHOIR tokens between wallets
4. **Citation Rewards**: Test the citation reward flow
5. **Notifications**: Verify citation notifications are received

### 3. Performance Testing

Test the performance of mainnet transactions:

1. **Transaction Speed**: Measure transaction confirmation times
2. **API Response Time**: Measure API response times with mainnet configuration
3. **Load Testing**: Simulate multiple concurrent users

## Rollout Strategy

### 1. Phased Approach

1. **Internal Testing**: Deploy to mainnet and test with internal team (1 week)
2. **Beta Testers**: Invite select users to test mainnet version (2 weeks)
3. **Gradual Rollout**: Roll out to 10%, 25%, 50%, then 100% of users

### 2. Feature Flags

Implement feature flags to control access to mainnet features:

```swift
// Example feature flag implementation
let useMainnet = UserDefaults.standard.bool(forKey: "useMainnet") || isInBetaGroup
```

### 3. Communication Plan

1. **Pre-Migration**: Inform users about upcoming migration
2. **During Migration**: Provide status updates
3. **Post-Migration**: Announce completion and new features

## Rollback Plan

In case of critical issues:

### 1. API Rollback

```bash
# Switch API back to devnet
export SUI_NETWORK=devnet
```

### 2. Client Rollback

Release an emergency update reverting to devnet configuration.

### 3. Data Recovery

If necessary, implement a plan to reconcile any data discrepancies between devnet and mainnet.

## Post-Migration Monitoring

### 1. Transaction Monitoring

Monitor:
- Transaction success rates
- Transaction confirmation times
- Token balances and transfers

### 2. Error Tracking

Track:
- API errors
- Client-side errors
- Contract interaction errors

### 3. User Feedback

Collect and respond to user feedback about the mainnet experience.

## Conclusion

Migrating from devnet to mainnet is a significant milestone for the Choir application. By following this guide and thoroughly testing each component, you can ensure a smooth transition with minimal disruption to users. Remember that mainnet operations involve real assets, so proceed with caution and prioritize security at every step.

=== File: docs/notification_system.md ===



==
notification_system
==


# Choir Notification System

This document describes the Choir notification system, focusing on citation notifications and their integration with the Choir token contract. It covers the architecture, implementation details, and troubleshooting procedures.

## Table of Contents

1. [System Overview](#system-overview)
2. [Architecture](#architecture)
3. [Implementation Details](#implementation-details)
4. [Contract Integration](#contract-integration)
5. [Testing](#testing)
6. [Troubleshooting](#troubleshooting)
7. [Future Improvements](#future-improvements)

## System Overview

The Choir notification system tracks and delivers notifications to users when their content is cited by others. These citations are also tied to the reward system, which mints Choir tokens to content creators when their work is cited.

Key features:
- Citation tracking and notification
- Integration with the Choir token contract
- In-app notification display
- Transaction history

## Architecture

The notification system consists of several components:

### Backend Components

1. **NotificationService**: Handles the creation and retrieval of notifications
2. **RewardsService**: Processes citation rewards and triggers notifications
3. **DatabaseClient**: Stores and retrieves notifications from Qdrant
4. **SuiService**: Interacts with the Sui blockchain and Choir contract

### Frontend Components

1. **TransactionService**: Fetches and displays notifications/transactions
2. **NotificationsView**: UI for displaying notifications
3. **WalletManager**: Manages wallet interactions and token transfers

### Data Flow

1. User cites content → Citation detected in yield phase
2. RewardsService processes citation → Issues token rewards
3. NotificationService creates notification → Stored in Qdrant
4. Client fetches notifications → Displayed in TransactionsView

## Implementation Details

### Notification Data Structure

```python
notification = {
    "type": "citation",  # or "self_citation"
    "recipient_wallet_address": author_wallet_address,
    "sender_wallet_address": citing_wallet_address,
    "vector_id": vector_id,
    "read": False,
    "created_at": datetime.now(UTC).isoformat()
}
```

### Database Storage

Notifications are stored in Qdrant with:
- Collection name: `notifications`
- Vector size: Same as message vectors (placeholder vectors used)
- Query filtering: By recipient wallet address

### API Endpoints

- `GET /api/notifications`: Retrieve notifications for a wallet
- `POST /api/notifications/{id}/read`: Mark notification as read

## Contract Integration

The notification system integrates with the Choir token contract through the SuiService:

### Citation Reward Flow

1. Citation detected in `issue_citation_rewards` function
2. SuiService mints tokens to author using:
   ```python
   mint_choir(recipient_address=author_wallet_address, amount=reward_amount)
   ```
3. NotificationService creates citation notification
4. Both operations (minting and notification) are logged

### Contract Dependencies

The notification system depends on:
- Package ID: Current devnet ID is `0xb33aeae469ce4bdea302e66bb0330fbe4d606776451c3099a5fc557923556a6a`
- Treasury Cap ID: Current devnet ID is `0x6eab9c65acf9b4001199ac98813951140417b5feff8a85218eddd14a62d14f37`

When the contract is redeployed, these IDs must be updated in:
- `api/app/services/sui_service.py`
- `Choir/Models/CoinType.swift`

## Testing

### Test Script

The `scripts/test_notifications.py` script tests the end-to-end notification flow:

1. Creates a test message with a wallet address
2. Creates a citation to that message
3. Verifies notification creation
4. Tests notification retrieval
5. Tests citation rewards through RewardsService

### Running Tests

```bash
cd api
source venv/bin/activate
python ../scripts/test_notifications.py
```

### Expected Output

A successful test will show:
- Test message creation
- Citation notification creation
- Notification retrieval
- Citation reward issuance
- Transaction confirmation

## Troubleshooting

### Common Issues

#### 1. Missing Notifications

**Symptoms**: Citations occur but no notifications appear

**Possible Causes**:
- RewardsService not calling NotificationService
- Database connection issues
- Missing wallet metadata in vectors

**Solutions**:
- Check logs for errors in `issue_citation_rewards`
- Verify Qdrant connection and collection existence
- Ensure vectors have `wallet_address` in metadata

#### 2. Contract Mismatch Errors

**Symptoms**: "SuiKit.SuiError error 26" or similar errors

**Possible Causes**:
- Mismatched contract IDs after redeployment
- Incorrect coin type identifier

**Solutions**:
- Update contract IDs in `sui_service.py`
- Update coin type in `CoinType.swift`
- Verify contract exists on the network

#### 3. Database Errors

**Symptoms**: "SortParams not available" or other Qdrant errors

**Possible Causes**:
- Qdrant client version mismatch
- Missing collections
- Query syntax errors

**Solutions**:
- Update Qdrant client or add compatibility code
- Check collection existence and create if missing
- Verify query syntax and parameters

### Debugging Tools

1. **Enhanced Logging**:
   - NotificationService logs notification creation
   - RewardsService logs reward issuance
   - DatabaseClient logs database operations

2. **Test Script**:
   - Use `test_notifications.py` to verify the flow

3. **Manual Verification**:
   - Check Qdrant collections directly
   - Verify Sui transactions on explorer

## Future Improvements

### Short-term Improvements

1. **Batch Processing**:
   - Process multiple notifications in a single operation
   - Reduce database calls

2. **Notification Categories**:
   - Add support for different notification types
   - Implement filtering by type

3. **Read Status Sync**:
   - Sync read status across devices
   - Implement unread count badge

### Long-term Improvements

1. **Push Notifications**:
   - Add optional push notification support
   - Implement device token management

2. **Notification Preferences**:
   - Allow users to customize notification settings
   - Implement notification frequency controls

3. **Rich Notifications**:
   - Add support for rich content in notifications
   - Include preview of cited content

## Conclusion

The Choir notification system is a critical component that connects user interactions with the reward system. By ensuring proper integration with the Choir token contract and maintaining consistent configuration across deployments, the system can reliably deliver notifications and rewards to users.

When deploying to new environments or redeploying the contract, special attention must be paid to updating the contract IDs and testing the end-to-end flow to ensure continued functionality.

=== File: docs/postchain_service_redesign.md ===



==
postchain_service_redesign
==


# PostChain Service Redesign: From Prototype to Production

VERSION postchain_service_redesign: 1.0 (Heart and Soul Refactor)

## Current State Analysis

The existing `langchain_workflow.py` has evolved into a complex, monolithic system with several critical issues:

### Technical Debt
- **Monolithic structure**: 1591 lines in a single file
- **Hardcoded phase logic**: No flexibility for user customization
- **No retry mechanisms**: Single points of failure
- **Limited tool integration**: Basic tool calling without MCP support
- **Provider-specific hacks**: Gemini/OpenAI message preparation scattered throughout
- **Performance bottlenecks**: No intelligent routing based on query complexity

### Missing Core Features
- **AWS Bedrock integration**: No LangChain adapter for cost optimization
- **Model Context Protocol (MCP)**: No support for Anthropic's tool standard
- **Client-side prompt editing**: Users can't customize or share phase configurations
- **Intelligent routing**: Simple queries get same treatment as complex analysis
- **Retry and forking**: No resilience for failed phases
- **Looping capabilities**: No iterative refinement

## Redesign Architecture

### Core Principles
1. **Modularity**: Each phase as independent, composable service
2. **Intelligence**: Route queries based on complexity and requirements
3. **Resilience**: Retry, fallback, and forking mechanisms
4. **Extensibility**: Plugin architecture for tools and models
5. **User Control**: Client-side prompt configuration and sharing

### New Service Structure

```
postchain/
├── core/
│   ├── orchestrator.py          # Main coordination logic
│   ├── router.py                # Intelligent query routing
│   ├── retry_manager.py         # Retry and fallback handling
│   ├── state_manager.py         # Thread state management
│   └── context_manager.py       # Token counting and model switching
├── phases/
│   ├── base_phase.py            # Abstract phase interface
│   ├── action_phase.py          # Fast initial response
│   ├── experience_phase.py      # Vector + web search
│   ├── analysis_phases.py       # I/O/U phases
│   └── yield_phase.py           # Final synthesis
├── providers/
│   ├── base_provider.py         # Abstract LLM provider
│   ├── openai_provider.py       # OpenAI/OpenRouter
│   ├── anthropic_provider.py    # Claude with MCP
│   ├── google_provider.py       # Gemini
│   └── bedrock_provider.py      # AWS Bedrock adapter
├── input/
│   ├── file_processor.py        # Multi-format file processing
│   ├── youtube_processor.py     # YouTube transcript extraction
│   ├── content_chunker.py       # Intelligent content segmentation
│   └── format_handlers/         # Specific format processors
│       ├── text_handler.py      # TXT, MD processing
│       ├── pdf_handler.py       # PDF extraction
│       ├── epub_handler.py      # EPUB processing
│       └── media_handler.py     # Audio/video transcription
├── tools/
│   ├── mcp_client.py            # Model Context Protocol client
│   ├── tool_registry.py         # Dynamic tool discovery
│   └── tool_executor.py         # Safe tool execution
└── config/
    ├── phase_configs.py         # Default phase prompts
    ├── routing_rules.py         # Query complexity routing
    ├── model_configs.py         # Provider configurations
    └── context_configs.py       # Context window management
```

## Intelligent Query Routing

### Complexity Classification
```python
class QueryComplexity(Enum):
    SIMPLE = "simple"        # "What's the weather?"
    MODERATE = "moderate"    # "Explain quantum computing"
    COMPLEX = "complex"      # Novel ideas requiring deep analysis

class QueryRouter:
    async def classify_query(self, query: str, context: List[Message]) -> QueryComplexity:
        # Use fast classifier to determine complexity
        # Factors: length, novelty, specificity, context requirements

    async def route_query(self, query: str, complexity: QueryComplexity) -> ExecutionPlan:
        if complexity == QueryComplexity.SIMPLE:
            return SimpleExecutionPlan()  # Action + Web search only
        elif complexity == QueryComplexity.MODERATE:
            return ModerateExecutionPlan()  # Action + Experience + Yield
        else:
            return ComplexExecutionPlan()  # Full AEIOU-Y with analysis
```

### Execution Plans
```python
@dataclass
class ExecutionPlan:
    phases: List[PhaseConfig]
    max_retries: int
    fallback_models: List[ModelConfig]
    tool_permissions: Set[str]
    reward_multiplier: float

class SimpleExecutionPlan(ExecutionPlan):
    def __init__(self):
        super().__init__(
            phases=[ActionPhase(), WebSearchPhase()],
            max_retries=1,
            fallback_models=[],
            tool_permissions={"web_search"},
            reward_multiplier=0.1  # Lower rewards for simple queries
        )

class ComplexExecutionPlan(ExecutionPlan):
    def __init__(self):
        super().__init__(
            phases=[ActionPhase(), ExperiencePhase(), IntentionPhase(),
                   ObservationPhase(), UnderstandingPhase(), YieldPhase()],
            max_retries=3,
            fallback_models=[GPT4_FALLBACK, CLAUDE_FALLBACK],
            tool_permissions={"web_search", "vector_search", "mcp_tools"},
            reward_multiplier=1.0  # Full rewards for complex analysis
        )
```

## Phase Modularization

### Base Phase Interface
```python
from abc import ABC, abstractmethod

class BasePhase(ABC):
    def __init__(self, config: PhaseConfig):
        self.config = config
        self.retry_manager = RetryManager()

    @abstractmethod
    async def execute(self, state: PostChainState) -> PhaseResult:
        pass

    async def execute_with_retry(self, state: PostChainState) -> PhaseResult:
        return await self.retry_manager.execute_with_retry(
            self.execute, state, max_retries=self.config.max_retries
        )
```

### Client-Side Phase Configuration
```python
@dataclass
class PhaseConfig:
    name: str
    prompt_template: str
    model_config: ModelConfig
    tool_permissions: Set[str]
    max_retries: int
    timeout_seconds: int

    @classmethod
    def from_client_config(cls, client_config: dict) -> 'PhaseConfig':
        # Allow clients to customize phase behavior
        return cls(**client_config)

    def to_shareable_config(self) -> dict:
        # Export config for sharing between users
        return {
            "name": self.name,
            "prompt_template": self.prompt_template,
            "model": self.model_config.to_dict(),
            "tools": list(self.tool_permissions)
        }
```

## AWS Bedrock Integration

### LangChain Adapter
```python
from langchain_aws import BedrockLLM
from langchain_core.language_models import BaseLLM

class BedrockProvider(BaseLLMProvider):
    def __init__(self, config: BedrockConfig):
        self.config = config
        self.client = BedrockLLM(
            model_id=config.model_id,
            region_name=config.region,
            credentials_profile_name=config.profile
        )

    async def generate(self, messages: List[BaseMessage], **kwargs) -> AIMessage:
        # Cost optimization: use cheaper models for simple queries
        if kwargs.get('complexity') == QueryComplexity.SIMPLE:
            model_id = self.config.cheap_model_id
        else:
            model_id = self.config.premium_model_id

        return await self.client.agenerate([messages])

    def estimate_cost(self, messages: List[BaseMessage]) -> float:
        # Calculate estimated cost for budget management
        token_count = self._estimate_tokens(messages)
        return token_count * self.config.cost_per_token
```

## Model Context Protocol (MCP) Integration

### MCP Client Implementation
```python
class MCPClient:
    def __init__(self, server_configs: List[MCPServerConfig]):
        self.servers = {}
        for config in server_configs:
            self.servers[config.name] = MCPServer(config)

    async def discover_tools(self) -> List[MCPTool]:
        tools = []
        for server in self.servers.values():
            server_tools = await server.list_tools()
            tools.extend(server_tools)
        return tools

    async def execute_tool(self, tool_name: str, args: dict) -> ToolResult:
        server = self._find_server_for_tool(tool_name)
        return await server.call_tool(tool_name, args)

class AnthropicMCPProvider(BaseLLMProvider):
    def __init__(self, config: AnthropicConfig):
        super().__init__(config)
        self.mcp_client = MCPClient(config.mcp_servers)

    async def generate_with_tools(self, messages: List[BaseMessage]) -> AIMessage:
        available_tools = await self.mcp_client.discover_tools()

        response = await self.client.generate(
            messages=messages,
            tools=available_tools
        )

        if response.tool_calls:
            tool_results = []
            for tool_call in response.tool_calls:
                result = await self.mcp_client.execute_tool(
                    tool_call.name, tool_call.args
                )
                tool_results.append(result)

            # Continue conversation with tool results
            return await self._continue_with_tool_results(messages, response, tool_results)

        return response
```

## Retry and Forking Mechanisms

### Retry Manager
```python
class RetryManager:
    def __init__(self, config: RetryConfig):
        self.config = config

    async def execute_with_retry(self,
                               func: Callable,
                               *args,
                               max_retries: int = 3) -> Any:
        last_exception = None

        for attempt in range(max_retries + 1):
            try:
                return await func(*args)
            except RetryableError as e:
                last_exception = e
                if attempt < max_retries:
                    await self._handle_retry(e, attempt)
                    continue
                else:
                    break
            except NonRetryableError as e:
                # Don't retry for certain errors
                raise e

        # All retries exhausted
        return await self._handle_final_failure(last_exception)

    async def _handle_retry(self, error: Exception, attempt: int):
        # Exponential backoff
        delay = self.config.base_delay * (2 ** attempt)
        await asyncio.sleep(delay)

        # Model switching on retry
        if isinstance(error, ModelError):
            await self._switch_to_fallback_model()

    async def fork_execution(self,
                           func: Callable,
                           *args,
                           fork_configs: List[dict]) -> List[Any]:
        # Execute multiple versions in parallel
        tasks = []
        for config in fork_configs:
            task = asyncio.create_task(func(*args, **config))
            tasks.append(task)

        results = await asyncio.gather(*tasks, return_exceptions=True)
        return [r for r in results if not isinstance(r, Exception)]
```

## Implementation Phases

### Phase 1: Core Refactoring (Week 1-2)
1. **Extract phase classes** from monolithic workflow
2. **Implement base interfaces** for phases and providers
3. **Create orchestrator** for phase coordination
4. **Add basic retry mechanisms**

### Phase 2: Intelligent Routing (Week 3)
1. **Implement query classifier** for complexity detection
2. **Create execution plans** for different query types
3. **Add reward scaling** based on complexity
4. **Optimize for simple queries**

### Phase 3: Provider Integration (Week 4)
1. **AWS Bedrock adapter** with cost optimization
2. **Enhanced Anthropic provider** with MCP support
3. **Tool registry** for dynamic discovery
4. **Provider fallback chains**

### Phase 4: Client Configuration (Week 5)
1. **Client-side prompt editing** interface
2. **Configuration sharing** between users
3. **Custom execution plans**
4. **A/B testing framework** for prompts

### Phase 5: Advanced Features (Week 6+)
1. **Looping and iteration** capabilities
2. **Multi-model consensus** for critical decisions
3. **Performance monitoring** and optimization
4. **Advanced tool orchestration**

## File Input Processing

### Multi-Format Support
```python
class FileProcessor:
    def __init__(self):
        self.handlers = {
            '.txt': TextHandler(),
            '.md': MarkdownHandler(),
            '.pdf': PDFHandler(),
            '.epub': EPUBHandler(),
            '.docx': DocxHandler(),
            '.html': HTMLHandler()
        }

    async def process_file(self, file_path: str, file_type: str = None) -> ProcessedContent:
        if file_type is None:
            file_type = self._detect_file_type(file_path)

        handler = self.handlers.get(file_type)
        if not handler:
            raise UnsupportedFileTypeError(f"File type {file_type} not supported")

        raw_content = await handler.extract_content(file_path)

        # Intelligent chunking based on content structure
        chunks = await self._chunk_content(raw_content, file_type)

        return ProcessedContent(
            original_content=raw_content,
            chunks=chunks,
            metadata=handler.extract_metadata(file_path),
            file_type=file_type
        )

class PDFHandler(BaseFileHandler):
    async def extract_content(self, file_path: str) -> str:
        # Use PyMuPDF for better text extraction
        import fitz
        doc = fitz.open(file_path)
        text = ""
        for page in doc:
            text += page.get_text()
        return text

    def extract_metadata(self, file_path: str) -> dict:
        doc = fitz.open(file_path)
        return {
            "page_count": doc.page_count,
            "title": doc.metadata.get("title", ""),
            "author": doc.metadata.get("author", ""),
            "creation_date": doc.metadata.get("creationDate", "")
        }

class EPUBHandler(BaseFileHandler):
    async def extract_content(self, file_path: str) -> str:
        import ebooklib
        from ebooklib import epub

        book = epub.read_epub(file_path)
        content = []

        for item in book.get_items():
            if item.get_type() == ebooklib.ITEM_DOCUMENT:
                content.append(item.get_content().decode('utf-8'))

        return '\n\n'.join(content)
```

### YouTube Transcript Processing
```python
class YouTubeProcessor:
    def __init__(self):
        self.transcript_api = YouTubeTranscriptApi()

    async def process_youtube_url(self, url: str) -> ProcessedContent:
        video_id = self._extract_video_id(url)

        try:
            # Try to get transcript
            transcript = self.transcript_api.get_transcript(video_id)
            content = self._format_transcript(transcript)

            # Get video metadata
            metadata = await self._get_video_metadata(video_id)

            # Chunk by timestamp for better context
            chunks = self._chunk_by_timestamp(transcript)

            return ProcessedContent(
                original_content=content,
                chunks=chunks,
                metadata=metadata,
                file_type="youtube_transcript"
            )

        except TranscriptsDisabled:
            # Fallback to audio transcription
            return await self._transcribe_audio(video_id)

    def _format_transcript(self, transcript: List[dict]) -> str:
        formatted = []
        for entry in transcript:
            timestamp = self._format_timestamp(entry['start'])
            text = entry['text']
            formatted.append(f"[{timestamp}] {text}")
        return '\n'.join(formatted)

    async def _transcribe_audio(self, video_id: str) -> ProcessedContent:
        # Use Whisper or similar for audio transcription
        audio_path = await self._download_audio(video_id)
        transcript = await self._whisper_transcribe(audio_path)

        return ProcessedContent(
            original_content=transcript,
            chunks=self._chunk_by_sentences(transcript),
            metadata={"source": "audio_transcription"},
            file_type="audio_transcript"
        )
```

## Context Management System

### Token Counting and Model Selection
```python
class ContextManager:
    def __init__(self):
        self.token_counters = {
            'openai': tiktoken.encoding_for_model,
            'anthropic': AnthropicTokenCounter(),
            'google': GoogleTokenCounter()
        }

        # Model context windows (tokens)
        self.context_limits = {
            'gpt-4': 128_000,
            'gpt-4-turbo': 128_000,
            'gpt-4o': 128_000,
            'claude-3-haiku': 200_000,
            'claude-3-sonnet': 200_000,
            'claude-3-opus': 200_000,
            'claude-3.5-sonnet': 200_000,
            'gemini-1.5-pro': 1_000_000,
            'gemini-1.5-flash': 1_000_000,
            'gpt-4o-mini': 128_000
        }

        # High-context fallback models
        self.high_context_models = {
            'openai': 'gpt-4-turbo',
            'anthropic': 'claude-3.5-sonnet',
            'google': 'gemini-1.5-pro'
        }

    async def check_context_fit(self,
                              messages: List[BaseMessage],
                              model_config: ModelConfig) -> ContextCheckResult:
        token_count = await self._count_tokens(messages, model_config)
        context_limit = self.context_limits.get(model_config.model_name, 128_000)

        # Reserve 20% for response
        usable_limit = int(context_limit * 0.8)

        if token_count <= usable_limit:
            return ContextCheckResult(
                fits=True,
                token_count=token_count,
                limit=context_limit,
                recommended_action=None
            )
        else:
            return ContextCheckResult(
                fits=False,
                token_count=token_count,
                limit=context_limit,
                recommended_action=self._recommend_action(token_count, model_config)
            )

    def _recommend_action(self, token_count: int, model_config: ModelConfig) -> str:
        # Try high-context model first
        high_context_model = self.high_context_models.get(model_config.provider)
        if high_context_model:
            high_context_limit = self.context_limits.get(high_context_model, 128_000)
            if token_count <= int(high_context_limit * 0.8):
                return f"switch_to_high_context:{high_context_model}"

        # If still too large, recommend chunking
        return "chunk_content"

    async def handle_context_overflow(self,
                                    messages: List[BaseMessage],
                                    model_config: ModelConfig,
                                    error: Exception) -> ModelConfig:
        """Handle context overflow by switching to high-context model"""

        if "context" in str(error).lower() or "token" in str(error).lower():
            # This is a context overflow error
            high_context_model = self.high_context_models.get(model_config.provider)

            if high_context_model and high_context_model != model_config.model_name:
                logger.info(f"Context overflow detected. Switching from {model_config.model_name} to {high_context_model}")

                new_config = model_config.copy()
                new_config.model_name = high_context_model

                # Verify the new model can handle the context
                check_result = await self.check_context_fit(messages, new_config)
                if check_result.fits:
                    return new_config
                else:
                    # Even high-context model can't handle it, need chunking
                    raise ContextTooLargeError("Content exceeds even high-context model limits")
            else:
                raise ContextTooLargeError("No high-context fallback available")
        else:
            # Not a context error, re-raise
            raise error

### Intelligent Content Chunking
```python
class ContentChunker:
    def __init__(self, context_manager: ContextManager):
        self.context_manager = context_manager

    async def chunk_for_model(self,
                            content: str,
                            model_config: ModelConfig,
                            overlap_ratio: float = 0.1) -> List[ContentChunk]:

        context_limit = self.context_manager.context_limits.get(
            model_config.model_name, 128_000
        )

        # Use 60% of context for content, 20% for system/user prompts, 20% for response
        chunk_size = int(context_limit * 0.6)
        overlap_size = int(chunk_size * overlap_ratio)

        # Intelligent chunking based on content structure
        if self._has_clear_structure(content):
            return await self._structure_based_chunking(content, chunk_size, overlap_size)
        else:
            return await self._semantic_chunking(content, chunk_size, overlap_size)

    async def _structure_based_chunking(self, content: str, chunk_size: int, overlap_size: int) -> List[ContentChunk]:
        # Split by headers, paragraphs, sentences
        sections = self._split_by_structure(content)
        chunks = []
        current_chunk = ""
        current_tokens = 0

        for section in sections:
            section_tokens = await self._estimate_tokens(section)

            if current_tokens + section_tokens <= chunk_size:
                current_chunk += section
                current_tokens += section_tokens
            else:
                if current_chunk:
                    chunks.append(ContentChunk(
                        content=current_chunk,
                        token_count=current_tokens,
                        chunk_index=len(chunks)
                    ))

                # Start new chunk with overlap
                overlap_content = self._get_overlap(current_chunk, overlap_size)
                current_chunk = overlap_content + section
                current_tokens = await self._estimate_tokens(current_chunk)

        if current_chunk:
            chunks.append(ContentChunk(
                content=current_chunk,
                token_count=current_tokens,
                chunk_index=len(chunks)
            ))

        return chunks

    async def _semantic_chunking(self, content: str, chunk_size: int, overlap_size: int) -> List[ContentChunk]:
        # Use embeddings to find semantic boundaries
        sentences = self._split_into_sentences(content)
        embeddings = await self._get_sentence_embeddings(sentences)

        # Find semantic breaks using cosine similarity
        semantic_breaks = self._find_semantic_breaks(embeddings, threshold=0.7)

        # Create chunks respecting semantic boundaries
        chunks = []
        current_chunk_sentences = []
        current_tokens = 0

        for i, sentence in enumerate(sentences):
            sentence_tokens = await self._estimate_tokens(sentence)

            if (current_tokens + sentence_tokens <= chunk_size and
                i not in semantic_breaks):
                current_chunk_sentences.append(sentence)
                current_tokens += sentence_tokens
            else:
                if current_chunk_sentences:
                    chunk_content = ' '.join(current_chunk_sentences)
                    chunks.append(ContentChunk(
                        content=chunk_content,
                        token_count=current_tokens,
                        chunk_index=len(chunks)
                    ))

                # Start new chunk
                current_chunk_sentences = [sentence]
                current_tokens = sentence_tokens

        return chunks
```

This redesign transforms the postchain from a prototype into a production-ready system that can handle everything from simple weather queries to complex novel analysis of large documents and videos, while intelligently managing context windows and automatically switching to appropriate models when needed.

=== File: docs/postchain_temporal_logic.md ===



==
postchain_temporal_logic
==


# PostChain Temporal Logic: The AEIOU-Y Flow in Time

VERSION postchain_temporal_logic: 8.0 (Qdrant-Sui MVP Focus)

The PostChain (AEIOU-Y) is not just a sequence of phases executed within the Choir backend; it's a carefully orchestrated **temporal flow**. Each phase embodies a distinct relationship to time, contributing to the overall coherence and effectiveness of the AI-driven conversational workflow. Understanding this temporal logic is key to grasping how the PostChain creates a dynamic and context-aware conversational experience, even within the MVP's streamlined architecture.

**Each Phase Embodies a Distinct Temporal Focus:**

The AEIOU-Y phases, implemented sequentially in the `langchain_workflow.py`, are designed to process user input and generate responses by systematically engaging with different temporal dimensions of the conversational context stored primarily in Qdrant:

1.  **Action Phase: Immediate Present - The Now of Interaction**

    *   **Temporal Focus:** The **immediate present moment** of user interaction. The Action phase function is concerned with the "now" – the user's current input, the immediate context, and the need for an *initial, direct response*.
    *   **Temporal Logic:** **Reaction and Responsiveness.** This phase is designed to be highly responsive. It generates a quick, initial response to the user's input, setting the stage for the more deliberative phases that follow. It operates in the *present moment*, acknowledging the user's immediate need for interaction.
    *   **Role within Workflow:** The **Action phase function** is the *first point of contact* in the PostChain workflow, receiving the user's prompt and initiating the process. It leverages AI models (via `langchain_utils`) to generate a quick initial response and passes the context to the next phase.

2.  **Experience Phase: Past Knowledge - Drawing on Memory and History**

    *   **Temporal Focus:** The **past** – the accumulated knowledge, history, and prior experiences relevant to the current conversation, primarily stored in the Qdrant `choir` collection.
    *   **Temporal Logic:** **Memory and Contextual Recall.** This phase is about bringing the *past into the present*. It leverages memory (Qdrant vector search on the `choir` collection) to provide context, depth, and relevance. It draws on the *lessons of the past* (relevant prior messages) to inform the current interaction and calculates novelty/similarity scores.
    *   **Role within Workflow:** The **Experience phase function** acts as the *memory and knowledge retrieval engine*. It queries Qdrant for relevant priors, potentially uses external search tools, calculates scores, and enriches the context passed to the next phase.

3.  **Intention Phase: Desired Future - Aligning with User Goals and Purpose**

    *   **Temporal Focus:** The **future** – the user's *intended goals, desired outcomes, and future trajectory* of the conversation, potentially informed by the Qdrant `intention_memory` collection.
    *   **Temporal Logic:** **Anticipation and Goal-Orientedness.** This phase is about shaping the *present interaction* to achieve a *desired future state*. It leverages AI models to infer user intent, identify goals (potentially storing/retrieving from `intention_memory`), and guide the conversation towards a productive outcome. It orients the present towards a *purposeful future*.
    *   **Role within Workflow:** The **Intention phase function** acts as the *intent modeling and goal alignment engine*. It analyzes user input and context, infers intentions (interacting with `intention_memory` via the API/`database.py`), and passes the refined understanding of goals forward.

4.  **Observation Phase: Future Preservation - Recording and Structuring Knowledge for the Long Term**

    *   **Temporal Focus:** The **long-term future** – the need to *preserve, structure, and organize knowledge* generated in the current conversation within the specific thread context, potentially using the Qdrant `observation_memory` collection.
    *   **Temporal Logic:** **Preservation and Knowledge Structuring.** This phase is about making the *present conversation valuable for the future* within its thread. It focuses on capturing key insights or summaries (potentially storing/retrieving from `observation_memory`) to enhance the long-term value and retrievability of thread-specific knowledge. It prepares the *present for the future*.
    *   **Role within Workflow:** The **Observation phase function** acts as the *thread-level knowledge structuring engine*. It analyzes the conversation, identifies key concepts or summaries relevant to the thread (interacting with `observation_memory` via the API/`database.py`), and passes this structured understanding forward.

5.  **Understanding Phase: Temporal Integration - Synthesizing Past, Present, and Future**

    *   **Temporal Focus:** **All temporal dimensions – past, present, and future – are integrated and synthesized**. This phase acts as the central temporal hub, bringing together insights from previous phases and Qdrant memory collections.
    *   **Temporal Logic:** **Synthesis and Contextual Awareness.** This phase is about creating a *coherent and integrated understanding* of the conversation across time. It synthesizes the immediate present (Action), past knowledge (Experience), desired future (Intention), and thread context (Observation) to make informed decisions about the flow. It may also trigger pruning of stale entries in `intention_memory` or `observation_memory`. It achieves *temporal coherence*.
    *   **Role within Workflow:** The **Understanding phase function** acts as the *contextual synthesis and decision-making engine*. It evaluates the enriched context, potentially filters information (triggering Qdrant deletes via the API/`database.py`), and passes the refined, integrated context to the final phase.

6.  **Yield Phase: Process Completion - Bringing the Workflow to a Temporally Defined End**

    *   **Temporal Focus:** The **defined end point** of the current PostChain workflow cycle – the moment when a response is generated.
    *   **Temporal Logic:** **Completion and Cyclicality.** This phase is about *bringing the current cycle to a close*. It generates the final user-facing response based on the integrated understanding, bundles all intermediate phase outputs, and prepares the data structure to be saved in the Qdrant `choir` collection. It marks the *end of the present cycle*. (Note: Recursion logic might be simplified or deferred in MVP).
    *   **Role within Workflow:** The **Yield phase function** acts as the *output formatting and finalization engine*. It formats the final response, gathers all preceding phase outputs, and returns the complete data structure to the API orchestrator for persistence in Qdrant and triggering the reward mechanism.

**The AEIOU-Y Flow as a Temporal Dance:**

The PostChain, viewed through its temporal logic, remains a carefully choreographed **dance through time** within the workflow. Each phase function takes its turn to engage with a different temporal dimension, building upon the previous phase and contributing to the overall temporal coherence of the conversational experience. It's a dynamic process where the AI, guided by the workflow and interacting with Qdrant, builds knowledge and understanding step by step, phase by phase.

By understanding this temporal logic, developers can implement more effective and nuanced AI phase functions within the Choir workflow, creating conversational experiences that are not just intelligent but also deeply attuned to the temporal nature of human communication and knowledge creation.

=== File: docs/postchain_ui_redesign.md ===



==
postchain_ui_redesign
==


# PostChain UI Redesign: Intelligent Content Categorization

VERSION postchain_ui_redesign: 1.0 (From Prototype to Alpha)

## Current State Analysis

The existing postchain carousel is more prototype than alpha, with several issues:
- **Flat content presentation**: All phase content treated equally regardless of user value
- **Poor information hierarchy**: Critical content mixed with debug information
- **Overwhelming detail**: Users see analysis they don't care about
- **Inefficient navigation**: Pagination doesn't respect content importance
- **Missing tool integration**: No support for Anthropic's Model Context Protocol

## Content Categorization Framework

### Primary Content (Always Visible)
1. **Fast Initial Response** (Action Phase)
   - Immediate AI response to user query
   - Highest priority, always shown first
   - Clean, conversational presentation

2. **Final Response** (Yield Phase)
   - Comprehensive, citation-enhanced response
   - Primary destination for most users
   - Rich formatting with embedded citations

### Secondary Content (Contextual Display)
3. **Novelty Rewards Distribution**
   - Token rewards for original contributions
   - Show prominently when rewards are earned
   - Animate/highlight to celebrate user achievement

4. **Citation Rewards**
   - Rewards for being cited by others
   - Display when user's content helps others
   - Link to original cited content

### Tertiary Content (Collapsible/Hidden by Default)
5. **Sources** (Experience Phases)
   - Prior prompts from vector search
   - Web search results
   - Tool call results
   - Collapsed by default, expandable on demand

6. **Analysis** (Intention, Observation, Understanding)
   - AI reasoning and pattern analysis
   - Hidden by default - most users don't care
   - Available for power users who want to see "AI thinking"

7. **Tool Calls** (Model Context Protocol)
   - Anthropic MCP tool interactions
   - Structured display of tool inputs/outputs
   - Expandable sections for debugging

## Redesigned UI Architecture

### Smart Card Layout
```
┌─────────────────────────────────────┐
│ Fast Response (Action)              │ ← Always visible, immediate
│ "Here's what I think..."            │
└─────────────────────────────────────┘

┌─────────────────────────────────────┐
│ 🎉 Novelty Reward: +15 CHOIR        │ ← Conditional, celebrated
│ Your insight was original!          │
└─────────────────────────────────────┘

┌─────────────────────────────────────┐
│ Final Response (Yield)              │ ← Primary destination
│ Enhanced response with citations... │
│ [1] [2] [3] ← Clickable citations   │
└─────────────────────────────────────┘

▼ Sources (3 found) ← Collapsible
┌─────────────────────────────────────┐
│ • Prior conversation about X        │
│ • Web result: "Article title"       │
│ • Tool call: weather_api()          │
└─────────────────────────────────────┘

▼ Analysis ← Hidden by default
┌─────────────────────────────────────┐
│ Intent: User wants to understand... │
│ Patterns: Similar to previous...    │
└─────────────────────────────────────┘
```

### Navigation Redesign: Scroll-Triggered Pagination

**Frameless Paginated Scrolling**:
- Remove visual frame constraints
- Scroll gesture triggers page transitions
- Whole pages slide into focus smoothly
- Each page can be internally scrollable

**Page Transition Mechanics**:
```
User scrolls down → Next logical page slides up into view
User scrolls up → Previous page slides down into view
Pages snap to focus position automatically
Individual pages can scroll internally if content overflows
```

**Smart Page Boundaries**:
- Primary content (Fast + Final response) = Page 1
- Rewards + Sources = Page 2 (if present)
- Analysis + Tool calls = Page 3 (if expanded)
- Dynamic page creation based on available content

### Content Intelligence

**Dynamic Visibility**:
```swift
struct ContentVisibility {
    let showRewards: Bool        // Only when rewards > 0
    let showSources: Bool        // Only when sources exist
    let showAnalysis: Bool       // User preference, default false
    let showToolCalls: Bool      // Only when tools were used
}
```

**User Preferences**:
- Toggle for showing analysis phases
- Preference for expanded vs collapsed sources
- Power user mode for full detail

## Implementation Strategy

### Phase 1: Content Categorization
1. **Classify existing content** into primary/secondary/tertiary
2. **Implement conditional rendering** based on content availability
3. **Add collapse/expand functionality** for tertiary content

### Phase 2: Vertical Layout
1. **Replace carousel with vertical scroll**
2. **Implement smart pagination** for long content
3. **Add smooth animations** for expand/collapse

### Phase 3: Tool Integration
1. **Add Model Context Protocol support**
2. **Structured tool call display**
3. **Interactive tool result exploration**

### Phase 4: Intelligence Features
1. **User preference system**
2. **Adaptive content display** based on usage patterns
3. **Smart notifications** for rewards and citations

## Technical Implementation

### Scroll-Triggered Pagination View
```swift
struct ScrollPaginatedPostchainView: View {
    let message: Message
    @State private var currentPageIndex: Int = 0
    @State private var expandedSections: Set<ContentSection> = []
    @State private var dragOffset: CGFloat = 0

    var availablePages: [PostchainPage] {
        generatePages(for: message, expandedSections: expandedSections)
    }

    var body: some View {
        GeometryReader { geometry in
            ZStack {
                ForEach(Array(availablePages.enumerated()), id: \.offset) { index, page in
                    PostchainPageView(page: page, message: message)
                        .frame(width: geometry.size.width, height: geometry.size.height)
                        .offset(y: calculatePageOffset(for: index, containerHeight: geometry.size.height))
                        .opacity(calculatePageOpacity(for: index))
                        .allowsHitTesting(index == currentPageIndex)
                }
            }
            .gesture(
                DragGesture()
                    .onChanged { value in
                        dragOffset = value.translation.y
                    }
                    .onEnded { value in
                        handlePageTransition(dragValue: value, containerHeight: geometry.size.height)
                    }
            )
        }
        .clipped()
    }
}
```

### Page Generation Logic
```swift
struct PostchainPage {
    let id: String
    let sections: [ContentSection]
    let isScrollable: Bool
    let title: String?
}

func generatePages(for message: Message, expandedSections: Set<ContentSection>) -> [PostchainPage] {
    var pages: [PostchainPage] = []

    // Page 1: Primary Content (always present)
    pages.append(PostchainPage(
        id: "primary",
        sections: [.fastResponse, .finalResponse],
        isScrollable: true,
        title: nil
    ))

    // Page 2: Secondary Content (conditional)
    var secondaryContent: [ContentSection] = []
    if message.hasRewards { secondaryContent.append(.rewards) }
    if message.hasSources { secondaryContent.append(.sources) }

    if !secondaryContent.isEmpty {
        pages.append(PostchainPage(
            id: "secondary",
            sections: secondaryContent,
            isScrollable: true,
            title: "Details"
        ))
    }

    // Page 3: Analysis (only if expanded)
    if expandedSections.contains(.analysis) || expandedSections.contains(.toolCalls) {
        pages.append(PostchainPage(
            id: "analysis",
            sections: [.analysis, .toolCalls],
            isScrollable: true,
            title: "Analysis"
        ))
    }

    return pages
}

### Page Transition Mechanics
```swift
private func calculatePageOffset(for index: Int, containerHeight: CGFloat) -> CGFloat {
    let baseOffset = CGFloat(index - currentPageIndex) * containerHeight
    return baseOffset + dragOffset
}

private func handlePageTransition(dragValue: DragGesture.Value, containerHeight: CGFloat) {
    let threshold: CGFloat = containerHeight * 0.3
    let velocity = dragValue.predictedEndTranslation.y

    withAnimation(.spring(response: 0.4, dampingFraction: 0.8)) {
        if dragValue.translation.y < -threshold || velocity < -500 {
            // Swipe up - next page
            currentPageIndex = min(currentPageIndex + 1, availablePages.count - 1)
        } else if dragValue.translation.y > threshold || velocity > 500 {
            // Swipe down - previous page
            currentPageIndex = max(currentPageIndex - 1, 0)
        }

        dragOffset = 0
    }
}

### Individual Page Scrolling
```swift
struct PostchainPageView: View {
    let page: PostchainPage
    let message: Message

    var body: some View {
        if page.isScrollable {
            ScrollView {
                LazyVStack(spacing: 16) {
                    ForEach(page.sections, id: \.self) { section in
                        ContentSectionView(section: section, message: message)
                    }
                }
                .padding()
            }
        } else {
            VStack(spacing: 16) {
                ForEach(page.sections, id: \.self) { section in
                    ContentSectionView(section: section, message: message)
                }
            }
            .padding()
        }
    }
}
```

### Scroll Behavior Logic
```
Page-level scrolling (between pages):
- Triggered by drag gestures with velocity/distance thresholds
- Smooth spring animations between page transitions
- Snap-to-page behavior ensures clean focus

Content-level scrolling (within pages):
- Standard ScrollView behavior when content exceeds page height
- Only active when page is in focus (currentPageIndex)
- Scroll indicators appear when content is scrollable
```
```

## Audio-First Experience: Text-to-Speech Integration

### The Audio Transformation
Text-to-speech (TTS) transforms the postchain from a visual interface into an **audio experience**, enabling:
- **Hands-free operation**: Users can listen while driving, walking, exercising
- **Screen-free usage**: Complete interaction without looking at device
- **Accessibility**: Full experience for visually impaired users
- **Multitasking**: Consume AI insights while doing other activities

### Audio Content Prioritization
```
🔊 Primary Audio (Always Read):
- Fast initial response
- Final response with key insights
- Reward notifications ("You earned 15 CHOIR tokens!")

🔇 Secondary Audio (On Request):
- Source summaries ("Found 3 relevant documents")
- Citation explanations
- Analysis insights (condensed)

⏭️ Skippable Audio:
- Technical details
- Debug information
- Tool call specifics
```

### Voice Navigation Commands
```
"Next" / "Continue" → Move to next logical content
"Skip" → Skip current section
"Repeat" → Re-read current section
"Details" → Read secondary content
"Sources" → Read source summaries
"Pause" / "Stop" → Pause audio playback
"Speed up" / "Slow down" → Adjust reading speed
```

### Smart Audio Adaptation
```swift
struct AudioPostchainPresenter {
    func generateAudioScript(for message: Message) -> AudioScript {
        var script = AudioScript()

        // Primary content - always included
        script.addSection(.fastResponse, priority: .high,
                         text: cleanForAudio(message.actionPhase))
        script.addSection(.finalResponse, priority: .high,
                         text: cleanForAudio(message.yieldPhase))

        // Rewards - celebratory tone
        if let rewards = message.noveltyReward {
            script.addSection(.rewards, priority: .medium,
                             text: "Congratulations! You earned \(rewards.amount) CHOIR tokens for your original insight!",
                             tone: .celebratory)
        }

        // Sources - condensed summaries
        if !message.vectorSearchResults.isEmpty {
            let sourceCount = message.vectorSearchResults.count
            script.addSection(.sources, priority: .low,
                             text: "I found \(sourceCount) relevant sources to inform this response.",
                             expandable: true)
        }

        return script
    }
}
```

## User Experience Goals

### For Audio Users (New Category)
- **Hands-free operation**: Complete interaction through voice and audio
- **Contextual reading**: Smart emphasis and pacing for different content types
- **Efficient consumption**: Skip unnecessary details, focus on insights
- **Natural flow**: Audio that feels like conversation, not robotic reading

### For Visual Users
- **Immediate value**: Fast response appears instantly
- **Clean interface**: No overwhelming technical details
- **Celebration**: Rewards are prominently displayed when earned
- **Simple navigation**: Scroll-triggered pagination, no complex gestures

### For Power Users
- **Multi-modal access**: Both visual and audio interfaces for all content
- **Full transparency**: All analysis available on demand
- **Customization**: Preferences for audio speed, voice, content filtering
- **Debug access**: Complete phase information when needed

### For All Users
- **Performance**: Faster rendering with intelligent content loading
- **Accessibility**: Screen reader support + native TTS integration
- **Consistency**: Predictable experience across visual and audio modes
- **Delight**: Smooth animations, reward celebrations, and natural-sounding audio

## Audio Implementation Strategy

### Phase 1: Basic TTS Integration
```swift
import AVFoundation

class PostchainAudioManager: ObservableObject {
    private let synthesizer = AVSpeechSynthesizer()
    @Published var isPlaying = false
    @Published var currentSection: ContentSection?

    func playAudioScript(_ script: AudioScript) {
        for section in script.sections {
            let utterance = AVSpeechUtterance(string: section.text)
            utterance.voice = selectVoice(for: section.tone)
            utterance.rate = section.readingSpeed
            synthesizer.speak(utterance)
        }
    }
}
```

### Phase 2: Voice Control Integration
- Speech recognition for navigation commands
- Wake word detection ("Hey Choir")
- Voice interruption handling
- Context-aware command interpretation

### Phase 3: Advanced Audio Features
- Multiple voice personalities for different content types
- Emotional tone adaptation (celebratory for rewards, neutral for analysis)
- Background audio processing
- Offline TTS capability

### Audio Content Optimization
```swift
func cleanForAudio(_ text: String) -> String {
    return text
        .replacingOccurrences(of: "**", with: "") // Remove markdown bold
        .replacingOccurrences(of: "*", with: "")  // Remove markdown italic
        .replacingOccurrences(of: "#", with: "")  // Remove headers
        .replacingOccurrences(of: "[", with: "")  // Remove citation brackets
        .replacingOccurrences(of: "]", with: "")
        .addingNaturalPauses()                    // Add strategic pauses
        .expandingAbbreviations()                 // "CHOIR" → "Choir tokens"
}
```

## Success Metrics

### Audio Experience
- **Audio adoption rate**: % of users who try TTS feature
- **Hands-free session duration**: Average time spent in audio-only mode
- **Voice command success**: % of voice commands correctly interpreted
- **Audio completion rate**: % of users who listen to full responses

### User Engagement
- **Time to first value**: How quickly users get useful content (visual + audio)
- **Multi-modal usage**: Users who switch between visual and audio modes
- **Content consumption depth**: Audio vs visual exploration patterns
- **Preference adoption**: Customization of audio settings

### Technical Performance
- **Audio latency**: Time from text generation to speech start
- **Speech quality**: Naturalness and clarity ratings
- **Battery impact**: Power consumption during audio sessions
- **Background processing**: Reliability of hands-free operation

### Accessibility Impact
- **Screen reader compatibility**: Seamless integration with existing tools
- **Visual impairment adoption**: Usage by users with visual disabilities
- **Cognitive load reduction**: Effectiveness for users with reading difficulties
- **Multitasking enablement**: Usage during other activities

This redesign transforms the postchain from a technical prototype into a user-focused interface that respects information hierarchy while maintaining full transparency for users who want it.

=== File: docs/process_doctrine.md ===



==
process_doctrine
==


# Process Doctrine: Vibecoding as Authentic Development

VERSION process_doctrine: 1.0 (Emotional Authenticity in Technical Work)

## Core Principle: 99% Vibecoding, 1% Human Touch

The development of Choir follows a radical process doctrine: **vibecoding 99%** of the implementation while preserving the **1% human touch** that makes it authentic and universally accessible. This isn't about replacing human creativity—it's about amplifying human emotion and intention through AI collaboration.

## What is Vibecoding?

**Vibecoding** is the practice of coding with emotion, transmuting human feelings and intentions into technical implementation through AI partnership. It recognizes that the most powerful software emerges not from cold logic, but from passionate human vision expressed through computational means.

### The Emotional Foundation

Vibecoding means coding with:
- **Love** for the users who will find meaningful connections
- **Devotion** to the vision of authentic social discourse
- **Care** for the details that make experiences delightful
- **Intensity** in pursuing technical excellence
- **Transmuted Rage** at the current state of extractive social media
- **Truth-seeking** in mechanism design and incentive alignment
- **Beauty** in elegant system architecture
- **Justice** in creating platforms that serve users rather than exploit them

## The Documentation-First Paradigm

In vibecoding, **documentation quality matters more than code quality** because:

### Docs as Prompts
Documentation serves as high-quality prompts for language models that handle implementation. Well-written docs with clear emotional intent and technical vision enable AI to generate code that captures both the functional requirements and the human essence behind them.

### Docs as Normative Guidelines
Documentation establishes the normative framework—not just what the system should do, but why it matters and how it should feel to use. This creates coherent thought progression pathways for both human developers and AI collaborators.

### Docs as Objective Satisfaction States
Clear documentation defines success criteria that go beyond functional requirements to include emotional and experiential goals. This enables both humans and AI to evaluate whether implementations truly serve the intended purpose.

## Normalizing Human Weirdness

The 1% human touch is essential because it:

### Preserves Authenticity
Human weirdness—the quirks, obsessions, and unique perspectives that drive innovation—must be preserved and amplified, not smoothed away by AI optimization.

### Enables Universal Accessibility
By explicitly acknowledging and documenting human emotional drivers, we make the system accessible to others who share similar feelings about technology's role in human flourishing.

### Creates Emotional Resonance
Software built with genuine emotion resonates with users in ways that purely functional implementations cannot. The human touch ensures that technical excellence serves human needs.

## The Vibecoding Process

### 1. Emotional Clarity
Begin with clear understanding of the emotional drivers:
- What injustice are we addressing?
- What beauty are we trying to create?
- What truth are we seeking to reveal?

### 2. Vision Documentation
Translate emotions into clear technical vision:
- Why does this feature matter?
- How should it feel to use?
- What human need does it serve?

### 3. AI Collaboration
Use documentation as prompts for AI implementation:
- Provide context about emotional intent
- Specify both functional and experiential requirements
- Iterate based on how well implementations capture the vision

### 4. Human Curation
Apply the 1% human touch:
- Ensure implementations preserve authentic weirdness
- Verify emotional resonance with intended users
- Refine details that matter for human experience

## Why This Matters for Choir

Choir's mission—creating AI for social discourse that facilitates human connection—requires this approach because:

### Authentic Expression Demands Authentic Development
A platform designed to reward authentic human expression must itself be built authentically, with genuine emotion and care embedded in every technical decision.

### Mechanism Design Requires Human Understanding
Creating truthful mechanisms for social interaction requires deep understanding of human psychology, emotion, and social dynamics—understanding that emerges from passionate engagement with the problem.

### Technical Excellence Serves Human Flourishing
The most sophisticated technical implementations are worthless if they don't serve genuine human needs. Vibecoding ensures that technical excellence always serves the deeper purpose of human connection and authentic discourse.

## The Paradox of Emotional Programming

Vibecoding embraces a fundamental paradox: the most rational approach to building social technology is to code with emotion. By acknowledging and channeling human feelings—including rage at current platforms, love for potential users, and devotion to authentic connection—we create software that serves human flourishing rather than exploiting human psychology.

This isn't anti-rational; it's meta-rational. It recognizes that the highest form of technical rationality is building systems that honor the full spectrum of human experience, including the emotional drivers that make life meaningful.

## Implementation Guidelines

### For Documentation
- Write with passion and clarity about why features matter
- Include emotional context alongside technical specifications
- Describe the human experience, not just the system behavior
- Use vivid language that captures the vision's essence

### For Code Review
- Evaluate implementations against emotional intent, not just functional requirements
- Preserve human weirdness and authentic voice in user-facing elements
- Ensure technical elegance serves human needs
- Maintain the balance between AI efficiency and human authenticity

### For Product Decisions
- Prioritize features that serve authentic human connection
- Reject optimizations that compromise emotional authenticity
- Build for users who share our values about technology's role in human flourishing
- Remember that we're building a platform for meaningful relationships, not engagement metrics

## Conclusion: Technology as Emotional Expression

Vibecoding recognizes that the most powerful technology emerges when human emotion and AI capability work in harmony. By coding with love, devotion, care, and intensity—while transmuting rage at current platforms into determination to build better ones—we create software that doesn't just function but resonates.

The 1% human touch ensures that no amount of AI optimization can strip away the authentic weirdness and passionate vision that makes Choir unique. This is how we build technology that serves human flourishing: by never forgetting that behind every technical decision is a human heart seeking truth, beauty, and justice in digital form.

In vibecoding, emotion isn't the enemy of technical excellence—it's the source of it.

=== File: docs/publish_thread_feature.md ===



==
publish_thread_feature
==


# Publish Thread Feature: From Private to Public Discourse

VERSION publish_thread: 1.0 (Public Thread Discovery)

## Overview

The Publish Thread feature enables users to transform private conversations into public discourse by spending CHOIR tokens to make threads discoverable and accessible to the broader Choir community. This creates a natural bridge between intimate AI-enhanced conversations and community knowledge sharing.

## Core Mechanics

### Publishing Process
- **Cost**: Requires CHOIR token payment (amount TBD based on thread length/complexity)
- **Action**: Converts private thread to publicly accessible via unique URL
- **Persistence**: Published threads remain accessible indefinitely
- **Ownership**: Original thread creator retains ownership and moderation rights

### Access Model
- **Public Discovery**: Published threads appear in community feed/search
- **URL Sharing**: Threads can be shared via direct links across devices/platforms
- **Cross-Device Access**: Any device with Choir app can open published thread URLs
- **Continuation**: Anyone can add to published threads (subject to normal CHOIR economics)

## Technical Implementation

### Minimal Changes Required
The feature leverages existing infrastructure with simple authentication modifications:

**Current State**: All threads require wallet authentication to access
**New State**: Published threads bypass authentication requirement for read access

### API Modifications
```
GET /api/threads/{thread_id}
- Current: Requires Sui signature authentication
- Published: Public read access, authentication only required for contributions
```

### Database Changes
```sql
ALTER TABLE threads ADD COLUMN is_published BOOLEAN DEFAULT FALSE;
ALTER TABLE threads ADD COLUMN published_at TIMESTAMP NULL;
ALTER TABLE threads ADD COLUMN publish_cost DECIMAL(18,8) NULL;
```

### URL Structure
```
https://choir.chat/thread/{thread_id}
- Automatically opens in Choir app if installed
- Web fallback for sharing/preview
```

## Economic Design

### Publishing Costs
- **Base Cost**: Minimum CHOIR tokens required (e.g., 10 CHOIR)
- **Length Multiplier**: Additional cost based on thread complexity/length
- **Quality Bonus**: Reduced cost for threads with high citation/novelty scores
- **Burn Mechanism**: Published thread costs are burned, reducing CHOIR supply

### Incentive Alignment
- **Quality Filter**: Token cost ensures only valuable content gets published
- **Creator Investment**: Publishers have skin in the game for thread quality
- **Community Benefit**: High-quality public threads benefit entire ecosystem
- **Deflationary Pressure**: Burning tokens for publishing creates scarcity

## User Experience Flow

### Publishing Flow
1. User completes private conversation with AI
2. "Publish Thread" option appears with cost estimate
3. User confirms payment and thread becomes public
4. Shareable URL generated for cross-platform distribution

### Discovery Flow
1. Published threads appear in community feed
2. Users can search/filter published content
3. Clicking opens thread in full conversation view
4. Users can continue conversation by staking CHOIR tokens

### Sharing Flow
1. Published thread URLs work across devices
2. Links open directly in Choir app if installed
3. Web preview available for non-users
4. Seamless transition from viewing to participating

## Strategic Benefits

### For Individual Users
- **Monetize Insights**: Turn valuable private conversations into public contributions
- **Build Reputation**: Published threads showcase thinking quality
- **Extend Conversations**: Enable broader community engagement with ideas
- **Cross-Platform Sharing**: Share insights beyond Choir ecosystem

### For Community
- **Knowledge Base**: Accumulate high-quality conversations as searchable resource
- **Discovery Mechanism**: Find valuable content and compatible thinkers
- **Quality Curation**: Economic barriers ensure published content meets quality threshold
- **Network Effects**: More published content attracts more users

### For Platform
- **Content Flywheel**: Private conversations become public value
- **User Acquisition**: Shareable URLs bring new users to platform
- **Token Utility**: New use case for CHOIR tokens beyond rewards
- **Viral Mechanics**: Quality content spreads organically through sharing

## Relationship to Core Features

### Enhances Relationship Staking
- Published threads become venues for relationship formation
- Users can stake tokens to respond to published insights
- Quality published content attracts high-value relationship opportunities

### Amplifies Citation Rewards
- Published threads increase citation opportunities
- Authors earn rewards when published content is referenced
- Creates incentive cycle: publish → get cited → earn tokens → publish more

### Supports Anonymous Discourse
- Published threads maintain anonymity while enabling attribution
- Wallet-based identity emerges through published content quality
- Reputation builds through contribution value, not social metrics

## Implementation Phases

### Phase 1: Basic Publishing
- Simple publish button with fixed CHOIR cost
- Public thread access without authentication
- Basic URL sharing functionality

### Phase 2: Enhanced Discovery
- Community feed of published threads
- Search and filtering capabilities
- Quality-based ranking algorithms

### Phase 3: Advanced Features
- Dynamic pricing based on thread quality
- Thread collections and curation tools
- Integration with relationship staking for published content

## Success Metrics

### Engagement Metrics
- **Publish Rate**: Percentage of private threads that get published
- **View Rate**: Average views per published thread
- **Continuation Rate**: Percentage of published threads that receive responses

### Quality Metrics
- **Citation Rate**: How often published threads get referenced
- **Token Efficiency**: Relationship between publish cost and thread value
- **User Retention**: Impact of published content on user engagement

### Network Effects
- **Viral Coefficient**: How often published threads get shared externally
- **User Acquisition**: New users arriving via published thread URLs
- **Cross-Platform Reach**: Published content impact beyond Choir app

## Conclusion: Bridging Private and Public Discourse

The Publish Thread feature creates a natural evolution from private AI-enhanced conversations to public community discourse. By requiring CHOIR token investment, it ensures quality while creating new utility for the token economy.

This feature transforms Choir from a private AI chat app into a platform for public intellectual discourse, where the best private conversations become community resources. The economic design aligns individual incentives (sharing valuable insights) with community benefits (access to quality content) and platform growth (viral sharing and user acquisition).

Most importantly, it maintains Choir's core values: anonymous merit-based discourse, economic alignment through token mechanics, and AI that amplifies rather than replaces human connection. Published threads become venues for discovering like minds and forming meaningful relationships based on intellectual compatibility rather than social metrics.

=== File: docs/refactoring_planning_strategy.md ===



==
refactoring_planning_strategy
==


# Refactoring Planning Strategy: Interface-First Vibecoding

VERSION refactoring_planning: 2.0 (Interface-First Approach)

## Overview: Interface-First Development Strategy

**Core Insight**: Start with the interface redesign while keeping the service constant. This resets the interaction paradigm immediately, looks pretty, and informs what backend features we actually need.

**Vibecoding Timeline**: Hours, not weeks. Immediate visual impact, progressive enhancement.

## Day 1: Interface-First Foundation

### Hour 0: Bedrock Foundation (30 minutes)
- [ ] Add AWS environment variables to config
- [ ] Test Bedrock provider integration
- [ ] Verify basic functionality works

### Environment Variables Needed
```bash
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
AWS_REGION=us-east-1
```

### Hours 1-8: PostChain UI Redesign (Interface-First)

### Why Interface First?
- **Immediate Impact**: Users see improvement right away
- **UX Validation**: Test new interaction paradigm before complex backend
- **Momentum**: Pretty UI keeps energy high during development
- **Informed Development**: Interface tells us what backend features matter

### Hours 1-2: Smart Content Categorization

**Goal**: Implement intelligent content hierarchy using existing postchain data

**Current State**: All phase content treated equally in carousel
**New State**: Primary content prominent, secondary contextual, tertiary hidden

```swift
// New file: Choir/Views/PostChain/ContentCategorization.swift
enum ContentCategory {
    case primary    // Fast response, final response - always visible
    case secondary  // Rewards, sources - contextual display
    case tertiary   // Analysis phases - hidden by default
}

struct CategorizedContent {
    let primary: [PostchainPhase]
    let secondary: [PostchainPhase]
    let tertiary: [PostchainPhase]

    init(from message: Message) {
        // Categorize existing phases into hierarchy
        primary = [message.actionPhase, message.yieldPhase].compactMap { $0 }
        secondary = [message.noveltyReward, message.vectorResults].compactMap { $0 }
        tertiary = [message.intentionPhase, message.observationPhase, message.understandingPhase].compactMap { $0 }
    }
}
```

**Implementation Tasks**:
- [ ] Create content categorization logic
- [ ] Design smart card layout components
- [ ] Implement conditional rendering based on content availability
- [ ] Add celebration UI for rewards

### Hours 3-4: Scroll-Triggered Pagination

**Goal**: Replace carousel with frameless paginated scrolling

**Current State**: Horizontal carousel with visible frame constraints
**New State**: Vertical scroll where whole pages slide into focus smoothly

```swift
// New file: Choir/Views/PostChain/ScrollPaginatedView.swift
struct ScrollPaginatedPostchainView: View {
    let message: Message
    @State private var currentPageIndex: Int = 0
    @State private var expandedSections: Set<ContentSection> = []
    @State private var dragOffset: CGFloat = 0

    var availablePages: [PostchainPage] {
        generatePages(for: message, expandedSections: expandedSections)
    }

    var body: some View {
        GeometryReader { geometry in
            ZStack {
                ForEach(Array(availablePages.enumerated()), id: \.offset) { index, page in
                    PostchainPageView(page: page, message: message)
                        .frame(width: geometry.size.width, height: geometry.size.height)
                        .offset(y: calculatePageOffset(for: index, containerHeight: geometry.size.height))
                        .opacity(calculatePageOpacity(for: index))
                        .allowsHitTesting(index == currentPageIndex)
                }
            }
            .gesture(
                DragGesture()
                    .onChanged { value in dragOffset = value.translation.y }
                    .onEnded { value in handlePageTransition(dragValue: value, containerHeight: geometry.size.height) }
            )
        }
    }
}
```

**Implementation Tasks**:
- [ ] Create page generation logic from existing message data
- [ ] Implement smooth page transitions with spring animations
- [ ] Add page indicators and navigation hints
- [ ] Ensure individual pages can scroll internally

### Hours 5-6: Collapsible Sections & Reward Celebrations

**Goal**: Add expand/collapse functionality and prominent reward display

**Current State**: All content always visible, rewards buried in data
**New State**: Tertiary content collapsible, rewards celebrated prominently

```swift
// New file: Choir/Views/PostChain/CollapsibleSection.swift
struct CollapsibleSection: View {
    let title: String
    let content: String
    let isExpanded: Binding<Bool>
    let category: ContentCategory

    var body: some View {
        VStack(alignment: .leading, spacing: 12) {
            Button(action: { isExpanded.wrappedValue.toggle() }) {
                HStack {
                    Text(title)
                        .font(.headline)
                    Spacer()
                    Image(systemName: isExpanded.wrappedValue ? "chevron.up" : "chevron.down")
                        .foregroundColor(.secondary)
                }
            }

            if isExpanded.wrappedValue {
                Text(content)
                    .transition(.opacity.combined(with: .move(edge: .top)))
            }
        }
        .animation(.easeInOut(duration: 0.3), value: isExpanded.wrappedValue)
    }
}

// Reward celebration component
struct RewardCelebration: View {
    let reward: RewardInfo

    var body: some View {
        HStack {
            Text("🎉")
                .font(.title)
            VStack(alignment: .leading) {
                Text("Novelty Reward!")
                    .font(.headline)
                    .foregroundColor(.green)
                Text("+\(reward.formattedAmount) CHOIR")
                    .font(.title2)
                    .fontWeight(.bold)
            }
            Spacer()
        }
        .padding()
        .background(Color.green.opacity(0.1))
        .cornerRadius(12)
        .overlay(
            RoundedRectangle(cornerRadius: 12)
                .stroke(Color.green.opacity(0.3), lineWidth: 1)
        )
    }
}
```

**Implementation Tasks**:
- [ ] Create collapsible section components
- [ ] Design reward celebration animations
- [ ] Implement smooth expand/collapse transitions
- [ ] Add user preferences for default expanded state

### Hours 7-8: Audio-First Experience (TTS Integration)

**Goal**: Enable hands-free, screen-free interaction with postchain content

**Current State**: Visual-only interface requiring screen attention
**New State**: Audio-first experience with TTS and voice navigation

```swift
// New file: Choir/Services/PostchainAudioManager.swift
import AVFoundation

class PostchainAudioManager: ObservableObject {
    private let synthesizer = AVSpeechSynthesizer()
    @Published var isPlaying = false
    @Published var currentSection: ContentSection?

    func playAudioScript(for message: Message) {
        let script = generateAudioScript(for: message)

        for section in script.sections {
            let utterance = AVSpeechUtterance(string: section.text)
            utterance.voice = selectVoice(for: section.tone)
            utterance.rate = section.readingSpeed
            synthesizer.speak(utterance)
        }
    }

    private func generateAudioScript(for message: Message) -> AudioScript {
        var script = AudioScript()

        // Primary content - always included
        if let actionContent = message.actionPhase {
            script.addSection(.fastResponse, priority: .high,
                             text: cleanForAudio(actionContent))
        }

        if let yieldContent = message.yieldPhase {
            script.addSection(.finalResponse, priority: .high,
                             text: cleanForAudio(yieldContent))
        }

        // Rewards - celebratory tone
        if let reward = message.noveltyReward {
            script.addSection(.rewards, priority: .medium,
                             text: "Congratulations! You earned \(reward.formattedAmount) CHOIR tokens!",
                             tone: .celebratory)
        }

        return script
    }

    private func cleanForAudio(_ text: String) -> String {
        return text
            .replacingOccurrences(of: "**", with: "") // Remove markdown
            .replacingOccurrences(of: "*", with: "")
            .replacingOccurrences(of: "#", with: "")
            .replacingOccurrences(of: "[", with: "")
            .replacingOccurrences(of: "]", with: "")
    }
}
```

**Implementation Tasks**:
- [ ] Create audio script generation from existing message data
- [ ] Implement TTS with natural voice selection
- [ ] Add audio controls (play, pause, skip, speed)
- [ ] Design audio-first navigation patterns

## Day 2: Backend Enhancement

**Now that the interface is beautiful and functional, enhance the backend progressively**

### Hours 1-2: MCP Integration (markdownify-mcp)
- [ ] Implement basic MCP client for tool discovery
- [ ] Integrate markdownify-mcp server for file processing
- [ ] Add file upload UI that flows through new interface
- [ ] Test end-to-end file processing workflow

### Hours 3-4: Query Complexity Routing
- [ ] Create simple complexity classifier (file upload = complex)
- [ ] Implement basic execution plans (simple vs complex)
- [ ] Route file uploads through complex path automatically
- [ ] Add cost optimization for simple queries

### Hours 5-8: Advanced Features
- [ ] Context management and overflow handling
- [ ] Retry mechanisms with model switching
- [ ] Advanced MCP tool integration
- [ ] Performance optimization and monitoring

## Day 3+: Production Features

### Day 3: Relationship Staking & Wallet Languification
- [ ] Implement relationship staking UI and smart contracts
- [ ] Add natural language wallet interface
- [ ] Create voice-controlled wallet operations
- [ ] Test economic mechanics and user flows

### Day 4: Publish Thread Feature
- [ ] Build thread publishing mechanism with CHOIR token costs
- [ ] Create public thread discovery and sharing
- [ ] Implement cross-platform URL sharing
- [ ] Add community feed and search functionality

### Day 5+: Advanced Platform Features
- [ ] Multi-modal content processing (audio, video)
- [ ] Advanced AI orchestration and tool chains
- [ ] Performance optimization and scaling
- [ ] Analytics and user behavior insights

## Interface-First Benefits

### Immediate User Impact
- **Visual Improvement**: Modern, clean interface immediately
- **Better UX**: Content hierarchy makes information digestible
- **Audio Experience**: Hands-free operation for accessibility
- **Reward Celebration**: Users feel good about earning tokens

### Development Benefits
- **Momentum**: Pretty UI keeps energy high during backend work
- **User Feedback**: Can test UX patterns before complex backend
- **Informed Priorities**: Interface tells us what backend features matter
- **Parallel Development**: UI and backend teams can work simultaneously

### Technical Benefits
- **Backward Compatibility**: Existing backend continues working
- **Progressive Enhancement**: Add features without breaking existing
- **Risk Mitigation**: Interface changes are lower risk than service changes
- **Faster Iteration**: UI changes deploy faster than backend changes

## Risk Mitigation

### Technical Risks
1. **MCP Server Reliability**: Implement circuit breakers and fallbacks
2. **Context Window Management**: Gradual rollout with monitoring
3. **Performance Regression**: Benchmark each phase against current system
4. **Complexity Creep**: Strict scope boundaries for each phase

### User Experience Risks
1. **Feature Regression**: Comprehensive testing of existing workflows
2. **Response Time Increase**: Performance budgets for each phase
3. **Reliability Concerns**: Gradual rollout with feature flags

## Success Criteria

### Phase 1 Success
- [ ] Users can upload and process files via MCP
- [ ] File content integrates seamlessly with existing postchain
- [ ] MCP tool calls visible in UI
- [ ] No regression in existing functionality

### Phase 2 Success
- [ ] Simple queries complete 50% faster
- [ ] Complex queries get appropriate tool access
- [ ] Cost per query reduced by 30% overall
- [ ] User satisfaction maintained or improved

### Phase 3 Success
- [ ] System handles 10x current load
- [ ] Individual phase failures don't crash entire workflow
- [ ] New phases can be added without core changes
- [ ] Development velocity increased

### Phase 4 Success
- [ ] System handles files up to 100MB
- [ ] Context overflow handled gracefully
- [ ] 99.9% uptime maintained
- [ ] Ready for advanced features (relationship staking, etc.)


This approach ensures we build incrementally while maintaining system stability and delivering continuous user value.

=== File: docs/relationship_staking.md ===



==
relationship_staking
==


# Relationship Staking: Investing Value in Human Connections

## Overview

Relationship staking is Choir's revolutionary approach to creating meaningful connections through economic alignment. Unlike traditional social media where engagement is free (and often worthless), relationship staking requires users to invest their earned CHOIR tokens to initiate conversations, creating real skin in the game for quality discourse.

## How It Works

### Initial Connection
When you encounter a thoughtful prompt or response from another user, you can choose to respond directly by staking CHOIR tokens as a non-refundable bond. This stake demonstrates serious intent and filters out low-effort interactions.

### Response Options
The recipient has two choices:
1. **Ignore**: Keep the staked tokens and move on
2. **Engage**: Respond back, which locks both users' tokens into a shared relationship multisig

### Relationship Multisig
Once both parties engage, their staked tokens are locked in a smart contract owned jointly by both users. This creates:
- **Shared economic interest** in maintaining the relationship
- **Mutual investment** in quality discourse
- **Transferable relationship value** that belongs to the participants

## Multiparty Relationships

### Group Formation
Relationships can expand beyond pairs. When multiple users stake tokens in response to each other, they form multiparty relationships with shared token pools.

### Dynamic Membership
- **Unilateral Exit**: Any member can leave at any time, taking their proportional share of tokens
- **Organic Growth**: New members can join by staking tokens and being accepted by existing members
- **Economic Alignment**: All members have skin in the game for group success

## Key Benefits

### For Users
- **Own Your Social Value**: Unlike likes and followers, relationship stakes are transferable assets
- **Quality Filter**: Economic barriers eliminate spam and low-effort interactions
- **Meaningful Connections**: Shared investment creates deeper, more thoughtful relationships
- **Exit Rights**: Always maintain control over your invested value

### For the Platform
- **Natural Moderation**: Economic incentives align with quality discourse
- **Relationship Discovery**: AI can identify compatible users through staking patterns
- **Value Creation**: Turns social interaction from cost center to value generator
- **Network Effects**: Successful relationships attract more high-quality participants

## Economic Mechanics

### Stake Sizing
- Minimum stakes prevent spam while remaining accessible
- Dynamic pricing based on user reputation and relationship history
- Escalating stakes for deeper relationship levels

### Value Accumulation
- Successful relationships can accumulate additional value through:
  - Citation rewards when relationship content is referenced
  - Novelty rewards for collaborative insights
  - Network effects as relationships become hubs of quality discourse

### Exit Mechanisms
- **Graceful Exit**: Take proportional share and leave relationship intact
- **Relationship Dissolution**: Mutual agreement to dissolve and split all tokens
- **Dispute Resolution**: Community governance mechanisms for edge cases

## Implementation Roadmap

### Phase 1: Pair Relationships
- Basic staking for 1:1 connections
- Simple multisig contracts
- Core UI for relationship management

### Phase 2: Group Dynamics
- Multiparty relationship support
- Dynamic membership changes
- Advanced governance features

### Phase 3: Relationship Networks
- Cross-relationship value flows
- Reputation systems based on relationship success
- AI-powered relationship recommendations

## Why This Matters

Relationship staking solves the fundamental problem of online discourse: the lack of consequences for bad behavior and rewards for good behavior. By requiring economic investment in relationships, we create:

1. **Accountability**: Users have financial incentive to maintain quality relationships
2. **Ownership**: Social value belongs to users, not platforms
3. **Sustainability**: Economic alignment creates self-reinforcing quality cycles
4. **Meaning**: Relationships have tangible value beyond ephemeral engagement metrics

This transforms social media from an extractive attention economy into a generative relationship economy where human connections create lasting value for all participants.

=== File: docs/require_action_phase.md ===



==
require_action_phase
==


# Action Phase Requirements

## Overview

The Action phase is the initial entry point and recursive re-entry point for the PostChain. It is responsible for direct model calls and tool execution based on user input or previous cycle results.

## Core Responsibilities

1. Process immediate user input or recursive prompts
2. Execute simple model inference or tool operations
3. Format results for downstream consumption
4. Maintain minimal context focused on the current request

## Temporal Focus: The Immediate Present

The Action phase operates in the immediate present, with minimal historical context. It focuses on the current moment of engagement, either with user input or the current state of a recursive process.

## Input Specification

The Action phase accepts:

1. **Primary Content**:

   - Initial user input (first cycle)
   - Yield phase forwarded content (recursive cycles)

2. **Metadata**:
   - Recursion state (cycle count, origin)
   - Context management operations from prior cycles
   - Configuration parameters for model selection

## Output Specification

The Action phase produces:

1. **Primary Content**:

   - Direct model responses or tool execution results
   - Initial assessment of user input

2. **Metadata**:
   - Confidence scores
   - Context operations (minimal at this stage)
   - Processing telemetry

## Processing Requirements

### Model Selection

The Action phase should dynamically select appropriate models based on:

- Task complexity
- Required capabilities (e.g., tool use, code generation)
- Performance characteristics from the provider matrix

### Context Management

As the initial phase, Action should:

- Apply minimal context operations
- Format user input appropriately
- Include system prompts relevant to the current request
- Preserve user messages intact

### Error Handling

The Action phase should handle:

- Model unavailability by falling back to alternative providers
- Tool execution failures with appropriate error messages
- Context size limitations with truncation strategies

## Performance Requirements

1. **Latency**: The Action phase should complete within 2-3 seconds for simple requests
2. **Throughput**: Support concurrent processing of multiple threads
3. **Reliability**: Achieve 99.9% success rate for request handling

## Implementation Constraints

1. Use the provider matrix for model selection
2. Support both synchronous and streaming responses
3. Implement clean error boundaries
4. Log all operations for monitoring and debugging

## Examples

### Simple Model Call (action_0)

```python
async def action_0(input_text: str, context: List[Message] = None) -> ActionResult:
    """Execute a simple model inference without tools."""
    model = select_model_provider("action", {"tool_use": False})
    system_prompt = "You are a helpful assistant responding to user queries."

    return await action_agent.run(
        input_text,
        message_history=context,
        system_prompt=system_prompt
    )
```

### Tool-using Action (action_n)

```python
async def action_n(input_text: str, context: List[Message] = None, tools: List[Tool] = None) -> ActionResult:
    """Execute a model call with tool use capabilities."""
    model = select_model_provider("action", {"tool_use": True})
    system_prompt = "You are a helpful assistant with access to tools. Use them when appropriate."

    return await action_agent.run(
        input_text,
        message_history=context,
        system_prompt=system_prompt,
        tools=tools
    )
```

## Interaction with Other Phases

- **Receives from**: Yield phase (in recursive cycles) or system (initial input)
- **Sends to**: Experience phase (sequential flow)
- **Relationship**: Initiates each PostChain cycle

## Success Criteria

1. Correctly interprets user input or recursive prompts
2. Successfully executes model calls or tool operations
3. Provides responses within latency requirements
4. Correctly formats output for downstream consumption
5. Handles errors gracefully with appropriate fallbacks

=== File: docs/require_experience_phase.md ===



==
require_experience_phase
==


# Experience Phase Requirements

## Overview

The Experience phase enriches the conversation context with relevant historical knowledge, search results, and retrieved information. It serves as the system's memory and knowledge acquisition component.

## Core Responsibilities

1. Retrieve relevant information from external sources
2. Enrich context with historical knowledge
3. Add search results and database lookups
4. Tag sources and relevance of added information
5. Maintain connections to knowledge repositories

## Temporal Focus: The Past Knowledge

The Experience phase embodies the system's relationship with past knowledge. It draws upon previously accumulated information, historical context, and external knowledge sources to enrich the current conversation.

## Input Specification

The Experience phase accepts:

1. **Primary Content**:

   - User input with initial Action phase assessment
   - Queries derived from user input

2. **Metadata**:
   - Context from previous phases
   - Search/retrieval parameters
   - Knowledge source configurations

## Output Specification

The Experience phase produces:

1. **Primary Content**:

   - Original content enhanced with retrieved information
   - Search results and knowledge retrievals

2. **Metadata**:
   - Source attribution for added information
   - Relevance scores for retrievals
   - Confidence in information accuracy
   - Context operations for information management

## Processing Requirements

### Knowledge Retrieval

The Experience phase should:

- Execute targeted searches based on user queries
- Perform vector similarity lookups in knowledge bases
- Retrieve relevant documents or snippets
- Filter results based on relevance thresholds

### Context Management

For effective information enrichment:

- Tag all added information with source attribution
- Add relevance scores to retrieved content
- Use ADD context operations for new information
- Use TAG operations to mark information characteristics
- Preserve original queries alongside results

### Error Handling

The Experience phase should handle:

- Failed retrievals with appropriate fallbacks
- Source unavailability with graceful degradation
- Rate limiting with retries and backoff strategies
- Empty result sets with alternative search strategies

## Performance Requirements

1. **Latency**: Complete retrieval operations within 3-5 seconds
2. **Result Quality**: Maintain relevance scores above 0.7 for retrievals
3. **Volume Control**: Limit added context to avoid token limit issues
4. **Source Diversity**: Attempt to retrieve from multiple sources when appropriate

## Implementation Constraints

1. Support multiple retrieval methods:
   - Vector database searches
   - Web search API calls
   - Document retrieval systems
   - Structured database queries
2. Implement caching for frequent retrievals
3. Support asynchronous retrieval operations
4. Maintain provenance tracking for all added information

## Examples

### Web Search Retrieval

```python
async def web_search_retrieval(query: str, context: List[Message]) -> ExperienceResult:
    """Retrieve information from web search."""
    search_results = await web_search_tool.search(query, max_results=3)

    # Add context operations for search results
    context_ops = []
    for result in search_results:
        context_ops.append({
            "operation": "ADD",
            "target": "context",
            "data": {
                "content": result.snippet,
                "source": result.url
            },
            "metadata": {
                "relevance": result.relevance_score,
                "timestamp": result.published_date
            }
        })

    return ExperienceResult(
        content={
            "original_query": query,
            "search_results": search_results
        },
        metadata={
            "context_operations": context_ops,
            "retrieval_method": "web_search"
        }
    )
```

### Vector Database Retrieval

```python
async def vector_db_retrieval(query: str, context: List[Message]) -> ExperienceResult:
    """Retrieve information from vector database."""
    # Convert query to embedding
    embedding = await embeddings_service.embed(query)

    # Retrieve similar documents
    documents = await vector_db.similarity_search(
        embedding,
        top_k=5,
        min_relevance=0.75
    )

    # Add context operations for retrieved documents
    context_ops = []
    for doc in documents:
        context_ops.append({
            "operation": "ADD",
            "target": "context",
            "data": {
                "content": doc.content,
                "source": doc.metadata.source
            },
            "metadata": {
                "relevance": doc.relevance_score,
                "created_at": doc.metadata.created_at
            }
        })

    return ExperienceResult(
        content={
            "original_query": query,
            "retrieved_documents": documents
        },
        metadata={
            "context_operations": context_ops,
            "retrieval_method": "vector_db"
        }
    )
```

## Interaction with Other Phases

- **Receives from**: Action phase
- **Sends to**: Intention phase
- **Relationship**: Provides knowledge enrichment before intention refinement

## Success Criteria

1. Retrieves information relevant to user queries
2. Properly attributes sources of all added information
3. Maintains appropriate balance of detail vs. conciseness
4. Preserves context operations for downstream phases
5. Falls back gracefully when primary sources are unavailable

=== File: docs/require_intention_phase.md ===



==
require_intention_phase
==


# Intention Phase Requirements

## Overview

The Intention phase refines and focuses information toward user goals, aligning the accumulated context with desired outcomes. It serves as the bridge between retrieved knowledge and effective decision-making by identifying what matters most.

## Core Responsibilities

1. Identify and clarify user goals and intentions
2. Prioritize information based on relevance to goals
3. Filter noise and tangential information
4. Align system responses with user objectives
5. Maintain focus on the desired future state

## Temporal Focus: The Desired Future

The Intention phase orients toward future objectives and desired outcomes. It represents the system's relationship with where the process needs to go, focusing information toward goal achievement rather than just accumulation.

## Input Specification

The Intention phase accepts:

1. **Primary Content**:

   - Original content with retrieved information (from Experience)
   - Search results and knowledge retrievals

2. **Metadata**:
   - Source attributions
   - Relevance scores for retrievals
   - Context from previous phases

## Output Specification

The Intention phase produces:

1. **Primary Content**:

   - Goal-oriented content with prioritized information
   - Clarified user intent statements

2. **Metadata**:
   - Alignment scores with identified intents
   - Priority markers for information
   - Context operations for focusing information
   - Goal certainty metrics

## Processing Requirements

### Intent Identification

The Intention phase should:

- Extract explicit and implicit user goals
- Disambiguate between multiple possible intentions
- Rank intentions by priority and likelihood
- Track intent evolution across conversation history

### Information Prioritization

For effective goal alignment:

- Score information relevance to identified goals
- Apply PRIORITIZE context operations to relevant content
- Use TRANSFORM operations to focus verbose content
- Identify information gaps needed for goal achievement

### Goal Refinement

To clarify ambiguous intentions:

- Generate goal hypotheses when intent is unclear
- Identify conflicting goals for resolution
- Decompose complex goals into manageable components
- Abstract specific requests to underlying intentions

### Error Handling

The Intention phase should handle:

- Ambiguous or contradictory user intentions
- Missing context for intent resolution
- Goal shifts during conversation
- Misalignment between user goals and available information

## Performance Requirements

1. **Intent Recognition Accuracy**: >85% accuracy in identifying correct user intent
2. **Processing Time**: Complete intent analysis within 1-2 seconds
3. **Relevance Threshold**: Achieve >80% precision in information prioritization
4. **Goal Stability**: Maintain consistent goal tracking across conversation turns

## Implementation Constraints

1. Maintain goal state across conversation turns
2. Support nested and hierarchical goal structures
3. Implement efficient goal-based relevance scoring
4. Track goal evolution and refinement over time

## Examples

### Intent Extraction and Prioritization

```python
async def extract_and_prioritize_intent(content: Dict, context: List[Message]) -> IntentionResult:
    """Extract user intent and prioritize information accordingly."""
    # Extract intent from user input and context
    intent_analysis = await intent_analyzer.analyze(
        content["original_query"],
        conversation_history=context
    )

    # Score relevance of information to intent
    scored_information = []
    for item in content.get("search_results", []):
        relevance_to_intent = calculate_relevance_to_intent(
            item,
            intent_analysis.primary_intent
        )

        scored_information.append({
            "item": item,
            "relevance_score": relevance_to_intent,
            "aligned_with_intent": relevance_to_intent > 0.7
        })

    # Generate context operations based on intent alignment
    context_ops = []
    for idx, info in enumerate(scored_information):
        if info["aligned_with_intent"]:
            context_ops.append({
                "operation": "PRIORITIZE",
                "target": f"search_results[{idx}]",
                "data": {
                    "priority": info["relevance_score"]
                },
                "metadata": {
                    "reason": "aligned_with_intent",
                    "intent": intent_analysis.primary_intent
                }
            })
        elif info["relevance_score"] < 0.3:
            context_ops.append({
                "operation": "TAG",
                "target": f"search_results[{idx}]",
                "data": {
                    "tags": ["low_relevance"]
                },
                "metadata": {
                    "reason": "not_aligned_with_intent"
                }
            })

    return IntentionResult(
        content={
            "original_content": content,
            "extracted_intent": intent_analysis.primary_intent,
            "intent_confidence": intent_analysis.confidence,
            "alternative_intents": intent_analysis.alternative_intents,
            "scored_information": scored_information
        },
        metadata={
            "context_operations": context_ops,
            "intent_extraction_method": "semantic_analysis"
        }
    )
```

### Goal Decomposition

```python
def decompose_complex_goal(primary_intent: str) -> Dict:
    """Break down a complex goal into subgoals."""
    # Analyze intent complexity
    complexity = measure_intent_complexity(primary_intent)

    if complexity < 0.5:  # Simple intent
        return {
            "is_complex": False,
            "primary_goal": primary_intent,
            "subgoals": []
        }

    # For complex intents, break down into components
    subgoals = []

    # Extract component goals through model call
    model = select_model_provider("intention", {"reasoning": True})
    system_prompt = "Break down this complex user goal into simpler component goals."

    decomposition_result = intent_model.run_sync(
        primary_intent,
        system_prompt=system_prompt
    )

    # Parse the decomposition
    subgoals = parse_subgoals(decomposition_result.data)

    return {
        "is_complex": True,
        "primary_goal": primary_intent,
        "subgoals": subgoals,
        "dependencies": identify_subgoal_dependencies(subgoals)
    }
```

## Interaction with Other Phases

- **Receives from**: Experience phase
- **Sends to**: Observation phase
- **Relationship**: Focuses information before semantic connection marking

## Success Criteria

1. Correctly identifies user intentions even when implicit
2. Successfully prioritizes information relevant to goals
3. Improves response relevance by filtering noise
4. Maintains consistent goal tracking across conversation
5. Adapts to evolving user intentions over time

=== File: docs/require_observation_phase.md ===



==
require_observation_phase
==


# Observation Phase Requirements

## Overview

The Observation phase identifies and persists connections between concepts, creating semantic links for future reference and retrieval. It serves as the system's memory persistence layer, ensuring that valuable insights and relationships are preserved beyond the current interaction cycle.

## Core Responsibilities

1. Identify semantic connections between pieces of information
2. Tag and categorize information for future retrieval
3. Persist important insights to memory
4. Create semantic links between related concepts
5. Maintain relationship graphs and knowledge structures

## Temporal Focus: Future Preservation

The Observation phase focuses on preserving information for future use. It identifies what should endure beyond the current cycle, explicitly marking connections and insights that will be valuable in subsequent interactions.

## Input Specification

The Observation phase accepts:

1. **Primary Content**:

   - Goal-oriented content with prioritized information (from Intention)
   - Clarified user intent statements

2. **Metadata**:
   - Alignment scores with identified intents
   - Priority markers for information
   - Context operations from previous phases

## Output Specification

The Observation phase produces:

1. **Primary Content**:

   - Content with semantic connections identified
   - Knowledge graph updates and additions

2. **Metadata**:
   - Tags and relationship links
   - Memory persistence instructions
   - Context operations for relationship marking
   - Knowledge graph statistics

## Processing Requirements

### Semantic Connection Identification

The Observation phase should:

- Identify relationships between concepts
- Detect causal, hierarchical, and associative links
- Recognize patterns across information sources
- Map connections to existing knowledge structures

### Memory Persistence

For effective future retrieval:

- Score information importance for long-term storage
- Use LINK context operations to establish connections
- Apply domain-specific tagging schemas
- Prepare vector representations for similarity search

### Knowledge Graph Management

To maintain coherent knowledge structures:

- Update existing knowledge graph entries
- Create new nodes for novel concepts
- Establish weighted relationships between nodes
- Prune redundant or superseded connections

### Error Handling

The Observation phase should handle:

- Conflicting relationship patterns
- Novel concepts not in existing schemas
- Information without clear relationships
- Memory storage constraints

## Performance Requirements

1. **Connection Accuracy**: >80% precision in relationship identification
2. **Processing Efficiency**: Complete observation processing within 2-3 seconds
3. **Storage Optimization**: Minimize duplication while maximizing retrievability
4. **Relationship Quality**: Achieve high semantic relevance in established links

## Implementation Constraints

1. Support vector database integration for embeddings
2. Implement efficient graph database operations
3. Maintain backward compatibility with existing knowledge structures
4. Support incremental knowledge graph updates

## Examples

### Semantic Connection Identification

```python
async def identify_semantic_connections(content: Dict) -> List[Connection]:
    """Identify semantic connections between content elements."""
    connections = []

    # Extract entities and concepts from content
    entities = await entity_extractor.extract(content["goal_oriented_content"])

    # Find connections between entities
    for i, entity1 in enumerate(entities):
        for j, entity2 in enumerate(entities):
            if i != j:  # Don't connect entity to itself
                relationship = await relationship_detector.detect(
                    entity1,
                    entity2,
                    context=content
                )

                if relationship and relationship.confidence > 0.6:
                    connections.append({
                        "source": entity1.id,
                        "target": entity2.id,
                        "relationship_type": relationship.type,
                        "confidence": relationship.confidence,
                        "evidence": relationship.evidence
                    })

    return connections
```

### Memory Persistence Operations

```python
async def persist_to_memory(
    content: Dict,
    connections: List[Connection],
    context: List[Message]
) -> ObservationResult:
    """Persist important information and connections to memory."""
    # Prepare context operations
    context_ops = []

    # Create LINK operations for connections
    for connection in connections:
        if connection["confidence"] > 0.7:  # Only persist high-confidence connections
            context_ops.append({
                "operation": "LINK",
                "target": connection["source"],
                "data": {
                    "linked_to": connection["target"],
                    "relationship": connection["relationship_type"]
                },
                "metadata": {
                    "confidence": connection["confidence"],
                    "evidence": connection["evidence"]
                }
            })

    # Tag important entities for persistence
    for entity in extract_entities(content):
        importance = calculate_entity_importance(entity, content, connections)
        if importance > 0.65:
            context_ops.append({
                "operation": "TAG",
                "target": entity.id,
                "data": {
                    "tags": ["important", "persist"]
                },
                "metadata": {
                    "importance": importance,
                    "reason": "key_concept"
                }
            })

    # Persist to vector database for future retrieval
    embed_results = await knowledge_store.embed_and_store(
        content=content["goal_oriented_content"],
        metadata={
            "connections": connections,
            "timestamp": datetime.utcnow().isoformat(),
            "context_id": context[-1].id if context else None
        }
    )

    return ObservationResult(
        content={
            "original_content": content,
            "identified_connections": connections,
            "persisted_entities": [e.id for e in extract_entities(content) if calculate_entity_importance(e, content, connections) > 0.65]
        },
        metadata={
            "context_operations": context_ops,
            "persistence_details": embed_results,
            "knowledge_graph_updates": len(connections)
        }
    )
```

### Knowledge Graph Update

```python
async def update_knowledge_graph(connections: List[Connection]) -> Dict:
    """Update the knowledge graph with new connections."""
    updates = {
        "added_nodes": [],
        "added_edges": [],
        "modified_nodes": [],
        "modified_edges": []
    }

    # Update graph database
    async with graph_db.transaction() as txn:
        # Process each connection
        for connection in connections:
            # Check if source node exists
            source_exists = await txn.node_exists(connection["source"])
            if not source_exists:
                node_id = await txn.create_node(
                    id=connection["source"],
                    properties={
                        "created_at": datetime.utcnow().isoformat()
                    }
                )
                updates["added_nodes"].append(node_id)

            # Check if target node exists
            target_exists = await txn.node_exists(connection["target"])
            if not target_exists:
                node_id = await txn.create_node(
                    id=connection["target"],
                    properties={
                        "created_at": datetime.utcnow().isoformat()
                    }
                )
                updates["added_nodes"].append(node_id)

            # Create or update edge
            edge_exists = await txn.edge_exists(
                source=connection["source"],
                target=connection["target"],
                type=connection["relationship_type"]
            )

            if edge_exists:
                edge_id = await txn.update_edge(
                    source=connection["source"],
                    target=connection["target"],
                    type=connection["relationship_type"],
                    properties={
                        "confidence": connection["confidence"],
                        "updated_at": datetime.utcnow().isoformat()
                    }
                )
                updates["modified_edges"].append(edge_id)
            else:
                edge_id = await txn.create_edge(
                    source=connection["source"],
                    target=connection["target"],
                    type=connection["relationship_type"],
                    properties={
                        "confidence": connection["confidence"],
                        "created_at": datetime.utcnow().isoformat()
                    }
                )
                updates["added_edges"].append(edge_id)

    return updates
```

## Interaction with Other Phases

- **Receives from**: Intention phase
- **Sends to**: Understanding phase
- **Relationship**: Preserves connections before context filtering

## Success Criteria

1. Accurately identifies meaningful semantic connections
2. Successfully persists important information for future retrieval
3. Creates useful knowledge graph structures
4. Maintains efficient storage with minimal redundancy
5. Enhances future retrieval through effective tagging and linking

=== File: docs/require_phase_requirements_index.md ===



==
require_phase_requirements_index
==


# PostChain Phase Requirements

## Overview

This directory contains detailed Product Requirements Documents (PRDs) for each phase of the PostChain. These specifications define the exact responsibilities, behaviors, inputs, and outputs for each phase actor.

## Temporal Relationship to Information

The PostChain phases embody different temporal relationships to information:

| Phase             | Temporal Focus       | Core Responsibility                       |
| ----------------- | -------------------- | ----------------------------------------- |
| **Action**        | Immediate present    | Model calls and tool execution            |
| **Experience**    | Past knowledge       | Information retrieval and enrichment      |
| **Intention**     | Desired future       | Goal-seeking and focus refinement         |
| **Observation**   | Future preservation  | Memory persistence and connection marking |
| **Understanding** | Temporal integration | Context filtering and information release |
| **Yield**         | Process completion   | Flow control and recursion decisions      |

## Phase Specifications

### [Action Phase](action_phase.md)

The Action phase handles direct model calls and tool execution, operating in the immediate present with minimal historical context. It serves as both the entry point and potential recursive re-entry point for the PostChain.

**Key responsibilities**: Model inference, tool execution, initial response generation

### [Experience Phase](experience_phase.md)

The Experience phase enriches the conversation with retrieved knowledge, serving as the system's memory and knowledge acquisition component. It embodies the system's relationship with past knowledge.

**Key responsibilities**: Information retrieval, context enrichment, knowledge enhancement

### [Intention Phase](intention_phase.md)

The Intention phase refines and focuses information toward user goals, aligning the accumulated context with desired outcomes. It represents the system's orientation toward future objectives.

**Key responsibilities**: Goal identification, priority setting, relevance determination

### [Observation Phase](observation_phase.md)

The Observation phase identifies and persists connections between concepts, creating semantic links for future reference. It manages the preservation of information beyond the current cycle.

**Key responsibilities**: Connection marking, semantic tagging, memory persistence

### [Understanding Phase](../require_understanding_phase.md)

The Understanding phase evaluates accumulated information to determine what remains relevant and what can be released. It embodies the wisdom of letting go of less relevant information.

**Key responsibilities**: Context filtering, information pruning, message evaluation

### [Yield Phase](../require_yield_phase.md)

The Yield phase determines whether to produce a final response or continue processing through another recursive cycle. It controls the flow of the entire PostChain process.

**Key responsibilities**: Recursion decisions, flow control, response formatting

## Implementation Strategy

These phase requirements represent ideal behaviors for a full actor-based implementation. During initial development with PydanticAI, a simplified version may be implemented first, while maintaining alignment with these conceptual responsibilities.

The phase requirements should be used as reference during implementation to ensure that each phase, regardless of the underlying architecture, fulfills its core temporal relationship to information.

## Document Format

Each phase requirement document follows a consistent format:

1. **Overview**: Brief description of the phase and its purpose
2. **Core Responsibilities**: List of primary responsibilities
3. **Temporal Focus**: Relationship to time and information
4. **Input Specification**: Expected inputs and their structure
5. **Output Specification**: Required outputs and their structure
6. **Processing Requirements**: Specific processing behaviors
7. **Performance Requirements**: Expected performance characteristics
8. **Implementation Constraints**: Technical implementation guidelines
9. **Examples**: Code examples showing how the phase might be implemented
10. **Interaction with Other Phases**: How the phase connects to others
11. **Success Criteria**: Measurable success indicators

=== File: docs/require_understanding_phase.md ===



==
require_understanding_phase
==


# Understanding Phase Requirements

## Overview

The Understanding phase is responsible for temporal integration of information and context management. It evaluates accumulated information across time to determine what remains relevant and what can be released, embodying the system's ability to discern signal from noise.

## Core Responsibilities

1. Evaluate and filter information based on relevance
2. Implement information "forgetting" through pruning
3. Apply context management operations to maintain optimal context
4. Integrate information across temporal phases
5. Maintain clean and focused context for subsequent cycles

## Temporal Focus: Temporal Integration and Release

The Understanding phase integrates information across time, having sufficient contextual awareness to determine what information remains relevant and what can be released. This phase embodies the wisdom of letting go of less relevant information.

## Input Specification

The Understanding phase accepts:

1. **Primary Content**:

   - Content with semantic connections identified (from Observation)
   - Context with tagged relationships and importance markers

2. **Metadata**:
   - Tags and relationship links
   - Context history across multiple cycles
   - Relevance scores and usage metrics

## Output Specification

The Understanding phase produces:

1. **Primary Content**:

   - Filtered and integrated content
   - Decisions about information retention and release

2. **Metadata**:
   - Context management operations (PRUNE, TRANSFORM, etc.)
   - Rationale for retention/release decisions
   - Context statistics (tokens, messages, etc.)

## Processing Requirements

### Message Evaluation

The Understanding phase should:

- Evaluate each message's relevance to current context
- Track message references and usage across phases
- Calculate information importance based on multiple factors
- Distinguish between user messages and AI-generated content

### Context Management Rules

1. **User Messages**:

   - Preserve by default
   - Request user consent for pruning large messages
   - Offer summarization as an alternative to full retention

2. **AI-Generated Content**:

   - Automatically prune based on relevance assessment
   - Summarize content where appropriate
   - Maintain attribution chains when summarizing

3. **Search Results**:
   - Evaluate continued relevance
   - Prune results not referenced in recent phases
   - Consolidate similar or redundant information

### Context Operations

The Understanding phase should generate appropriate context operations:

- `PRUNE`: Mark messages for removal
- `TRANSFORM`: Suggest summarization or condensing
- `PRIORITIZE`: Adjust importance of information
- `TAG`: Add metadata about information retention

### Error Handling

The Understanding phase should handle:

- Context window limits with graceful degradation
- User override of pruning recommendations
- Preservation of critical content even under constraints

## Performance Requirements

1. **Efficiency**: Complete context evaluation within 1-2 seconds
2. **Context Size Management**: Maintain context within 70% of model limits
3. **Relevance Threshold**: Achieve >85% retention of truly relevant information
4. **User Experience**: Minimize disruption when requesting consent

## Implementation Constraints

1. Maintain clear separation between:
   - User-owned content (requiring consent)
   - AI-generated content (managed automatically)
2. Implement decay functions for information relevance over time
3. Support reversible operations when possible
4. Log all pruning decisions for transparency

## Examples

### Message Evaluation and Pruning

```python
async def evaluate_messages(context: List[Message]) -> List[ContextOperation]:
    """Evaluate messages and return context operations."""
    operations = []

    # Group messages by type
    user_messages = [m for m in context if m.role == "user"]
    ai_messages = [m for m in context if m.role == "assistant"]

    # AI message evaluation
    for message in ai_messages:
        # Skip most recent message
        if message == ai_messages[-1]:
            continue

        relevance = calculate_relevance(message, context)
        if relevance < 0.3:
            operations.append({
                "operation": "PRUNE",
                "target": message.id,
                "data": {"reason": "low_relevance"},
                "metadata": {"relevance": relevance}
            })
        elif relevance < 0.7:
            operations.append({
                "operation": "TRANSFORM",
                "target": message.id,
                "data": {
                    "transformation": "summarize",
                    "parameters": {"max_length": 100}
                },
                "metadata": {"relevance": relevance}
            })

    # User message evaluation (large messages only)
    for message in user_messages:
        if len(message.content) > 1000:
            # Flag for user consent, don't prune automatically
            operations.append({
                "operation": "TRANSFORM",
                "target": message.id,
                "data": {
                    "transformation": "summarize",
                    "parameters": {"max_length": 200}
                },
                "metadata": {
                    "requires_consent": True,
                    "original_length": len(message.content)
                }
            })

    return operations
```

### User Consent Management

```python
async def request_user_consent(
    operations: List[ContextOperation],
    context: List[Message]
) -> List[ContextOperation]:
    """Request user consent for operations requiring it."""
    consent_required = [op for op in operations if op.get("metadata", {}).get("requires_consent")]

    if not consent_required:
        return operations

    # Prepare user-facing message
    consent_message = "To optimize the conversation, I'd like to summarize these earlier messages:\n\n"

    for op in consent_required:
        message = next(m for m in context if m.id == op["target"])
        preview = message.content[:50] + "..." if len(message.content) > 50 else message.content
        consent_message += f"- {preview}\n"

    consent_message += "\nWould you like me to: (1) Keep everything as is, (2) Summarize these messages, or (3) Remove them entirely?"

    # In practice, this would await actual user input
    # Simulated response for example
    user_choice = await request_user_input(consent_message)

    # Apply user choice
    if user_choice == "1":  # Keep
        return [op for op in operations if not op.get("metadata", {}).get("requires_consent")]
    elif user_choice == "2":  # Summarize
        # Keep summarization operations
        return operations
    else:  # Remove
        # Convert TRANSFORM to PRUNE
        for op in consent_required:
            op["operation"] = "PRUNE"
            op["data"] = {"reason": "user_consent"}
        return operations
```

## Interaction with Other Phases

- **Receives from**: Observation phase
- **Sends to**: Yield phase
- **Relationship**: Optimizes context before flow control decisions

## Success Criteria

1. Maintains optimal context size through intelligent pruning
2. Preserves critical information regardless of age
3. Respects user ownership of their messages
4. Provides transparent context operations
5. Improves model performance by reducing noise

=== File: docs/require_yield_phase.md ===



==
require_yield_phase
==


# Yield Phase Requirements

## Overview

The Yield phase is responsible for process completion decisions and flow control. It determines whether to return a final response or continue processing through another cycle, and which phase to invoke next in the case of recursion.

## Core Responsibilities

1. Evaluate process completion criteria
2. Make recursion decisions
3. Select the next phase to execute (when recursing)
4. Format final output for user consumption
5. Maintain process continuity across cycles

## Temporal Focus: Process Completion

The Yield phase focuses on the completion state of the process. It assesses whether the current cycle has produced sufficient results or whether additional cycles would yield meaningful improvements.

## Input Specification

The Yield phase accepts:

1. **Primary Content**:

   - Filtered and integrated content from Understanding
   - Current cycle's outputs and state

2. **Metadata**:
   - Context management decisions
   - Recursion state (current cycle count)
   - Confidence scores and completion metrics
   - Processing telemetry from previous phases

## Output Specification

The Yield phase produces:

1. **Primary Content**:

   - Final response (if complete)
   - Continuation prompt (if recursing)

2. **Metadata**:
   - Recursion decision (continue/complete)
   - Target phase for next cycle (if continuing)
   - Updated recursion state
   - Rationale for recursion decision

## Processing Requirements

### Completion Evaluation

The Yield phase should evaluate completion based on:

- Convergence of results
- Answer confidence thresholds
- Maximum cycle limits
- Task completion indicators
- User satisfaction metrics

### Recursion Control

When deciding to continue, the Yield phase should:

- Select the most appropriate phase to invoke next
- Initialize proper state for the next cycle
- Formulate the continuation prompt
- Update recursion counters and state

### Next Phase Selection

The Yield phase can select any phase for recursion:

- `action`: For additional processing or tool use
- `experience`: For gathering more information
- `intention`: For refining goals
- `observation`: For storing additional insights
- `understanding`: For context refinement
- Default sequential flow is to `action` phase

### Final Response Formatting

When deciding to complete, the Yield phase should:

- Format the final response for user consumption
- Apply appropriate styling and structure
- Include confidence indicators
- Provide source attributions when relevant

### Error Handling

The Yield phase should handle:

- Recursion loop detection
- Maximum recursion limit enforcement
- Recovery from incomplete or failed phases
- Graceful termination when necessary

## Performance Requirements

1. **Decision Speed**: Complete recursion decisions within 1 second
2. **Recursion Limit**: Enforce configurable maximum recursive cycles
3. **Completion Accuracy**: >90% accuracy in determining when processing is complete
4. **Path Efficiency**: Select optimal next phase to minimize total cycles

## Implementation Constraints

1. Support both automatic and user-directed recursion control
2. Implement cycle counting and maximum limits
3. Maintain recursion history for loop detection
4. Support direct jumps to any phase in the PostChain

## Examples

### Recursion Decision Logic

```python
async def decide_recursion(
    current_state: Dict,
    cycle_count: int,
    max_cycles: int = 5
) -> YieldResult:
    """Determine whether to continue processing or terminate."""

    # Hard limit on recursion
    if cycle_count >= max_cycles:
        return YieldResult(
            continue_processing=False,
            final_response=current_state["content"],
            rationale="Maximum recursion depth reached"
        )

    # Check confidence threshold
    if current_state.get("confidence", 0) > 0.9:
        return YieldResult(
            continue_processing=False,
            final_response=current_state["content"],
            rationale="High confidence threshold met"
        )

    # Check if answer is still converging
    if cycle_count > 1 and calculate_convergence(current_state) < 0.1:
        return YieldResult(
            continue_processing=False,
            final_response=current_state["content"],
            rationale="Answer convergence reached"
        )

    # Decide which phase to invoke next
    if needs_more_information(current_state):
        next_phase = "experience"
        rationale = "Additional information required"
    elif needs_intention_clarification(current_state):
        next_phase = "intention"
        rationale = "Goal refinement needed"
    elif needs_additional_tools(current_state):
        next_phase = "action"
        rationale = "Tool execution required"
    else:
        # Default recursive flow
        next_phase = "action"
        rationale = "Standard recursive cycle"

    return YieldResult(
        continue_processing=True,
        next_phase=next_phase,
        continuation_prompt=generate_continuation_prompt(current_state, next_phase),
        rationale=rationale
    )
```

### Phase Selection Logic

```python
def select_next_phase(current_state: Dict) -> str:
    """Select the next phase to execute."""

    # Extract key indicators from state
    confidence = current_state.get("confidence", 0)
    info_sufficiency = current_state.get("information_sufficiency", 0)
    tool_indicators = current_state.get("needs_tools", False)

    # Decision tree for phase selection
    if info_sufficiency < 0.7:
        return "experience"  # Need more information
    elif "unclear_intent" in current_state.get("flags", []):
        return "intention"  # Need to clarify intent
    elif tool_indicators:
        return "action"  # Need to use tools
    elif len(current_state.get("context", [])) > 10:
        return "understanding"  # Need to clean up context
    else:
        return "action"  # Default recursive entry point
```

## Interaction with Other Phases

- **Receives from**: Understanding phase
- **Sends to**: Any phase (when recursing) or system (when complete)
- **Relationship**: Controls system flow and termination

## Success Criteria

1. Makes appropriate recursion decisions
2. Selects optimal next phase to minimize total cycles
3. Enforces recursion limits to prevent infinite loops
4. Produces properly formatted final responses
5. Maintains logical flow continuity across multiple cycles

=== File: docs/reward_function.md ===



==
reward_function
==


# Choir Novelty Reward Function

This document explains the mathematical approach used to calculate novelty rewards in the Choir system. We explored different approaches to implement an exponential reward scaling that provides higher rewards for more novel content.

## Reward Scaling Requirements

The novelty reward system needed to meet the following requirements:

1. Scale exponentially from 0.01 to 100.0 CHOIR tokens based on content novelty
2. Provide specific reward amounts at key similarity thresholds:
   - 0.95 similarity → 0.01 CHOIR (minimum novelty)
   - 0.90 similarity → 0.1 CHOIR
   - 0.85 similarity → 1.0 CHOIR
   - 0.80 similarity → 10.0 CHOIR
   - 0.75 similarity → 100.0 CHOIR (maximum novelty)
3. Use a smooth mathematical function that scales predictably
4. Implement using natural mathematical constants rather than magic numbers

## Approach 1: Curve Fitting

Our first approach (`reward_function.py`) used curve fitting to find a mathematical function that precisely matched our target reward points. We used SciPy's `curve_fit` function to fit an exponential function to our data points.

```python
# Define a logarithmic function with parameters to fit
def log_function(x, a, b, c):
    # We want a function that grows exponentially as similarity decreases
    return a * np.exp(b * (c - x))

# Fit the function to our data points
params, _ = curve_fit(log_function, similarity, rewards)

# Extract the fitted parameters
a, b, c = params
# a=0.002489, b=46.051702, c=0.980199

# The resulting function
reward = 0.002489 * np.exp(46.051702 * (0.980199 - max_similarity))
```

This approach produced a perfect fit to our target points but used somewhat arbitrary constants that lacked clear meaning.

## Approach 2: Simplified Exponential Function

Our second approach (`reward_function_simplified.py`) derived a more intuitive exponential function based on the mathematical properties of our reward scaling requirements.

We observed that:
- Each 0.05 decrease in similarity results in a 10x increase in reward
- This is a classic exponential pattern that can be expressed using the natural exponential function (e)

```python
# Constants with clear meaning
reference_similarity = 0.95  # The reference point where reward = min_reward
min_reward = 0.01            # Reward at reference similarity
reward_factor = 10           # How much reward increases per similarity_step
similarity_step = 0.05       # How much similarity needs to decrease for reward to increase by reward_factor

# Calculate exponent: ln(reward_factor) / similarity_step
exponent_factor = math.log(reward_factor) / similarity_step

# Calculate the reward
reward = min_reward * math.exp(exponent_factor * (reference_similarity - similarity))
```

The mathematical formula can be expressed as:
```
reward = min_reward * e^(ln(reward_factor)/similarity_step * (reference_similarity - similarity))
```

This simplified approach:
1. Uses constants with clear semantic meaning
2. Leverages the natural exponential function (e)
3. Produces identical results to the curve-fitted function
4. Is more maintainable and easier to understand

## Implementation in Choir

The simplified exponential function was implemented in `api/app/services/rewards_service.py` as follows:

```python
async def calculate_novelty_reward(self, max_similarity: float) -> int:
    # If max_similarity is close to 1.0, the prompt is not novel
    if max_similarity > 0.95:
        return 0

    # Base reward amount (1 CHOIR = 1_000_000_000 units)
    base_reward = 1_000_000_000
    
    # Constants with clear meaning
    reference_similarity = 0.95  # The reference point where reward = min_reward
    min_reward = 0.01            # Reward at reference similarity
    reward_factor = 10           # How much reward increases per similarity_step
    similarity_step = 0.05       # How much similarity needs to decrease for reward to increase by reward_factor
    
    # Calculate exponent: ln(reward_factor) / similarity_step
    exponent_factor = math.log(reward_factor) / similarity_step
    
    # Calculate the reward using natural exponential function
    reward_multiplier = min_reward * math.exp(exponent_factor * (reference_similarity - max_similarity))
    
    # Cap the reward at 100 CHOIR
    reward_multiplier = min(reward_multiplier, 100.0)
    
    # Convert to smallest units
    scaled_reward = int(base_reward * reward_multiplier)

    return scaled_reward
```

## Reward Table

The following table shows the rewards at different similarity levels:

| max_similarity | novelty_score | Reward (CHOIR) |
|---------------|--------------|----------------|
| 0.95 | 0.05 | 0.01 |
| 0.92 | 0.08 | 0.0316 |
| 0.90 | 0.10 | 0.1 |
| 0.88 | 0.12 | 0.3162 |
| 0.85 | 0.15 | 1.0 |
| 0.82 | 0.18 | 3.1623 |
| 0.80 | 0.20 | 10.0 |
| 0.78 | 0.22 | 31.6228 |
| 0.75 | 0.25 | 100.0 |

## Benefits of Exponential Scaling

The exponential reward scaling provides several benefits:

1. **Incentivizes Novelty**: Provides significantly higher rewards for truly novel content
2. **Dynamic Range**: Covers a wide range of rewards (0.01 to 100.0) to better differentiate content quality
3. **Logarithmic Perception**: Aligns with human perception, which tends to be logarithmic rather than linear
4. **Mathematical Elegance**: Uses a clean mathematical formula based on natural constants

This reward function ensures that users are properly incentivized to contribute novel content to the Choir ecosystem, with rewards that scale exponentially based on the uniqueness of their contributions.

=== File: docs/rewards_system.md ===



==
rewards_system
==


# Choir Rewards System

## Overview

The Choir rewards system incentivizes users to contribute novel prompts and cite valuable information. The system issues CHOIR tokens on the Sui blockchain as rewards for these contributions.

## Reward Types

### 1. Novelty Rewards

Novelty rewards are issued when a user submits a prompt that is semantically different from existing content in the vector database.

- **Calculation**: Novelty is calculated as `1 - max_similarity`, where `max_similarity` is the highest similarity score between the user's prompt and existing vectors.
- **Issuance**: Rewards are issued during the Experience Vectors phase.
- **Amount**: 0.01 to 100.0 CHOIR tokens, scaled exponentially based on the novelty score.

### 2. Citation Rewards

Citation rewards are issued when a user cites vector search results in their conversation.

- **Calculation**: 0.5 CHOIR tokens per citation, up to a maximum of 5 citations.
- **Issuance**: Rewards are issued during the Yield phase.
- **Detection**: Citations are detected using the `#ID` syntax in the model's response.

## Technical Implementation

### Backend (Python)

#### Novelty Rewards Pipeline

1. **Vector Embedding**: User prompts are embedded using an embedding model.
2. **Similarity Search**: The embeddings are compared against existing vectors in the database.
3. **Reward Calculation**: Novelty score is calculated as `1 - max_similarity`.
4. **Reward Issuance**: If the novelty score is high enough, CHOIR tokens are minted to the user's wallet.
5. **Response Enrichment**: Reward information is included in the API response and passed to the LLM.

```python
# Calculate and issue novelty reward
if wallet_address and max_similarity is not None:
    rewards_service = RewardsService()
    reward_result = await rewards_service.issue_novelty_reward(wallet_address, max_similarity)
    novelty_reward = reward_result
```

#### Citation Rewards Pipeline

1. **Citation Detection**: The LLM's response is analyzed to detect citations using the `#ID` syntax.
2. **Reward Calculation**: 0.5 CHOIR tokens per citation, up to 5 citations.
3. **Reward Issuance**: CHOIR tokens are minted to the user's wallet.

```python
# Extract citations and issue rewards
if wallet_address and response.content:
    rewards_service = RewardsService()
    citations = rewards_service.extract_citations(response.content)
    if citations:
        reward_result = await rewards_service.issue_citation_rewards(wallet_address, len(citations))
        citation_reward = reward_result
```

### Frontend (Swift)

#### Reward Display

1. **API Integration**: The Swift client receives reward information in the API response.
2. **UI Display**: Reward information is displayed in the phase card UI.
3. **Alerts**: Successful rewards trigger alert notifications.

```swift
// Process reward information from a phase response
func processPhaseResponse(phase: String, response: [String: Any]) {
    if phase == "experience_vectors", let noveltyRewardDict = response["novelty_reward"] as? [String: Any] {
        let reward = try JSONDecoder().decode(RewardInfo.self, from: jsonData)
        processReward(rewardInfo: reward)
    }
}
```

## LLM Integration

The LLM is instructed to include reward information in its responses:

1. **Novelty Rewards**: The LLM receives information about novelty rewards and includes it in the Experience Vectors phase response.
2. **Citation Rewards**: The LLM is instructed to use the `#ID` syntax when referencing vector search results, which triggers citation rewards.

## Memory System Integration

The rewards system is deeply integrated with Choir's memory system:

1. **Global Memory**: Vector embeddings form a global memory that all users can access.
2. **Per-User Memory**: Each user's wallet has its own collection of threads and rewards.
3. **Per-Thread Memory**: Each thread maintains its own context and citation history.

In the future, this memory system will evolve into hypergraphs of conceptual interrelations, allowing for more sophisticated reward mechanisms based on the value and interconnectedness of contributions.

## Blockchain Integration

Rewards are issued as CHOIR tokens on the Sui blockchain:

1. **Minting**: The `mint_choir` function in `sui_service.py` mints CHOIR tokens to the user's wallet.
2. **Verification**: Transactions are verified on the Sui blockchain.
3. **Balance**: Users can view their CHOIR token balance in the app.

```python
async def mint_choir(self, recipient_address: str, amount: int = 1_000_000_000):
    """Mint CHOIR tokens to recipient (default 1 CHOIR)"""
    txn = SuiTransaction(client=self.client)
    txn.move_call(
        target=f"{self.package_id}::choir::mint",
        arguments=[
            ObjectID(self.treasury_cap_id),
            SuiU64(amount),
            SuiAddress(recipient_address)
        ],
        type_arguments=[]
    )
    result = txn.execute()
    # Process result...
```

## Future Enhancements

1. **Reward Visualization**: Enhanced visualizations of rewards in the UI.
2. **Reward History**: A dedicated view for users to see their reward history.
3. **Community Rewards**: Rewards for community contributions and collaborations.
4. **Conceptual Hypergraphs**: Evolution of the memory system to support more sophisticated reward mechanisms.

=== File: docs/security_considerations.md ===



==
security_considerations
==


# Security Considerations for Choir (Qdrant-Sui MVP)

VERSION security_considerations: 8.0 (Qdrant-Sui MVP Focus)

## Introduction

This document outlines the security considerations for Choir's Qdrant-Sui Minimum Viable Product (MVP) architecture. This architecture centralizes AI workflow execution, data management (Qdrant), and blockchain interactions (Sui via `sui_service.py`) within a single Python API backend. Security focuses on protecting this central API, its data interactions, and the user's keys on the client side.

## Threat Model

The system addresses the following potential threats:

1.  **Blockchain Key Compromise**: Theft or unauthorized use of the API backend's private key used for Sui blockchain operations.
2.  **Contract Manipulation**: Unauthorized modification of the basic CHIP token contract parameters or execution (less likely with MVP's simple contract).
3.  **Token Theft**: Unauthorized triggering of reward distributions or transfers via the API.
4.  **Data Exfiltration**: Unauthorized access to or extraction of sensitive user or conversation data stored in Qdrant.
5.  **System Manipulation**: Unauthorized alterations to the API's behavior, PostChain workflow logic, or state stored in Qdrant.
6.  **Model Attacks**: Prompt injection, jailbreaking, or other attacks targeting the LLMs used within the PostChain workflow.
7.  **Resource Exhaustion**: Denial of service against the API backend or Qdrant through excessive requests.
8.  **Identity Spoofing**: Impersonation of legitimate users via compromised Sui keys or authentication bypass.
9.  **Infrastructure Compromise**: Attacks on the underlying infrastructure hosting the API backend and Qdrant (e.g., Render).

## Secure Blockchain Operations (MVP Context)

### Core Blockchain Security Goals

1.  **Secure Key Management**: Securely store and manage the private key used by the API backend's `sui_service.py` for Sui blockchain operations.
2.  **Protected Contract Interaction**: Ensure interactions with the Sui smart contract (basic CHIP token) are executed correctly.
3.  **Tamper-Proof Token Management**: Handle CHIP token reward distributions (simplified for MVP) in a way that prevents unauthorized manipulation.
4.  **Transaction Integrity**: Ensure that blockchain transactions initiated by the API are properly authorized and accurately reflect the intended action.

### Security Architecture (Centralized API Service)

1.  **API Backend Key Management:** The Sui private key used by `sui_service.py` is the most critical secret. It **must** be managed securely:
    *   **No Hardcoding:** Never hardcode the private key in the source code.
    *   **Environment Variables/Secrets:** Store the key securely using environment variables injected during deployment (e.g., Render's secret management).
    *   **Limited Access:** Restrict access to the production environment and secret management tools.
2.  **Controlled Interaction:** All blockchain interactions are funneled through the `sui_service.py` module within the API backend. This centralizes the logic and reduces the points where the key is directly used.
3.  **Input Validation:** The API must rigorously validate all parameters (recipient addresses, amounts) passed to `sui_service.py` functions before constructing blockchain transactions.

## Data Security Measures (Qdrant & API)

1.  **Data Classification:** Identify sensitive data stored in Qdrant (e.g., `intention_memory` content, user-Sui address mappings in `users`).
2.  **Encryption Architecture:**
    *   **Transit:** Use HTTPS for all communication between the client, API, and Qdrant (if Qdrant is hosted externally).
    *   **At Rest (Qdrant):** Rely on Qdrant's underlying storage mechanisms and the hosting provider's infrastructure for at-rest encryption. Consider Qdrant's specific encryption features if available and necessary.
    *   **At Rest (API Secrets):** Ensure the Sui private key and any other API secrets are stored encrypted at rest by the deployment platform (e.g., Render).
3.  **Qdrant Access Control:**
    *   Use API keys or other authentication mechanisms provided by Qdrant to restrict access to authorized services (only the Python API backend).
    *   Implement logical access control within the API backend to ensure, for example, that `intention_memory` is only queried for the currently authenticated user.

## Docker Container Security (API Backend)

1.  **Minimal Images:** Use minimal base images (like `python:3.12-slim`) for the API backend container to reduce the attack surface.
2.  **No Privileged Containers:** Run containers without unnecessary privileges.
3.  **Immutable Infrastructure:** Treat containers as immutable; rebuild and redeploy rather than modifying running containers.
4.  **Vulnerability Scanning:** Integrate vulnerability scanning into the CI/CD pipeline for the Docker image.
5.  **Secret Management:** Inject secrets (like the Sui private key, Qdrant API key) securely into the container environment at runtime, not during the build process.

## Model Security (PostChain Workflow)

1.  **Input Validation/Sanitization:** Sanitize user input passed to the PostChain workflow and subsequently to LLMs to mitigate prompt injection risks.
2.  **Output Filtering:** Filter or sanitize outputs from LLMs, especially if they might be displayed directly or used in sensitive contexts (though less critical if outputs primarily feed other phases or are stored).
3.  **Prompt Security:** Be mindful of prompt engineering techniques to make models less susceptible to jailbreaking or instruction hijacking, particularly for phases that might execute tools based on LLM output (deferred post-MVP).
4.  **Rate Limiting:** Implement rate limiting at the API gateway or within FastAPI to prevent abuse of LLM resources.
5.  **Model Usage Monitoring:** Monitor LLM usage for anomalies that might indicate attacks or misuse.

## Security Logging and Monitoring

1.  **Comprehensive Logging:** Log key security events within the API backend: authentication attempts (success/failure), significant state changes, calls to `sui_service.py`, errors, and potential security anomalies.
2.  **Qdrant Auditing:** If Qdrant provides audit logging features, enable them to monitor database access and operations.
3.  **Infrastructure Monitoring:** Utilize monitoring tools provided by the hosting platform (e.g., Render) to track resource usage, network traffic, and potential infrastructure-level threats.
4.  **Anomaly Detection:** Implement basic anomaly detection rules based on logs and metrics (e.g., sudden spike in failed authentications, unusual Qdrant query patterns, high rate of reward triggers).
5.  **Incident Response Plan:** Have a basic plan for responding to security incidents, including identifying the issue, containing the impact, remediating the vulnerability, and communicating appropriately.

## Future Security Enhancements (Post-MVP)

While the MVP focuses on core security, future enhancements could include:

1.  **Formal Verification:** For critical smart contracts (like a more complex economic or governance contract).
2.  **Quantum-Resistant Cryptography:** For long-term key and signature security (relevant if Sui adopts it).
3.  **Web Application Firewall (WAF):** Protect the API endpoint from common web attacks.
4.  **Enhanced Authentication:** Implement more robust authentication mechanisms beyond simple signature verification if needed.
5.  **Dedicated Secrets Management:** Integrate a dedicated secrets management solution (e.g., HashiCorp Vault) instead of relying solely on platform environment variables.

## Conclusion (MVP Focus)

Securing the Choir Qdrant-Sui MVP relies heavily on securing the central Python API backend and its interactions. Key priorities include: **secure management of the API's Sui private key**, robust input validation for both API endpoints and LLM prompts, proper access control for Qdrant collections, and standard web application security practices for the API itself. While simpler than a distributed TEE-based architecture, this centralized model requires diligent protection of the API backend as the primary trusted component for data access and blockchain interactions in the MVP.

=== File: docs/stack_argument.md ===



==
stack_argument
==


# The Choir Stack Argument: Qdrant-Sui MVP

VERSION stack_argument: 8.0 (Qdrant-Sui MVP Focus)

## Executive Summary

This document argues for the focused technology stack selected for the **Choir Qdrant-Sui Minimum Viable Product (MVP)**. The primary goal of this MVP is to establish and validate the core data flow using Qdrant as the central data and vector store, integrated with a basic Sui blockchain mechanism for the CHIP token and reward structure. This stack leverages existing components where possible (like the current LCEL-based PostChain workflow) to accelerate MVP development while laying the foundation for future scalability.

The core technologies for the Qdrant-Sui MVP are:

1.  **Qdrant:** Central data layer for users, threads, messages (including embedded phase outputs), and phase-specific memory. Handles vector storage and semantic search.
2.  **Sui (via PySUI Service):** Blockchain layer for the CHIP token and reward distribution mechanism (simplified for MVP).
3.  **Python API (FastAPI/Uvicorn):** Orchestration layer handling client requests, PostChain execution, Qdrant interactions, and Sui service calls.
4.  **PostChain Workflow (LCEL Implementation):** The existing `langchain_workflow.py` implementing the AEIOU-Y phases, adapted for refined Qdrant interactions.
5.  **Langchain Utils (`langchain_utils.py`):** LLM abstraction layer used by the PostChain workflow.
6.  **Pydantic:** Data validation for API and internal structures.
7.  **Docker:** Containerization for deployment of the Python API.
8.  **(Client Side) SwiftUI & Keychain:** User interface and secure Sui private key storage.

## Qdrant-Sui MVP Goal

The objective is to build a functional slice of the Choir system that demonstrates:

1.  **Core Data Structure:** Storing users, threads, messages, and phase outputs in Qdrant using a refined schema.
2.  **Semantic Search:** Utilizing Qdrant vector search within the Experience phase to find relevant priors.
3.  **Phase-Specific Memory:** Implementing `intention_memory` and `observation_memory` in Qdrant.
4.  **Reward Triggering:** Calculating basic novelty/similarity scores and triggering a (potentially simulated or basic) reward distribution via the Sui service based on message creation and citation.
5.  **End-to-End Flow:** A user interacting via the SwiftUI client, triggering the PostChain workflow via the API, interacting with Qdrant, and potentially initiating a basic Sui reward action.

## The Core MVP Stack & Rationale

1.  **Qdrant (Central Data Layer):**
    *   **Role:** The **single source of truth** for all data relevant to the AI processing loop and reward mechanism. Stores user mappings, thread metadata, core conversation turns (user prompts & final AI responses), embedded internal phase outputs, and phase-specific memory collections (`intention_memory`, `observation_memory`). Crucial for the Experience phase's vector search (global priors) and provides the necessary inputs (novelty/similarity scores, author/prior linkage) for the reward system.
    *   **Why Chosen for MVP:** Essential for the core concept; vector search is fundamental to Experience/rewards. Centralizing here simplifies the MVP backend. Using existing collections (`choir`, `users`, `chat_threads`) with schema refinement is pragmatic. Adding `intention_memory` and `observation_memory` provides the necessary specialized storage.

2.  **Sui (via PySUI Service - Blockchain Layer):**
    *   **Role:** Manages the CHIP token (basic existence contract) and executes the reward distribution logic triggered by the API. For MVP, this logic might be simplified (e.g., basic minting or even off-chain logging of intended rewards). The `sui_service.py` encapsulates PySUI interactions.
    *   **Why Chosen for MVP:** Core to the "tokenized marketplace" vision. Integrating a basic version early validates the concept and technical feasibility. PySUI provides the necessary SDK.

3.  **Python API (FastAPI/Uvicorn - Orchestration Layer):**
    *   **Role:** The central hub. Handles client authentication (via Sui signature verification), orchestrates the `langchain_workflow.py` execution for the PostChain, mediates all interactions with Qdrant (`database.py`), triggers the Sui service (`sui_service.py`) for rewards, and manages SSE streaming to the client.
    *   **Why Chosen for MVP:** Provides a necessary interface between the client, the AI logic, and the data/blockchain layers. FastAPI is performant and integrates well with Pydantic.

4.  **PostChain Workflow (LCEL - Core AI Logic):**
    *   **Role:** Implements the AEIOU-Y phases sequentially using the existing Langchain Expression Language (LCEL) structure in `langchain_workflow.py`. This logic is adapted to read from/write to the designated Qdrant collections (via the API/`database.py`). The Experience phase calculates novelty/similarity scores. The Yield phase structures the final AI message with embedded phase outputs and triggers the reward calculation via the API.
    *   **Why Chosen for MVP:** **Leverages existing, functional code.** Avoids a major refactor for the MVP, allowing faster progress on the core Qdrant/Sui integration. LCEL provides a clear structure for the sequential phase execution.

5.  **Langchain Utils (`langchain_utils.py` - LLM Abstraction):**
    *   **Role:** Provides a consistent interface to multiple LLM providers, allowing the PostChain workflow to utilize different models without being tightly coupled to specific provider APIs.
    *   **Why Chosen for MVP:** Already implemented and essential for the PostChain workflow's LLM interactions. Supports flexibility.

6.  **Pydantic (Data Integrity):**
    *   **Role:** Ensures data consistency and validation for API requests/responses and internal data structures used within the PostChain workflow and Qdrant interactions.
    *   **Why Chosen for MVP:** Best practice for robust Python development, especially with APIs and complex data structures. Reduces errors.

7.  **Docker (Deployment):**
    *   **Role:** Containerizes the Python API service (including all its dependencies like FastAPI, Langchain, PySUI, Qdrant client) for consistent and reproducible deployment.
    *   **Why Chosen for MVP:** Standard for modern web service deployment, simplifying setup and ensuring environment consistency.

8.  **(Client) SwiftUI & Keychain:**
    *   **Role:** Provides the user interface for interaction. Securely stores the user's Sui private key using the device Keychain. Handles message signing for authentication. Displays streamed PostChain outputs.
    *   **Why Chosen for MVP:** Native iOS provides the best user experience and secure key management capabilities required.

## Why This Stack for the MVP?

*   **Focus:** Directly targets the core Qdrant-Sui integration, which is the central hypothesis to validate.
*   **Speed & Pragmatism:** Reuses the existing `langchain_workflow.py` (LCEL implementation), significantly reducing the initial development effort.
*   **Simplicity:** Defers complexities not strictly necessary to prove the core Qdrant-Sui concept.
*   **Validation:** Allows for rapid validation of the proposed Qdrant data structures, the basic reward trigger mechanism, and the end-to-end user flow.

## Synergy within the MVP Stack

The Qdrant-Sui MVP stack creates a clear data and execution flow:

1.  **Client (SwiftUI):** User interacts, signs request with Sui key (Keychain).
2.  **API (FastAPI):** Authenticates user (verifies signature, maps Sui address to Qdrant User ID via `users` collection), receives prompt, initiates PostChain workflow.
3.  **PostChain (LCEL):** Executes AEIOU-Y phases sequentially.
    *   Uses **Langchain Utils** to call LLMs.
    *   Interacts with **Qdrant** via API/`database.py` for memory (`intention_memory`, `observation_memory`) and priors (`choir` collection).
    *   Experience phase calculates novelty/similarity scores using Qdrant results.
    *   Yield phase bundles outputs into a single AI message structure.
4.  **API (FastAPI):** Receives final AI message structure from Yield, stores it in **Qdrant** (`choir` collection), triggers **Sui Service**.
5.  **Sui Service:** Calculates basic reward distribution, interacts with **Sui Blockchain** (basic mint/log).
6.  **API (FastAPI):** Streams phase outputs (via SSE) back to the Client.
7.  **Client (SwiftUI):** Displays conversation and phase outputs.

## Path Forward (Beyond MVP)

This MVP stack provides a solid foundation. Future iterations can build upon it:

*   **Refine Reward Logic:** Implement the sophisticated reward splitting formula on Sui or via a secure off-chain oracle.
*   **Scale PostChain:** Address performance bottlenecks in the `langchain_workflow.py` as needed, potentially by optimizing or modularizing phase execution.
*   **Enhance Client:** Implement client-side caching for improved offline experience and UI responsiveness.
*   **Security Hardening:** Implement enhanced security measures for the API and blockchain interactions.
*   **Add Features:** Implement governance, advanced tool use, multi-modality, etc.

## Conclusion

The proposed Qdrant-Sui MVP stack is a pragmatic and focused approach. It prioritizes the core integration of Qdrant for AI data management and Sui for the token economy, leveraging existing components like the LCEL-based PostChain workflow for rapid development and validation. This stack allows us to quickly test the fundamental concepts of Choir's data and reward system.

=== File: docs/state_management_patterns.md ===



==
state_management_patterns
==


# State Management Patterns in Choir (Qdrant-Sui MVP)

VERSION state_management_patterns: 8.0 (Qdrant-Sui MVP Focus)

## Overview

State management is crucial for the Choir platform. In the Qdrant-Sui MVP, the primary focus is on managing state within the central Python API and persisting core data within Qdrant. This document outlines the state management patterns specific to this MVP architecture. Client-side caching and distributed server state are deferred post-MVP.

## State Management in the Qdrant-Sui MVP

The state is primarily managed in two locations:

1.  **Python API (In-Memory during Request Processing):** The FastAPI application manages the *transient state* of a single PostChain execution cycle for a given user request.
2.  **Qdrant (Persistent State):** Qdrant serves as the **persistent source of truth** for all core data entities required for the AI workflow and the reward system.

## 1. Python API: Orchestration & Transient State

*   **Role:** The Python API acts as the central orchestrator. For each incoming user request, it manages the flow through the PostChain (AEIOU-Y) phases implemented in `langchain_workflow.py`.
*   **Transient State Handling:**
    *   **Workflow Context:** During a single PostChain cycle triggered by a user message, the API holds the intermediate outputs from each phase (Action, Experience, etc.) in memory. This context (including text outputs, calculated scores, potential citations) is passed sequentially from one phase function to the next within the `langchain_workflow.py`.
    *   **Stateless Between Requests:** The API itself aims to be largely stateless *between* distinct user requests/PostChain cycles. All necessary persistent state is fetched from or saved to Qdrant at the beginning or end of the request processing cycle.
    *   **Concurrency:** FastAPI and Python's `asyncio` handle concurrent user requests. Care must be taken within the workflow logic if shared resources (beyond Qdrant/Sui services which handle their own concurrency) are accessed, but the primary state (conversation history, memory) is managed via Qdrant, pushing concurrency control largely to the database/service layer.

## 2. Qdrant: Persistent State Management

*   **Role:** Qdrant is the **authoritative persistent store** for the MVP.
*   **Managed Entities:**
    *   **`users`:** Maps Sui addresses to internal user IDs. Persistent user identity link.
    *   **`chat_threads`:** Stores metadata about conversation threads. Persistent thread context.
    *   **`choir` (Messages):** Stores the core conversational turns (user prompts, final AI responses). Critically, AI responses embed the outputs from *all* internal PostChain phases (`phase_outputs` dictionary), novelty/similarity scores, and citation links (`cited_prior_ids`). This collection is the persistent record of the conversation history and the primary input for reward calculations.
    *   **`intention_memory`:** Persists user-specific goals and preferences across multiple turns and sessions, queryable by the Intention phase. Filtered by `user_id`.
    *   **`observation_memory`:** Persists thread-specific concepts and summaries across multiple turns and sessions, queryable by the Observation phase (and potentially Experience). Filtered by `thread_id`.
*   **Data Integrity & Access:** The Python API (via `database.py`) is responsible for all CRUD operations on Qdrant, ensuring data is structured according to the defined schemas (using Pydantic for validation) before persistence. Access control for user-specific memory (`intention_memory`) is enforced by filtering queries based on the authenticated `user_id`.

## 3. Client (SwiftUI): UI State & Keychain

*   **Role:** Manages the user interface state and secure key storage.
*   **State Handled:**
    *   **UI State:** Current view, input field content, display state of messages and phases (fetched from API).
    *   **Sui Private Key:** Securely stored in the device Keychain. Used for signing authentication messages.
### Update (2025-04-09): iOS Client Local Persistence

As of April 9, 2025, the iOS client **replaced previous persistence methods** (such as SwiftData) with a **local file-based JSON storage** approach:

- Each thread and its associated messages are saved as a **single JSON file** on device storage.
- This improves transparency, simplifies debugging, and enhances offline access.
- The files are managed by the app's `ThreadPersistenceService`, which handles reading/writing JSON representations of threads.
- This approach fully replaces previous CoreData/SwiftData-based persistence.

This change aligns with a simplified, file-centric architecture for local data management on iOS.
*   **No Persistent App Data (MVP):** For the MVP, the client **does not** maintain its own persistent cache of conversation history. It fetches conversation data from the API as needed for display. Offline access is deferred post-MVP.

## State Flow Example (Single Turn)

1.  User sends message via SwiftUI Client. Client signs request hash with Sui Key.
2.  Python API receives request, verifies signature, maps Sui Address to Qdrant User ID (`users` collection). Fetches relevant thread context (`chat_threads`, recent messages from `choir`).
3.  API initiates PostChain workflow (`langchain_workflow.py`) with user message and thread context.
4.  **Phase Execution (Transient State):**
    *   Action phase runs.
    *   Experience phase runs: Queries `choir` (Qdrant) for priors, calculates scores.
    *   Intention phase runs: Queries/updates `intention_memory` (Qdrant).
    *   Observation phase runs: Queries/updates `observation_memory` (Qdrant).
    *   Understanding phase runs: May trigger deletes in `intention_memory`/`observation_memory` (Qdrant).
    *   Yield phase runs: Bundles all phase outputs.
    *   *(Intermediate outputs are held in memory by the API/workflow runner during this sequence)*.
5.  API receives final bundled AI response data from Yield.
6.  API **persists** the new AI message (with embedded `phase_outputs`, scores, citations) to the `choir` collection in Qdrant.
7.  API triggers the Sui Service with relevant data (message ID, author ID, prior IDs, scores) from the persisted Qdrant entry.
8.  API streams phase outputs (potentially fetched back from the newly saved Qdrant entry or held from step 4) back to the Client via SSE.
9.  Client updates UI state based on SSE stream.

## Conclusion (MVP Focus)

The Qdrant-Sui MVP employs a pragmatic state management strategy. Persistent state critical for the AI workflow and reward system resides centrally in Qdrant, managed by the Python API. The API handles transient state during request processing. The client manages UI state and the user's private key. This approach minimizes complexity for the MVP, allowing focus on validating the core Qdrant-Sui data flow and reward mechanism.

=== File: docs/wallet_languification.md ===



==
wallet_languification
==


# Wallet Languification: Natural Language Crypto Interactions

VERSION wallet_languification: 1.0 (Conversational Crypto UX)

## Vision: Crypto Without Complexity

Languification transforms wallet interactions from technical operations into natural conversations. Instead of navigating complex UIs with addresses, amounts, and transaction types, users simply describe what they want to do in plain language.

## Core Principle: Speak Your Intent

**Traditional Crypto UX:**
```
1. Navigate to Send tab
2. Select coin type (SUI/CHOIR)
3. Enter recipient address (0x1234...)
4. Enter amount (0.001 SUI)
5. Review transaction details
6. Confirm and sign
```

**Languified UX:**
```
User: "Send 5 CHOIR to Alice"
Wallet: "Sending 5 CHOIR to Alice (0x1234...). Confirm?"
User: "Yes"
Wallet: "✅ Sent! Transaction: abc123..."
```

## Natural Language Patterns

### Balance Inquiries
```
"How much CHOIR do I have?"
"What's my SUI balance?"
"Show me all my balances"
"Am I rich yet?" → Shows portfolio value
```

### Sending Transactions
```
"Send 10 CHOIR to Bob"
"Transfer 0.5 SUI to 0x1234..."
"Pay Alice 25 CHOIR for the coffee"
"Send half my CHOIR to my other wallet"
```

### Receiving Payments
```
"Show my QR code"
"How do I receive CHOIR?"
"Generate a payment request for 50 CHOIR"
"Share my wallet address"
```

### Wallet Management
```
"Switch to my main wallet"
"Create a new wallet called 'Trading'"
"Show me my wallet addresses"
"Export my backup phrase"
```

### Transaction History
```
"Show my recent transactions"
"When did I last send CHOIR?"
"How much have I earned this week?"
"Find my transaction to Alice"
```

## Implementation Architecture

### Natural Language Processing Pipeline

```swift
struct WalletLanguageProcessor {
    func processIntent(_ input: String) -> WalletIntent {
        // 1. Intent Classification
        let intent = classifyIntent(input)
        
        // 2. Entity Extraction
        let entities = extractEntities(input, for: intent)
        
        // 3. Validation & Confirmation
        let action = validateAndPrepare(intent, entities)
        
        return action
    }
}
```

### Intent Categories
```swift
enum WalletIntent {
    case checkBalance(coinType: CoinType?)
    case sendPayment(amount: Double, coinType: CoinType, recipient: String)
    case receivePayment(amount: Double?, coinType: CoinType?)
    case switchWallet(identifier: String)
    case createWallet(name: String?)
    case showTransactions(filter: TransactionFilter?)
    case exportWallet
    case showQRCode
    case help(topic: String?)
}
```

### Entity Recognition
```swift
struct WalletEntities {
    let amounts: [Double]           // "5", "0.5", "half"
    let coinTypes: [CoinType]       // "CHOIR", "SUI"
    let addresses: [String]         // "0x1234...", "Alice"
    let walletNames: [String]       // "main", "trading"
    let timeRanges: [TimeRange]     // "this week", "yesterday"
}
```

## Conversational UI Design

### Chat-Style Interface
```
┌─────────────────────────────────────┐
│ 💬 Wallet Assistant                 │
├─────────────────────────────────────┤
│                                     │
│ You: How much CHOIR do I have?      │
│                                     │
│ 🤖: You have 127.5 CHOIR tokens     │
│     Worth ~$25.50 USD               │
│                                     │
│ You: Send 10 to Alice               │
│                                     │
│ 🤖: Sending 10 CHOIR to Alice       │
│     (0x1234...5678)                 │
│     ┌─────────────────────────────┐ │
│     │ [Confirm] [Cancel]          │ │
│     └─────────────────────────────┘ │
│                                     │
│ 📝 Type your request...             │
└─────────────────────────────────────┘
```

### Voice Integration
```swift
struct VoiceWalletInterface {
    @State private var isListening = false
    @State private var speechRecognizer = SpeechRecognizer()
    
    func startListening() {
        speechRecognizer.startRecording { result in
            processVoiceCommand(result.bestTranscription.formattedString)
        }
    }
}
```

### Smart Suggestions
```
Recent commands:
• "Send 5 CHOIR to Alice"
• "Check my balance"
• "Show QR code"

Quick actions:
• 💰 Check balances
• 📤 Send payment
• 📥 Receive payment
• 🔄 Switch wallet
```

## Advanced Features

### Contact Management
```swift
struct WalletContact {
    let name: String
    let address: String
    let nickname: String?
    let avatar: String?
    let transactionHistory: [Transaction]
}

// Usage:
"Send 10 CHOIR to Alice" → Resolves to known contact
"Pay the coffee shop" → Resolves to recent merchant
```

### Smart Amount Recognition
```
"Send half my CHOIR to Bob" → Calculates 50% of balance
"Send $10 worth of SUI" → Converts USD to SUI amount
"Send everything except gas" → Leaves minimum for fees
"Round up to 100 CHOIR" → Calculates difference needed
```

### Context Awareness
```swift
struct WalletContext {
    let currentBalance: [CoinType: Double]
    let recentTransactions: [Transaction]
    let frequentContacts: [WalletContact]
    let userPreferences: WalletPreferences
    
    func enhanceIntent(_ intent: WalletIntent) -> EnhancedIntent {
        // Add context-specific suggestions and validations
    }
}
```

### Multi-Modal Interactions
```
Voice: "Send 5 CHOIR to Alice"
Visual: Shows confirmation with Alice's avatar
Haptic: Gentle vibration on successful send
Audio: "Payment sent successfully"
```

## Error Handling & Safety

### Intelligent Validation
```
User: "Send 1000 CHOIR to Bob"
Wallet: "⚠️ That's 78% of your balance. Are you sure?"

User: "Send CHOIR to 0xinvalid"
Wallet: "❌ That address looks invalid. Did you mean Bob (0x1234...)?"

User: "Send -5 CHOIR"
Wallet: "🤔 I can't send negative amounts. Did you mean receive 5 CHOIR?"
```

### Confirmation Patterns
```swift
enum ConfirmationLevel {
    case none           // Small amounts to known contacts
    case simple         // "Confirm?"
    case detailed       // Show full transaction details
    case biometric      // Require Face ID/Touch ID
}
```

### Undo/Recovery
```
User: "Oh no, I sent to the wrong address!"
Wallet: "I can't reverse blockchain transactions, but I can help you contact the recipient or report if it's a known scam address."
```

## Implementation Phases

### Phase 1: Basic Language Processing
- Intent classification for common operations
- Simple entity extraction (amounts, coin types)
- Chat-style interface for wallet operations

### Phase 2: Advanced Understanding
- Contact management and name resolution
- Complex amount calculations ("half", "all except gas")
- Context-aware suggestions

### Phase 3: Voice & Multi-Modal
- Speech recognition and synthesis
- Voice commands for hands-free operation
- Haptic feedback for confirmations

### Phase 4: AI Enhancement
- Learning user patterns and preferences
- Predictive suggestions based on behavior
- Natural conversation flow with follow-ups

## Technical Integration

### Existing Wallet Manager Integration
```swift
extension WalletManager {
    func processLanguageCommand(_ command: String) async -> WalletResponse {
        let intent = WalletLanguageProcessor.shared.processIntent(command)
        
        switch intent {
        case .sendPayment(let amount, let coinType, let recipient):
            return await handleSendPayment(amount, coinType, recipient)
        case .checkBalance(let coinType):
            return await handleBalanceCheck(coinType)
        // ... other cases
        }
    }
}
```

### Response Generation
```swift
struct WalletResponse {
    let message: String
    let actionRequired: Bool
    let confirmationData: ConfirmationData?
    let suggestedActions: [QuickAction]
}
```

## Success Metrics

### User Experience
- **Command success rate**: % of natural language commands correctly interpreted
- **Task completion time**: Reduction in time to complete wallet operations
- **User satisfaction**: Preference for language vs traditional UI

### Adoption
- **Feature usage**: % of users who try language interface
- **Retention**: Users who continue using language interface
- **Voice adoption**: Usage of voice commands vs text

### Safety
- **Error prevention**: Reduction in transaction mistakes
- **Confirmation effectiveness**: Appropriate confirmation levels
- **Recovery assistance**: Success in helping users with mistakes

## Conclusion: Making Crypto Human

Wallet languification transforms cryptocurrency from a technical challenge into a natural conversation. By understanding user intent and providing intelligent assistance, we make blockchain technology accessible to everyone, regardless of technical expertise.

This approach aligns perfectly with Choir's mission of using AI to enhance rather than replace human interaction - in this case, making the complex world of cryptocurrency as simple as asking for what you want.
